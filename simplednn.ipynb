{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 06:00:12.316594: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/project/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2023-06-08 06:00:16.789605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:16.817525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:16.817804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:16.822966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:16.823378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:16.823674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:18.192218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:18.192478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:18.192634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:18.192763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4412 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Reshape\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint ,EarlyStopping\n",
    "temperature = 0.03\n",
    "learning_rate=0.001\n",
    "\n",
    "def dataloader(path, featType):\n",
    "    \"\"\"\n",
    "    Load data from a MATLAB file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the MATLAB file.\n",
    "        featType (int): Type of features to load.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Tuple containing input features, labels, weights, and additional information.\n",
    "    \"\"\"\n",
    "    data = scipy.io.loadmat(path)\n",
    "    print(data.keys())\n",
    "\n",
    "    AF = data['AF']\n",
    "    x1 = AF[:-2]\n",
    "    y = AF[-2]\n",
    "    w = AF[-1]\n",
    "\n",
    "    if featType == 1:\n",
    "        x = x1\n",
    "    else:\n",
    "        x2 = data['CF']\n",
    "        x = np.concatenate((x1, x2), axis=0)\n",
    "    return x.T, y.T, w.T, data['CF_info']\n",
    "\n",
    "def calculate_accuracy(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy between two arrays.\n",
    "\n",
    "    Args:\n",
    "        arr1 (array): First array.\n",
    "        arr2 (array): Second array.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy between the two arrays.\n",
    "    \"\"\"\n",
    "    count = sum(1 for itr1, itr2 in zip(arr1, arr2) if itr1 == itr2)\n",
    "    return count / len(arr1)\n",
    "\n",
    "def normalization(feats):\n",
    "\n",
    "    \"\"\"\n",
    "    Normalize the input features using standard scaling.\n",
    "\n",
    "    Args:\n",
    "        feats (array): Input features.\n",
    "\n",
    "    Returns:\n",
    "        array: Normalized features.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(feats)\n",
    "    scaler = StandardScaler()\n",
    "    x_new = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return x_new\n",
    "\n",
    "def make_partitions(arr_words, arr_labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Create partitions based on word boundaries and labels.\n",
    "\n",
    "    Args:\n",
    "        arr_words (array): Array of words.\n",
    "        arr_labels (array): Array of labels.\n",
    "\n",
    "    Returns:\n",
    "        array: Partitions based on word boundaries and labels.\n",
    "    \"\"\"\n",
    "    v = []\n",
    "    temp = []\n",
    "\n",
    "    for i in range(len(arr_words) - 1):\n",
    "        word = arr_words[i]\n",
    "        next_word = arr_words[i + 1]\n",
    "        temp.append(arr_labels[i])\n",
    "\n",
    "        if word != next_word or i == len(arr_words) - 2:\n",
    "            if i == len(arr_words) - 2:\n",
    "                temp.append(arr_labels[i + 1])\n",
    "\n",
    "            numpy_temp = np.array(temp)\n",
    "            temp_max = np.amax(numpy_temp)\n",
    "            numpy_temp = np.divide(numpy_temp, temp_max)\n",
    "            v = np.concatenate((v, numpy_temp), axis=None)\n",
    "            temp.clear()\n",
    "\n",
    "    v1 = [1 if i == 1 else 0 for i in v]\n",
    "    return v1\n",
    "    \n",
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "featType = 1; #Acoustic or Acoustic+context\n",
    "if featType == 1:\n",
    "  original_dim = 19\n",
    "else:\n",
    "  original_dim = 38\n",
    "\n",
    "\n",
    "# print('Classification with::::::',os.path.basename(filee))\n",
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "# print('test file:::::::',os.path.basename(test_path))\n",
    "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
    "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
    "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "\n",
    "xtest_a = normalization(xtest)\n",
    "xtest_ac = normalization(xtest1)\n",
    "xtrain = normalization(xtrain)\n",
    "xtrain1 = normalization(xtrain1)\n",
    "\n",
    "woPP=[]; wPP=[]\n",
    "input_shape1 = (19,1)\n",
    "input_shape2 = (38,1)\n",
    "temperature = 0.03\n",
    "learning_rate=0.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# Splitting xtrain and ytrain into training and validation sets\n",
    "xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting xtrain1 and ytrain1 into training and validation sets\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "batch_size = 16\n",
    "hidden_units = 64\n",
    "projection_units = 128\n",
    "num_epochs = 200\n",
    "dropout_rate = 0.3\n",
    "num_classes = 2\n",
    "input_shape1 = (19,)\n",
    "input_shape = (38,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(hidden_units):\n",
    "\n",
    "    # for layer in encoder.layers:\n",
    "    #     layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # features = encoder(inputs)\n",
    "    features=layers.Dense(hidden_units, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "units=64\n",
    "\n",
    "classifier = create_classifier(units)\n",
    "\n",
    "# history = classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 38)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2496      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,833\n",
      "Trainable params: 8,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-06-08 06:01:23.037078: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x21750420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-08 06:01:23.037119: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2023-06-08 06:01:23.078425: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-08 06:01:23.465049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-08 06:01:23.808974: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/295 [============================>.] - ETA: 0s - loss: 0.6032 - binary_accuracy: 0.6685"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 7s 7ms/step - loss: 0.6032 - binary_accuracy: 0.6689 - val_loss: 0.5147 - val_binary_accuracy: 0.7447\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.5209 - binary_accuracy: 0.7572 - val_loss: 0.4793 - val_binary_accuracy: 0.7795\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4922 - binary_accuracy: 0.7691 - val_loss: 0.4565 - val_binary_accuracy: 0.8007\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4714 - binary_accuracy: 0.7782 - val_loss: 0.4441 - val_binary_accuracy: 0.8066\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4507 - binary_accuracy: 0.7975 - val_loss: 0.4247 - val_binary_accuracy: 0.8185\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4310 - binary_accuracy: 0.8092 - val_loss: 0.4108 - val_binary_accuracy: 0.8253\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4238 - binary_accuracy: 0.8128 - val_loss: 0.4002 - val_binary_accuracy: 0.8278\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4124 - binary_accuracy: 0.8185 - val_loss: 0.3871 - val_binary_accuracy: 0.8329\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3977 - binary_accuracy: 0.8319 - val_loss: 0.3806 - val_binary_accuracy: 0.8388\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3939 - binary_accuracy: 0.8283 - val_loss: 0.3638 - val_binary_accuracy: 0.8524\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3825 - binary_accuracy: 0.8323 - val_loss: 0.3613 - val_binary_accuracy: 0.8465\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3865 - binary_accuracy: 0.8321 - val_loss: 0.3584 - val_binary_accuracy: 0.8507\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3780 - binary_accuracy: 0.8321 - val_loss: 0.3516 - val_binary_accuracy: 0.8550\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3703 - binary_accuracy: 0.8423 - val_loss: 0.3492 - val_binary_accuracy: 0.8558\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3646 - binary_accuracy: 0.8466 - val_loss: 0.3497 - val_binary_accuracy: 0.8609\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3564 - binary_accuracy: 0.8444 - val_loss: 0.3395 - val_binary_accuracy: 0.8626\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3542 - binary_accuracy: 0.8457 - val_loss: 0.3374 - val_binary_accuracy: 0.8626\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.3564 - binary_accuracy: 0.8453 - val_loss: 0.3348 - val_binary_accuracy: 0.8668\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3434 - binary_accuracy: 0.8551 - val_loss: 0.3282 - val_binary_accuracy: 0.8736\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3420 - binary_accuracy: 0.8538 - val_loss: 0.3330 - val_binary_accuracy: 0.8736\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3369 - binary_accuracy: 0.8521 - val_loss: 0.3243 - val_binary_accuracy: 0.8702\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3310 - binary_accuracy: 0.8629 - val_loss: 0.3202 - val_binary_accuracy: 0.8796\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3278 - binary_accuracy: 0.8597 - val_loss: 0.3260 - val_binary_accuracy: 0.8702\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3251 - binary_accuracy: 0.8599 - val_loss: 0.3201 - val_binary_accuracy: 0.8719\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3197 - binary_accuracy: 0.8633 - val_loss: 0.3195 - val_binary_accuracy: 0.8728\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3250 - binary_accuracy: 0.8612 - val_loss: 0.3190 - val_binary_accuracy: 0.8711\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3202 - binary_accuracy: 0.8625 - val_loss: 0.3132 - val_binary_accuracy: 0.8770\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3171 - binary_accuracy: 0.8684 - val_loss: 0.3104 - val_binary_accuracy: 0.8787\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3113 - binary_accuracy: 0.8686 - val_loss: 0.3084 - val_binary_accuracy: 0.8846\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3108 - binary_accuracy: 0.8705 - val_loss: 0.3101 - val_binary_accuracy: 0.8787\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3239 - binary_accuracy: 0.8657 - val_loss: 0.3106 - val_binary_accuracy: 0.8796\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3060 - binary_accuracy: 0.8680 - val_loss: 0.3091 - val_binary_accuracy: 0.8770\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3045 - binary_accuracy: 0.8733 - val_loss: 0.3077 - val_binary_accuracy: 0.8872\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3013 - binary_accuracy: 0.8720 - val_loss: 0.3047 - val_binary_accuracy: 0.8821\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3105 - binary_accuracy: 0.8710 - val_loss: 0.3075 - val_binary_accuracy: 0.8838\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3042 - binary_accuracy: 0.8697 - val_loss: 0.3082 - val_binary_accuracy: 0.8813\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2846 - binary_accuracy: 0.8848 - val_loss: 0.3071 - val_binary_accuracy: 0.8804\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2966 - binary_accuracy: 0.8792 - val_loss: 0.3045 - val_binary_accuracy: 0.8821\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2951 - binary_accuracy: 0.8788 - val_loss: 0.3057 - val_binary_accuracy: 0.8804\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2931 - binary_accuracy: 0.8782 - val_loss: 0.3038 - val_binary_accuracy: 0.8813\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2867 - binary_accuracy: 0.8837 - val_loss: 0.2934 - val_binary_accuracy: 0.8906\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2911 - binary_accuracy: 0.8841 - val_loss: 0.2937 - val_binary_accuracy: 0.8897\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2940 - binary_accuracy: 0.8773 - val_loss: 0.2920 - val_binary_accuracy: 0.8889\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2817 - binary_accuracy: 0.8852 - val_loss: 0.2865 - val_binary_accuracy: 0.8914\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2838 - binary_accuracy: 0.8805 - val_loss: 0.2891 - val_binary_accuracy: 0.8914\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2756 - binary_accuracy: 0.8850 - val_loss: 0.2922 - val_binary_accuracy: 0.8838\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2798 - binary_accuracy: 0.8826 - val_loss: 0.2908 - val_binary_accuracy: 0.8855\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2752 - binary_accuracy: 0.8858 - val_loss: 0.2869 - val_binary_accuracy: 0.8897\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2726 - binary_accuracy: 0.8905 - val_loss: 0.2861 - val_binary_accuracy: 0.8923\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2801 - binary_accuracy: 0.8818 - val_loss: 0.2914 - val_binary_accuracy: 0.8914\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2723 - binary_accuracy: 0.8892 - val_loss: 0.2842 - val_binary_accuracy: 0.8965\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2711 - binary_accuracy: 0.8907 - val_loss: 0.2847 - val_binary_accuracy: 0.8923\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.2769 - binary_accuracy: 0.8837 - val_loss: 0.2878 - val_binary_accuracy: 0.8863\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2671 - binary_accuracy: 0.8960 - val_loss: 0.2804 - val_binary_accuracy: 0.8974\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2665 - binary_accuracy: 0.8962 - val_loss: 0.2789 - val_binary_accuracy: 0.8991\n",
      "Epoch 56/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2680 - binary_accuracy: 0.8918 - val_loss: 0.2848 - val_binary_accuracy: 0.8914\n",
      "Epoch 57/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2644 - binary_accuracy: 0.8901 - val_loss: 0.2858 - val_binary_accuracy: 0.8923\n",
      "Epoch 58/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2724 - binary_accuracy: 0.8856 - val_loss: 0.2826 - val_binary_accuracy: 0.8889\n",
      "Epoch 59/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2631 - binary_accuracy: 0.8871 - val_loss: 0.2773 - val_binary_accuracy: 0.8965\n",
      "Epoch 60/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2652 - binary_accuracy: 0.8899 - val_loss: 0.2790 - val_binary_accuracy: 0.8957\n",
      "Epoch 61/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2661 - binary_accuracy: 0.8930 - val_loss: 0.2786 - val_binary_accuracy: 0.8889\n",
      "Epoch 62/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2601 - binary_accuracy: 0.8935 - val_loss: 0.2790 - val_binary_accuracy: 0.8948\n",
      "Epoch 63/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2681 - binary_accuracy: 0.8856 - val_loss: 0.2757 - val_binary_accuracy: 0.8914\n",
      "Epoch 64/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2603 - binary_accuracy: 0.8907 - val_loss: 0.2813 - val_binary_accuracy: 0.8889\n",
      "Epoch 65/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2591 - binary_accuracy: 0.8918 - val_loss: 0.2849 - val_binary_accuracy: 0.8931\n",
      "Epoch 66/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2524 - binary_accuracy: 0.8992 - val_loss: 0.2834 - val_binary_accuracy: 0.8914\n",
      "Epoch 67/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2566 - binary_accuracy: 0.8971 - val_loss: 0.2788 - val_binary_accuracy: 0.8931\n",
      "Epoch 68/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2591 - binary_accuracy: 0.8962 - val_loss: 0.2737 - val_binary_accuracy: 0.8965\n",
      "Epoch 69/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2534 - binary_accuracy: 0.8966 - val_loss: 0.2755 - val_binary_accuracy: 0.8940\n",
      "Epoch 70/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2565 - binary_accuracy: 0.8930 - val_loss: 0.2787 - val_binary_accuracy: 0.8931\n",
      "Epoch 71/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2528 - binary_accuracy: 0.8983 - val_loss: 0.2781 - val_binary_accuracy: 0.8923\n",
      "Epoch 72/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2436 - binary_accuracy: 0.9034 - val_loss: 0.2765 - val_binary_accuracy: 0.8923\n",
      "Epoch 73/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2512 - binary_accuracy: 0.8990 - val_loss: 0.2754 - val_binary_accuracy: 0.8940\n",
      "Epoch 74/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2573 - binary_accuracy: 0.8986 - val_loss: 0.2718 - val_binary_accuracy: 0.8957\n",
      "Epoch 75/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2475 - binary_accuracy: 0.8986 - val_loss: 0.2752 - val_binary_accuracy: 0.8931\n",
      "Epoch 76/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2448 - binary_accuracy: 0.9028 - val_loss: 0.2710 - val_binary_accuracy: 0.9016\n",
      "Epoch 77/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2428 - binary_accuracy: 0.9013 - val_loss: 0.2725 - val_binary_accuracy: 0.8923\n",
      "Epoch 78/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2464 - binary_accuracy: 0.8992 - val_loss: 0.2787 - val_binary_accuracy: 0.8957\n",
      "Epoch 79/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2455 - binary_accuracy: 0.9034 - val_loss: 0.2809 - val_binary_accuracy: 0.8923\n",
      "Epoch 80/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2415 - binary_accuracy: 0.8990 - val_loss: 0.2766 - val_binary_accuracy: 0.8999\n",
      "Epoch 81/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2489 - binary_accuracy: 0.8996 - val_loss: 0.2754 - val_binary_accuracy: 0.8948\n",
      "Epoch 82/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2484 - binary_accuracy: 0.8990 - val_loss: 0.2686 - val_binary_accuracy: 0.8991\n",
      "Epoch 83/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2386 - binary_accuracy: 0.9051 - val_loss: 0.2690 - val_binary_accuracy: 0.8914\n",
      "Epoch 84/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2383 - binary_accuracy: 0.9034 - val_loss: 0.2692 - val_binary_accuracy: 0.8957\n",
      "Epoch 85/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2381 - binary_accuracy: 0.9015 - val_loss: 0.2726 - val_binary_accuracy: 0.8906\n",
      "Epoch 86/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2360 - binary_accuracy: 0.9024 - val_loss: 0.2756 - val_binary_accuracy: 0.8914\n",
      "Epoch 87/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2336 - binary_accuracy: 0.9117 - val_loss: 0.2812 - val_binary_accuracy: 0.8931\n",
      "Epoch 88/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2251 - binary_accuracy: 0.9041 - val_loss: 0.2773 - val_binary_accuracy: 0.8940\n",
      "Epoch 89/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2348 - binary_accuracy: 0.9041 - val_loss: 0.2734 - val_binary_accuracy: 0.8965\n",
      "Epoch 90/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2459 - binary_accuracy: 0.9011 - val_loss: 0.2725 - val_binary_accuracy: 0.8931\n",
      "Epoch 91/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2345 - binary_accuracy: 0.9077 - val_loss: 0.2654 - val_binary_accuracy: 0.8982\n",
      "Epoch 92/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2408 - binary_accuracy: 0.8983 - val_loss: 0.2694 - val_binary_accuracy: 0.8957\n",
      "Epoch 93/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2352 - binary_accuracy: 0.9073 - val_loss: 0.2691 - val_binary_accuracy: 0.8931\n",
      "Epoch 94/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2371 - binary_accuracy: 0.9060 - val_loss: 0.2697 - val_binary_accuracy: 0.8957\n",
      "Epoch 95/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2336 - binary_accuracy: 0.9066 - val_loss: 0.2697 - val_binary_accuracy: 0.8982\n",
      "Epoch 96/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2283 - binary_accuracy: 0.9094 - val_loss: 0.2704 - val_binary_accuracy: 0.8923\n",
      "Epoch 97/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2323 - binary_accuracy: 0.9102 - val_loss: 0.2691 - val_binary_accuracy: 0.8948\n",
      "Epoch 98/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2319 - binary_accuracy: 0.9111 - val_loss: 0.2735 - val_binary_accuracy: 0.8957\n",
      "Epoch 99/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2243 - binary_accuracy: 0.9085 - val_loss: 0.2667 - val_binary_accuracy: 0.8974\n",
      "Epoch 100/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2339 - binary_accuracy: 0.9058 - val_loss: 0.2698 - val_binary_accuracy: 0.8982\n",
      "Epoch 101/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2334 - binary_accuracy: 0.9064 - val_loss: 0.2706 - val_binary_accuracy: 0.8965\n",
      "Epoch 101: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8307e5970>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.summary()\n",
    "classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.8993526832324076\n",
      "F1 Score: 0.8856193640246796\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(xtest_ac)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "accuracy = accuracy_score(ytest, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(ytest, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "Model: \"classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_71 (InputLayer)       [(None, 38)]              0         \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 64)                2496      \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_120 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,833\n",
      "Trainable params: 8,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/279 [============================>.] - ETA: 0s - loss: 0.5647 - binary_accuracy: 0.7115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - 4s 6ms/step - loss: 0.5645 - binary_accuracy: 0.7115 - val_loss: 0.4816 - val_binary_accuracy: 0.7817\n",
      "Epoch 2/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.4834 - binary_accuracy: 0.7695 - val_loss: 0.4308 - val_binary_accuracy: 0.8068\n",
      "Epoch 3/200\n",
      "279/279 [==============================] - 2s 7ms/step - loss: 0.4521 - binary_accuracy: 0.7987 - val_loss: 0.4123 - val_binary_accuracy: 0.8212\n",
      "Epoch 4/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.4277 - binary_accuracy: 0.8059 - val_loss: 0.3932 - val_binary_accuracy: 0.8248\n",
      "Epoch 5/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.4070 - binary_accuracy: 0.8198 - val_loss: 0.3844 - val_binary_accuracy: 0.8302\n",
      "Epoch 6/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.3922 - binary_accuracy: 0.8290 - val_loss: 0.3734 - val_binary_accuracy: 0.8446\n",
      "Epoch 7/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3871 - binary_accuracy: 0.8331 - val_loss: 0.3676 - val_binary_accuracy: 0.8491\n",
      "Epoch 8/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3785 - binary_accuracy: 0.8351 - val_loss: 0.3602 - val_binary_accuracy: 0.8428\n",
      "Epoch 9/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3725 - binary_accuracy: 0.8398 - val_loss: 0.3651 - val_binary_accuracy: 0.8419\n",
      "Epoch 10/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.3663 - binary_accuracy: 0.8403 - val_loss: 0.3499 - val_binary_accuracy: 0.8518\n",
      "Epoch 11/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3592 - binary_accuracy: 0.8441 - val_loss: 0.3557 - val_binary_accuracy: 0.8419\n",
      "Epoch 12/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3558 - binary_accuracy: 0.8479 - val_loss: 0.3451 - val_binary_accuracy: 0.8473\n",
      "Epoch 13/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3533 - binary_accuracy: 0.8479 - val_loss: 0.3581 - val_binary_accuracy: 0.8500\n",
      "Epoch 14/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3495 - binary_accuracy: 0.8528 - val_loss: 0.3502 - val_binary_accuracy: 0.8527\n",
      "Epoch 15/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.3404 - binary_accuracy: 0.8551 - val_loss: 0.3445 - val_binary_accuracy: 0.8518\n",
      "Epoch 16/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3424 - binary_accuracy: 0.8533 - val_loss: 0.3358 - val_binary_accuracy: 0.8562\n",
      "Epoch 17/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3414 - binary_accuracy: 0.8603 - val_loss: 0.3328 - val_binary_accuracy: 0.8544\n",
      "Epoch 18/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3406 - binary_accuracy: 0.8558 - val_loss: 0.3329 - val_binary_accuracy: 0.8571\n",
      "Epoch 19/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3302 - binary_accuracy: 0.8560 - val_loss: 0.3284 - val_binary_accuracy: 0.8634\n",
      "Epoch 20/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3283 - binary_accuracy: 0.8591 - val_loss: 0.3319 - val_binary_accuracy: 0.8634\n",
      "Epoch 21/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3282 - binary_accuracy: 0.8614 - val_loss: 0.3306 - val_binary_accuracy: 0.8616\n",
      "Epoch 22/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3258 - binary_accuracy: 0.8582 - val_loss: 0.3347 - val_binary_accuracy: 0.8643\n",
      "Epoch 23/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3145 - binary_accuracy: 0.8690 - val_loss: 0.3331 - val_binary_accuracy: 0.8571\n",
      "Epoch 24/200\n",
      "279/279 [==============================] - 2s 7ms/step - loss: 0.3187 - binary_accuracy: 0.8634 - val_loss: 0.3367 - val_binary_accuracy: 0.8571\n",
      "Epoch 25/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3140 - binary_accuracy: 0.8683 - val_loss: 0.3289 - val_binary_accuracy: 0.8616\n",
      "Epoch 26/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3073 - binary_accuracy: 0.8715 - val_loss: 0.3294 - val_binary_accuracy: 0.8652\n",
      "Epoch 27/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3092 - binary_accuracy: 0.8672 - val_loss: 0.3283 - val_binary_accuracy: 0.8661\n",
      "Epoch 28/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3066 - binary_accuracy: 0.8699 - val_loss: 0.3289 - val_binary_accuracy: 0.8679\n",
      "Epoch 29/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3032 - binary_accuracy: 0.8760 - val_loss: 0.3293 - val_binary_accuracy: 0.8706\n",
      "Epoch 30/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3098 - binary_accuracy: 0.8713 - val_loss: 0.3322 - val_binary_accuracy: 0.8625\n",
      "Epoch 31/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2984 - binary_accuracy: 0.8719 - val_loss: 0.3306 - val_binary_accuracy: 0.8643\n",
      "Epoch 32/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3015 - binary_accuracy: 0.8735 - val_loss: 0.3294 - val_binary_accuracy: 0.8679\n",
      "Epoch 33/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3031 - binary_accuracy: 0.8704 - val_loss: 0.3277 - val_binary_accuracy: 0.8679\n",
      "Epoch 34/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2967 - binary_accuracy: 0.8769 - val_loss: 0.3289 - val_binary_accuracy: 0.8661\n",
      "Epoch 35/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2946 - binary_accuracy: 0.8771 - val_loss: 0.3205 - val_binary_accuracy: 0.8715\n",
      "Epoch 36/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2933 - binary_accuracy: 0.8782 - val_loss: 0.3245 - val_binary_accuracy: 0.8679\n",
      "Epoch 37/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2890 - binary_accuracy: 0.8789 - val_loss: 0.3197 - val_binary_accuracy: 0.8742\n",
      "Epoch 38/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2905 - binary_accuracy: 0.8816 - val_loss: 0.3212 - val_binary_accuracy: 0.8643\n",
      "Epoch 39/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2881 - binary_accuracy: 0.8753 - val_loss: 0.3250 - val_binary_accuracy: 0.8724\n",
      "Epoch 40/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2750 - binary_accuracy: 0.8829 - val_loss: 0.3171 - val_binary_accuracy: 0.8760\n",
      "Epoch 41/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2822 - binary_accuracy: 0.8823 - val_loss: 0.3195 - val_binary_accuracy: 0.8652\n",
      "Epoch 42/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2900 - binary_accuracy: 0.8838 - val_loss: 0.3174 - val_binary_accuracy: 0.8742\n",
      "Epoch 43/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2724 - binary_accuracy: 0.8879 - val_loss: 0.3220 - val_binary_accuracy: 0.8769\n",
      "Epoch 44/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2730 - binary_accuracy: 0.8874 - val_loss: 0.3150 - val_binary_accuracy: 0.8796\n",
      "Epoch 45/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2820 - binary_accuracy: 0.8879 - val_loss: 0.3134 - val_binary_accuracy: 0.8760\n",
      "Epoch 46/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2792 - binary_accuracy: 0.8829 - val_loss: 0.3163 - val_binary_accuracy: 0.8688\n",
      "Epoch 47/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2737 - binary_accuracy: 0.8825 - val_loss: 0.3188 - val_binary_accuracy: 0.8742\n",
      "Epoch 48/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2774 - binary_accuracy: 0.8814 - val_loss: 0.3155 - val_binary_accuracy: 0.8751\n",
      "Epoch 49/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2773 - binary_accuracy: 0.8890 - val_loss: 0.3152 - val_binary_accuracy: 0.8787\n",
      "Epoch 50/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2724 - binary_accuracy: 0.8861 - val_loss: 0.3164 - val_binary_accuracy: 0.8832\n",
      "Epoch 51/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2790 - binary_accuracy: 0.8854 - val_loss: 0.3182 - val_binary_accuracy: 0.8751\n",
      "Epoch 52/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2675 - binary_accuracy: 0.8942 - val_loss: 0.3192 - val_binary_accuracy: 0.8751\n",
      "Epoch 53/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2684 - binary_accuracy: 0.8886 - val_loss: 0.3214 - val_binary_accuracy: 0.8661\n",
      "Epoch 54/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2702 - binary_accuracy: 0.8852 - val_loss: 0.3181 - val_binary_accuracy: 0.8742\n",
      "Epoch 55/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2638 - binary_accuracy: 0.8886 - val_loss: 0.3186 - val_binary_accuracy: 0.8769\n",
      "Epoch 55: early stopping\n",
      "107/107 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8786808009422851\n",
      "F1 Score: 0.8598639455782313\n"
     ]
    }
   ],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'ITA_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "featType = 1; #Acoustic or Acoustic+context\n",
    "if featType == 1:\n",
    "  original_dim = 19\n",
    "else:\n",
    "  original_dim = 38\n",
    "\n",
    "# print('Classification with::::::',os.path.basename(filee))\n",
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "# print('test file:::::::',os.path.basename(test_path))\n",
    "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
    "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
    "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "\n",
    "\n",
    "xtest_a = normalization(xtest)\n",
    "xtest_ac = normalization(xtest1)\n",
    "xtrain = normalization(xtrain)\n",
    "xtrain1 = normalization(xtrain1)\n",
    "\n",
    "woPP=[]; wPP=[]\n",
    "input_shape1 = (19,1)\n",
    "input_shape2 = (38,1)\n",
    "temperature = 0.03\n",
    "learning_rate=0.001\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# Splitting xtrain and ytrain into training and validation sets\n",
    "xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting xtrain1 and ytrain1 into training and validation sets\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)\n",
    "def create_classifier(hidden_units):\n",
    "\n",
    "    # for layer in encoder.layers:\n",
    "    #     layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # features = encoder(inputs)\n",
    "    features=layers.Dense(hidden_units, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "units=64\n",
    "\n",
    "classifier = create_classifier(units)\n",
    "\n",
    "# history = classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "classifier.summary()\n",
    "classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(xtest_ac)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "accuracy = accuracy_score(ytest, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(ytest, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "data = scipy.io.loadmat(filee)\n",
    "print(data.keys())\n",
    "AF = data['AF']\n",
    "x1 = AF[:-2]\n",
    "y = AF[-2]\n",
    "w = AF[-1]\n",
    "x2 = data['CF']\n",
    "x = np.concatenate((x1, x2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'ITA_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "data = scipy.io.loadmat(filee)\n",
    "print(data.keys())\n",
    "AF = data['AF']\n",
    "x3 = AF[:-2]\n",
    "y1 = AF[-2]\n",
    "w1 = AF[-1]\n",
    "x4 = data['CF']\n",
    "x5 = np.concatenate((x3, x4), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfinal = np.concatenate((x, x5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 11455)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xfinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfinal=np.concatenate((y, y1), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3831474976.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(x(,:5564))\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(x(,:5564))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices_arr1 = [0, 1, 4, 5, 7]\n",
    "\n",
    "# # Create a mask to select the specified features from arr1\n",
    "# mask_arr1 = np.zeros_like(x, dtype=bool)\n",
    "# mask_arr1[:, indices_arr1] = True\n",
    "\n",
    "# # Create a mask to select the remaining features from arr2\n",
    "# mask_arr2 = ~mask_arr1\n",
    "\n",
    "# # Create the new array by combining selected features from arr1 and arr2\n",
    "# new_array = np.where(mask_arr1, x, x5)\n",
    "\n",
    "# # Print the shape of the new array\n",
    "# print(new_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xfinal=np.concatenate((x, x5), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 11455)\n"
     ]
    }
   ],
   "source": [
    "# print(xfinal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yfinal=np.concatenate((y,y1),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11455,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yfinal.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis1: axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[39m.\u001b[39;49mswapaxes(yfinal,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mswapaxes\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:594\u001b[0m, in \u001b[0;36mswapaxes\u001b[0;34m(a, axis1, axis2)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_swapaxes_dispatcher)\n\u001b[1;32m    551\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mswapaxes\u001b[39m(a, axis1, axis2):\n\u001b[1;32m    552\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[39m    Interchange two axes of an array.\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m \n\u001b[1;32m    593\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mswapaxes\u001b[39;49m\u001b[39m'\u001b[39;49m, axis1, axis2)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[0;31mAxisError\u001b[0m: axis1: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "np.swapaxes(yfinal,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06249927, 0.05942424, 0.03385573, ..., 0.18293962, 0.15400188,\n",
       "        0.0673501 ],\n",
       "       [0.07567894, 0.20975891, 0.03232541, ..., 0.15583806, 0.14611988,\n",
       "        0.06193826],\n",
       "       [0.05082466, 0.07201191, 0.02582735, ..., 0.12746012, 0.13397337,\n",
       "        0.05902464],\n",
       "       ...,\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [38, 11455]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m xtra_a, xval_a, ytra_a, yval_a \u001b[39m=\u001b[39m train_test_split(xtrain, ytrain, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39m# Splitting xtrain1 and ytrain1 into training and validation sets\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m xtra_ac, xval_ac, ytra_ac, yval_ac \u001b[39m=\u001b[39m train_test_split(xtrain1, yfinal, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_classifier\u001b[39m(hidden_units):\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m     \u001b[39m# for layer in encoder.layers:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[39m#     layer.trainable = trainable\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     inputs \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39minput_shape)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2559\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2556\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[1;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m   2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[1;32m   2564\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[0;32m--> 443\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[1;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [38, 11455]"
     ]
    }
   ],
   "source": [
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "# print('test file:::::::',os.path.basename(test_path))\n",
    "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
    "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
    "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "\n",
    "\n",
    "xtest_a = normalization(xtest)\n",
    "xtest_ac = normalization(xtest1)\n",
    "xtrain = normalization(xtrain)\n",
    "xtrain1 = normalization(xfinal)\n",
    "\n",
    "woPP=[]; wPP=[]\n",
    "input_shape1 = (19,1)\n",
    "input_shape2 = (38,1)\n",
    "temperature = 0.03\n",
    "learning_rate=0.001\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# Splitting xtrain and ytrain into training and validation sets\n",
    "xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting xtrain1 and ytrain1 into training and validation sets\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, yfinal, test_size=0.2, random_state=42)\n",
    "def create_classifier(hidden_units):\n",
    "\n",
    "    # for layer in encoder.layers:\n",
    "    #     layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # features = encoder(inputs)\n",
    "    features=layers.Dense(hidden_units, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "units=64\n",
    "\n",
    "classifier = create_classifier(units)\n",
    "\n",
    "# history = classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "classifier.summary()\n",
    "classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(xtest_ac)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "accuracy = accuracy_score(ytest, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(ytest, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
