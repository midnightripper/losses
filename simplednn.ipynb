{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 06:00:12.316594: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/project/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2023-06-08 06:00:16.789605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:16.817525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:16.817804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:16.822966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:16.823378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:16.823674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:18.192218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:18.192478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:18.192634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 06:00:18.192763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4412 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "class Encoder1(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Encoder1, self).__init__()\n",
    "    self.latent_dim = latent_dim \n",
    "    inputs = keras.Input(19,)\n",
    "    outputs = inputs  \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      inputs,\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.Dropout(0.3),\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(32, activation='relu'),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    return encoded\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Reshape\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint ,EarlyStopping\n",
    "temperature = 0.03\n",
    "learning_rate=0.001\n",
    "\n",
    "def dataloader(path, featType):\n",
    "    \"\"\"\n",
    "    Load data from a MATLAB file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the MATLAB file.\n",
    "        featType (int): Type of features to load.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Tuple containing input features, labels, weights, and additional information.\n",
    "    \"\"\"\n",
    "    data = scipy.io.loadmat(path)\n",
    "    print(data.keys())\n",
    "\n",
    "    AF = data['AF']\n",
    "    x1 = AF[:-2]\n",
    "    y = AF[-2]\n",
    "    w = AF[-1]\n",
    "\n",
    "    if featType == 1:\n",
    "        x = x1\n",
    "    else:\n",
    "        x2 = data['CF']\n",
    "        x = np.concatenate((x1, x2), axis=0)\n",
    "    return x.T, y.T, w.T, data['CF_info']\n",
    "\n",
    "def calculate_accuracy(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy between two arrays.\n",
    "\n",
    "    Args:\n",
    "        arr1 (array): First array.\n",
    "        arr2 (array): Second array.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy between the two arrays.\n",
    "    \"\"\"\n",
    "    count = sum(1 for itr1, itr2 in zip(arr1, arr2) if itr1 == itr2)\n",
    "    return count / len(arr1)\n",
    "\n",
    "def normalization(feats):\n",
    "\n",
    "    \"\"\"\n",
    "    Normalize the input features using standard scaling.\n",
    "\n",
    "    Args:\n",
    "        feats (array): Input features.\n",
    "\n",
    "    Returns:\n",
    "        array: Normalized features.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(feats)\n",
    "    scaler = StandardScaler()\n",
    "    x_new = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return x_new\n",
    "\n",
    "def make_partitions(arr_words, arr_labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Create partitions based on word boundaries and labels.\n",
    "\n",
    "    Args:\n",
    "        arr_words (array): Array of words.\n",
    "        arr_labels (array): Array of labels.\n",
    "\n",
    "    Returns:\n",
    "        array: Partitions based on word boundaries and labels.\n",
    "    \"\"\"\n",
    "    v = []\n",
    "    temp = []\n",
    "\n",
    "    for i in range(len(arr_words) - 1):\n",
    "        word = arr_words[i]\n",
    "        next_word = arr_words[i + 1]\n",
    "        temp.append(arr_labels[i])\n",
    "\n",
    "        if word != next_word or i == len(arr_words) - 2:\n",
    "            if i == len(arr_words) - 2:\n",
    "                temp.append(arr_labels[i + 1])\n",
    "\n",
    "            numpy_temp = np.array(temp)\n",
    "            temp_max = np.amax(numpy_temp)\n",
    "            numpy_temp = np.divide(numpy_temp, temp_max)\n",
    "            v = np.concatenate((v, numpy_temp), axis=None)\n",
    "            temp.clear()\n",
    "\n",
    "    v1 = [1 if i == 1 else 0 for i in v]\n",
    "    return v1\n",
    "    \n",
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "featType = 1; #Acoustic or Acoustic+context\n",
    "if featType == 1:\n",
    "  original_dim = 19\n",
    "else:\n",
    "  original_dim = 38\n",
    "\n",
    "\n",
    "# print('Classification with::::::',os.path.basename(filee))\n",
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "# print('test file:::::::',os.path.basename(test_path))\n",
    "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
    "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
    "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "\n",
    "xtest_a = normalization(xtest)\n",
    "xtest_ac = normalization(xtest1)\n",
    "xtrain = normalization(xtrain)\n",
    "xtrain1 = normalization(xtrain1)\n",
    "\n",
    "woPP=[]; wPP=[]\n",
    "input_shape1 = (19,1)\n",
    "input_shape2 = (38,1)\n",
    "temperature = 0.03\n",
    "learning_rate=0.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# Splitting xtrain and ytrain into training and validation sets\n",
    "xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting xtrain1 and ytrain1 into training and validation sets\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "batch_size = 16\n",
    "hidden_units = 64\n",
    "projection_units = 128\n",
    "num_epochs = 200\n",
    "dropout_rate = 0.3\n",
    "num_classes = 2\n",
    "input_shape1 = (19,)\n",
    "input_shape = (38,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(hidden_units):\n",
    "\n",
    "    # for layer in encoder.layers:\n",
    "    #     layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # features = encoder(inputs)\n",
    "    features=layers.Dense(hidden_units, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "units=64\n",
    "\n",
    "classifier = create_classifier(units)\n",
    "\n",
    "# history = classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 38)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2496      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,833\n",
      "Trainable params: 8,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-06-08 06:01:23.037078: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x21750420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-08 06:01:23.037119: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2023-06-08 06:01:23.078425: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-08 06:01:23.465049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-08 06:01:23.808974: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/295 [============================>.] - ETA: 0s - loss: 0.6032 - binary_accuracy: 0.6685"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 7s 7ms/step - loss: 0.6032 - binary_accuracy: 0.6689 - val_loss: 0.5147 - val_binary_accuracy: 0.7447\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.5209 - binary_accuracy: 0.7572 - val_loss: 0.4793 - val_binary_accuracy: 0.7795\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4922 - binary_accuracy: 0.7691 - val_loss: 0.4565 - val_binary_accuracy: 0.8007\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4714 - binary_accuracy: 0.7782 - val_loss: 0.4441 - val_binary_accuracy: 0.8066\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4507 - binary_accuracy: 0.7975 - val_loss: 0.4247 - val_binary_accuracy: 0.8185\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4310 - binary_accuracy: 0.8092 - val_loss: 0.4108 - val_binary_accuracy: 0.8253\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4238 - binary_accuracy: 0.8128 - val_loss: 0.4002 - val_binary_accuracy: 0.8278\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4124 - binary_accuracy: 0.8185 - val_loss: 0.3871 - val_binary_accuracy: 0.8329\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3977 - binary_accuracy: 0.8319 - val_loss: 0.3806 - val_binary_accuracy: 0.8388\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3939 - binary_accuracy: 0.8283 - val_loss: 0.3638 - val_binary_accuracy: 0.8524\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3825 - binary_accuracy: 0.8323 - val_loss: 0.3613 - val_binary_accuracy: 0.8465\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3865 - binary_accuracy: 0.8321 - val_loss: 0.3584 - val_binary_accuracy: 0.8507\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3780 - binary_accuracy: 0.8321 - val_loss: 0.3516 - val_binary_accuracy: 0.8550\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3703 - binary_accuracy: 0.8423 - val_loss: 0.3492 - val_binary_accuracy: 0.8558\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3646 - binary_accuracy: 0.8466 - val_loss: 0.3497 - val_binary_accuracy: 0.8609\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3564 - binary_accuracy: 0.8444 - val_loss: 0.3395 - val_binary_accuracy: 0.8626\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3542 - binary_accuracy: 0.8457 - val_loss: 0.3374 - val_binary_accuracy: 0.8626\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.3564 - binary_accuracy: 0.8453 - val_loss: 0.3348 - val_binary_accuracy: 0.8668\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3434 - binary_accuracy: 0.8551 - val_loss: 0.3282 - val_binary_accuracy: 0.8736\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3420 - binary_accuracy: 0.8538 - val_loss: 0.3330 - val_binary_accuracy: 0.8736\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3369 - binary_accuracy: 0.8521 - val_loss: 0.3243 - val_binary_accuracy: 0.8702\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3310 - binary_accuracy: 0.8629 - val_loss: 0.3202 - val_binary_accuracy: 0.8796\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3278 - binary_accuracy: 0.8597 - val_loss: 0.3260 - val_binary_accuracy: 0.8702\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3251 - binary_accuracy: 0.8599 - val_loss: 0.3201 - val_binary_accuracy: 0.8719\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3197 - binary_accuracy: 0.8633 - val_loss: 0.3195 - val_binary_accuracy: 0.8728\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3250 - binary_accuracy: 0.8612 - val_loss: 0.3190 - val_binary_accuracy: 0.8711\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3202 - binary_accuracy: 0.8625 - val_loss: 0.3132 - val_binary_accuracy: 0.8770\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3171 - binary_accuracy: 0.8684 - val_loss: 0.3104 - val_binary_accuracy: 0.8787\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3113 - binary_accuracy: 0.8686 - val_loss: 0.3084 - val_binary_accuracy: 0.8846\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3108 - binary_accuracy: 0.8705 - val_loss: 0.3101 - val_binary_accuracy: 0.8787\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3239 - binary_accuracy: 0.8657 - val_loss: 0.3106 - val_binary_accuracy: 0.8796\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3060 - binary_accuracy: 0.8680 - val_loss: 0.3091 - val_binary_accuracy: 0.8770\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3045 - binary_accuracy: 0.8733 - val_loss: 0.3077 - val_binary_accuracy: 0.8872\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3013 - binary_accuracy: 0.8720 - val_loss: 0.3047 - val_binary_accuracy: 0.8821\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3105 - binary_accuracy: 0.8710 - val_loss: 0.3075 - val_binary_accuracy: 0.8838\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3042 - binary_accuracy: 0.8697 - val_loss: 0.3082 - val_binary_accuracy: 0.8813\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2846 - binary_accuracy: 0.8848 - val_loss: 0.3071 - val_binary_accuracy: 0.8804\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2966 - binary_accuracy: 0.8792 - val_loss: 0.3045 - val_binary_accuracy: 0.8821\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2951 - binary_accuracy: 0.8788 - val_loss: 0.3057 - val_binary_accuracy: 0.8804\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2931 - binary_accuracy: 0.8782 - val_loss: 0.3038 - val_binary_accuracy: 0.8813\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2867 - binary_accuracy: 0.8837 - val_loss: 0.2934 - val_binary_accuracy: 0.8906\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2911 - binary_accuracy: 0.8841 - val_loss: 0.2937 - val_binary_accuracy: 0.8897\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2940 - binary_accuracy: 0.8773 - val_loss: 0.2920 - val_binary_accuracy: 0.8889\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2817 - binary_accuracy: 0.8852 - val_loss: 0.2865 - val_binary_accuracy: 0.8914\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2838 - binary_accuracy: 0.8805 - val_loss: 0.2891 - val_binary_accuracy: 0.8914\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2756 - binary_accuracy: 0.8850 - val_loss: 0.2922 - val_binary_accuracy: 0.8838\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2798 - binary_accuracy: 0.8826 - val_loss: 0.2908 - val_binary_accuracy: 0.8855\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2752 - binary_accuracy: 0.8858 - val_loss: 0.2869 - val_binary_accuracy: 0.8897\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2726 - binary_accuracy: 0.8905 - val_loss: 0.2861 - val_binary_accuracy: 0.8923\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2801 - binary_accuracy: 0.8818 - val_loss: 0.2914 - val_binary_accuracy: 0.8914\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2723 - binary_accuracy: 0.8892 - val_loss: 0.2842 - val_binary_accuracy: 0.8965\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2711 - binary_accuracy: 0.8907 - val_loss: 0.2847 - val_binary_accuracy: 0.8923\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.2769 - binary_accuracy: 0.8837 - val_loss: 0.2878 - val_binary_accuracy: 0.8863\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2671 - binary_accuracy: 0.8960 - val_loss: 0.2804 - val_binary_accuracy: 0.8974\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2665 - binary_accuracy: 0.8962 - val_loss: 0.2789 - val_binary_accuracy: 0.8991\n",
      "Epoch 56/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2680 - binary_accuracy: 0.8918 - val_loss: 0.2848 - val_binary_accuracy: 0.8914\n",
      "Epoch 57/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2644 - binary_accuracy: 0.8901 - val_loss: 0.2858 - val_binary_accuracy: 0.8923\n",
      "Epoch 58/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2724 - binary_accuracy: 0.8856 - val_loss: 0.2826 - val_binary_accuracy: 0.8889\n",
      "Epoch 59/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2631 - binary_accuracy: 0.8871 - val_loss: 0.2773 - val_binary_accuracy: 0.8965\n",
      "Epoch 60/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2652 - binary_accuracy: 0.8899 - val_loss: 0.2790 - val_binary_accuracy: 0.8957\n",
      "Epoch 61/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2661 - binary_accuracy: 0.8930 - val_loss: 0.2786 - val_binary_accuracy: 0.8889\n",
      "Epoch 62/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2601 - binary_accuracy: 0.8935 - val_loss: 0.2790 - val_binary_accuracy: 0.8948\n",
      "Epoch 63/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2681 - binary_accuracy: 0.8856 - val_loss: 0.2757 - val_binary_accuracy: 0.8914\n",
      "Epoch 64/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2603 - binary_accuracy: 0.8907 - val_loss: 0.2813 - val_binary_accuracy: 0.8889\n",
      "Epoch 65/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2591 - binary_accuracy: 0.8918 - val_loss: 0.2849 - val_binary_accuracy: 0.8931\n",
      "Epoch 66/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2524 - binary_accuracy: 0.8992 - val_loss: 0.2834 - val_binary_accuracy: 0.8914\n",
      "Epoch 67/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2566 - binary_accuracy: 0.8971 - val_loss: 0.2788 - val_binary_accuracy: 0.8931\n",
      "Epoch 68/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2591 - binary_accuracy: 0.8962 - val_loss: 0.2737 - val_binary_accuracy: 0.8965\n",
      "Epoch 69/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2534 - binary_accuracy: 0.8966 - val_loss: 0.2755 - val_binary_accuracy: 0.8940\n",
      "Epoch 70/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2565 - binary_accuracy: 0.8930 - val_loss: 0.2787 - val_binary_accuracy: 0.8931\n",
      "Epoch 71/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2528 - binary_accuracy: 0.8983 - val_loss: 0.2781 - val_binary_accuracy: 0.8923\n",
      "Epoch 72/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2436 - binary_accuracy: 0.9034 - val_loss: 0.2765 - val_binary_accuracy: 0.8923\n",
      "Epoch 73/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2512 - binary_accuracy: 0.8990 - val_loss: 0.2754 - val_binary_accuracy: 0.8940\n",
      "Epoch 74/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2573 - binary_accuracy: 0.8986 - val_loss: 0.2718 - val_binary_accuracy: 0.8957\n",
      "Epoch 75/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2475 - binary_accuracy: 0.8986 - val_loss: 0.2752 - val_binary_accuracy: 0.8931\n",
      "Epoch 76/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2448 - binary_accuracy: 0.9028 - val_loss: 0.2710 - val_binary_accuracy: 0.9016\n",
      "Epoch 77/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2428 - binary_accuracy: 0.9013 - val_loss: 0.2725 - val_binary_accuracy: 0.8923\n",
      "Epoch 78/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2464 - binary_accuracy: 0.8992 - val_loss: 0.2787 - val_binary_accuracy: 0.8957\n",
      "Epoch 79/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2455 - binary_accuracy: 0.9034 - val_loss: 0.2809 - val_binary_accuracy: 0.8923\n",
      "Epoch 80/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2415 - binary_accuracy: 0.8990 - val_loss: 0.2766 - val_binary_accuracy: 0.8999\n",
      "Epoch 81/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2489 - binary_accuracy: 0.8996 - val_loss: 0.2754 - val_binary_accuracy: 0.8948\n",
      "Epoch 82/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2484 - binary_accuracy: 0.8990 - val_loss: 0.2686 - val_binary_accuracy: 0.8991\n",
      "Epoch 83/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2386 - binary_accuracy: 0.9051 - val_loss: 0.2690 - val_binary_accuracy: 0.8914\n",
      "Epoch 84/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2383 - binary_accuracy: 0.9034 - val_loss: 0.2692 - val_binary_accuracy: 0.8957\n",
      "Epoch 85/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2381 - binary_accuracy: 0.9015 - val_loss: 0.2726 - val_binary_accuracy: 0.8906\n",
      "Epoch 86/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2360 - binary_accuracy: 0.9024 - val_loss: 0.2756 - val_binary_accuracy: 0.8914\n",
      "Epoch 87/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2336 - binary_accuracy: 0.9117 - val_loss: 0.2812 - val_binary_accuracy: 0.8931\n",
      "Epoch 88/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2251 - binary_accuracy: 0.9041 - val_loss: 0.2773 - val_binary_accuracy: 0.8940\n",
      "Epoch 89/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2348 - binary_accuracy: 0.9041 - val_loss: 0.2734 - val_binary_accuracy: 0.8965\n",
      "Epoch 90/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2459 - binary_accuracy: 0.9011 - val_loss: 0.2725 - val_binary_accuracy: 0.8931\n",
      "Epoch 91/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2345 - binary_accuracy: 0.9077 - val_loss: 0.2654 - val_binary_accuracy: 0.8982\n",
      "Epoch 92/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2408 - binary_accuracy: 0.8983 - val_loss: 0.2694 - val_binary_accuracy: 0.8957\n",
      "Epoch 93/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2352 - binary_accuracy: 0.9073 - val_loss: 0.2691 - val_binary_accuracy: 0.8931\n",
      "Epoch 94/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2371 - binary_accuracy: 0.9060 - val_loss: 0.2697 - val_binary_accuracy: 0.8957\n",
      "Epoch 95/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2336 - binary_accuracy: 0.9066 - val_loss: 0.2697 - val_binary_accuracy: 0.8982\n",
      "Epoch 96/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2283 - binary_accuracy: 0.9094 - val_loss: 0.2704 - val_binary_accuracy: 0.8923\n",
      "Epoch 97/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2323 - binary_accuracy: 0.9102 - val_loss: 0.2691 - val_binary_accuracy: 0.8948\n",
      "Epoch 98/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2319 - binary_accuracy: 0.9111 - val_loss: 0.2735 - val_binary_accuracy: 0.8957\n",
      "Epoch 99/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2243 - binary_accuracy: 0.9085 - val_loss: 0.2667 - val_binary_accuracy: 0.8974\n",
      "Epoch 100/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2339 - binary_accuracy: 0.9058 - val_loss: 0.2698 - val_binary_accuracy: 0.8982\n",
      "Epoch 101/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2334 - binary_accuracy: 0.9064 - val_loss: 0.2706 - val_binary_accuracy: 0.8965\n",
      "Epoch 101: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8307e5970>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.summary()\n",
    "classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.8993526832324076\n",
      "F1 Score: 0.8856193640246796\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(xtest_ac)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "accuracy = accuracy_score(ytest, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(ytest, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "Model: \"classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_71 (InputLayer)       [(None, 38)]              0         \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 64)                2496      \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_120 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,833\n",
      "Trainable params: 8,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/279 [============================>.] - ETA: 0s - loss: 0.5647 - binary_accuracy: 0.7115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - 4s 6ms/step - loss: 0.5645 - binary_accuracy: 0.7115 - val_loss: 0.4816 - val_binary_accuracy: 0.7817\n",
      "Epoch 2/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.4834 - binary_accuracy: 0.7695 - val_loss: 0.4308 - val_binary_accuracy: 0.8068\n",
      "Epoch 3/200\n",
      "279/279 [==============================] - 2s 7ms/step - loss: 0.4521 - binary_accuracy: 0.7987 - val_loss: 0.4123 - val_binary_accuracy: 0.8212\n",
      "Epoch 4/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.4277 - binary_accuracy: 0.8059 - val_loss: 0.3932 - val_binary_accuracy: 0.8248\n",
      "Epoch 5/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.4070 - binary_accuracy: 0.8198 - val_loss: 0.3844 - val_binary_accuracy: 0.8302\n",
      "Epoch 6/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.3922 - binary_accuracy: 0.8290 - val_loss: 0.3734 - val_binary_accuracy: 0.8446\n",
      "Epoch 7/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3871 - binary_accuracy: 0.8331 - val_loss: 0.3676 - val_binary_accuracy: 0.8491\n",
      "Epoch 8/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3785 - binary_accuracy: 0.8351 - val_loss: 0.3602 - val_binary_accuracy: 0.8428\n",
      "Epoch 9/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3725 - binary_accuracy: 0.8398 - val_loss: 0.3651 - val_binary_accuracy: 0.8419\n",
      "Epoch 10/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.3663 - binary_accuracy: 0.8403 - val_loss: 0.3499 - val_binary_accuracy: 0.8518\n",
      "Epoch 11/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3592 - binary_accuracy: 0.8441 - val_loss: 0.3557 - val_binary_accuracy: 0.8419\n",
      "Epoch 12/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3558 - binary_accuracy: 0.8479 - val_loss: 0.3451 - val_binary_accuracy: 0.8473\n",
      "Epoch 13/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3533 - binary_accuracy: 0.8479 - val_loss: 0.3581 - val_binary_accuracy: 0.8500\n",
      "Epoch 14/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3495 - binary_accuracy: 0.8528 - val_loss: 0.3502 - val_binary_accuracy: 0.8527\n",
      "Epoch 15/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.3404 - binary_accuracy: 0.8551 - val_loss: 0.3445 - val_binary_accuracy: 0.8518\n",
      "Epoch 16/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3424 - binary_accuracy: 0.8533 - val_loss: 0.3358 - val_binary_accuracy: 0.8562\n",
      "Epoch 17/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3414 - binary_accuracy: 0.8603 - val_loss: 0.3328 - val_binary_accuracy: 0.8544\n",
      "Epoch 18/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3406 - binary_accuracy: 0.8558 - val_loss: 0.3329 - val_binary_accuracy: 0.8571\n",
      "Epoch 19/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3302 - binary_accuracy: 0.8560 - val_loss: 0.3284 - val_binary_accuracy: 0.8634\n",
      "Epoch 20/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3283 - binary_accuracy: 0.8591 - val_loss: 0.3319 - val_binary_accuracy: 0.8634\n",
      "Epoch 21/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3282 - binary_accuracy: 0.8614 - val_loss: 0.3306 - val_binary_accuracy: 0.8616\n",
      "Epoch 22/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3258 - binary_accuracy: 0.8582 - val_loss: 0.3347 - val_binary_accuracy: 0.8643\n",
      "Epoch 23/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3145 - binary_accuracy: 0.8690 - val_loss: 0.3331 - val_binary_accuracy: 0.8571\n",
      "Epoch 24/200\n",
      "279/279 [==============================] - 2s 7ms/step - loss: 0.3187 - binary_accuracy: 0.8634 - val_loss: 0.3367 - val_binary_accuracy: 0.8571\n",
      "Epoch 25/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3140 - binary_accuracy: 0.8683 - val_loss: 0.3289 - val_binary_accuracy: 0.8616\n",
      "Epoch 26/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.3073 - binary_accuracy: 0.8715 - val_loss: 0.3294 - val_binary_accuracy: 0.8652\n",
      "Epoch 27/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3092 - binary_accuracy: 0.8672 - val_loss: 0.3283 - val_binary_accuracy: 0.8661\n",
      "Epoch 28/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3066 - binary_accuracy: 0.8699 - val_loss: 0.3289 - val_binary_accuracy: 0.8679\n",
      "Epoch 29/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3032 - binary_accuracy: 0.8760 - val_loss: 0.3293 - val_binary_accuracy: 0.8706\n",
      "Epoch 30/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3098 - binary_accuracy: 0.8713 - val_loss: 0.3322 - val_binary_accuracy: 0.8625\n",
      "Epoch 31/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2984 - binary_accuracy: 0.8719 - val_loss: 0.3306 - val_binary_accuracy: 0.8643\n",
      "Epoch 32/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3015 - binary_accuracy: 0.8735 - val_loss: 0.3294 - val_binary_accuracy: 0.8679\n",
      "Epoch 33/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3031 - binary_accuracy: 0.8704 - val_loss: 0.3277 - val_binary_accuracy: 0.8679\n",
      "Epoch 34/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2967 - binary_accuracy: 0.8769 - val_loss: 0.3289 - val_binary_accuracy: 0.8661\n",
      "Epoch 35/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2946 - binary_accuracy: 0.8771 - val_loss: 0.3205 - val_binary_accuracy: 0.8715\n",
      "Epoch 36/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2933 - binary_accuracy: 0.8782 - val_loss: 0.3245 - val_binary_accuracy: 0.8679\n",
      "Epoch 37/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2890 - binary_accuracy: 0.8789 - val_loss: 0.3197 - val_binary_accuracy: 0.8742\n",
      "Epoch 38/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2905 - binary_accuracy: 0.8816 - val_loss: 0.3212 - val_binary_accuracy: 0.8643\n",
      "Epoch 39/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2881 - binary_accuracy: 0.8753 - val_loss: 0.3250 - val_binary_accuracy: 0.8724\n",
      "Epoch 40/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2750 - binary_accuracy: 0.8829 - val_loss: 0.3171 - val_binary_accuracy: 0.8760\n",
      "Epoch 41/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2822 - binary_accuracy: 0.8823 - val_loss: 0.3195 - val_binary_accuracy: 0.8652\n",
      "Epoch 42/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2900 - binary_accuracy: 0.8838 - val_loss: 0.3174 - val_binary_accuracy: 0.8742\n",
      "Epoch 43/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2724 - binary_accuracy: 0.8879 - val_loss: 0.3220 - val_binary_accuracy: 0.8769\n",
      "Epoch 44/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2730 - binary_accuracy: 0.8874 - val_loss: 0.3150 - val_binary_accuracy: 0.8796\n",
      "Epoch 45/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2820 - binary_accuracy: 0.8879 - val_loss: 0.3134 - val_binary_accuracy: 0.8760\n",
      "Epoch 46/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2792 - binary_accuracy: 0.8829 - val_loss: 0.3163 - val_binary_accuracy: 0.8688\n",
      "Epoch 47/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2737 - binary_accuracy: 0.8825 - val_loss: 0.3188 - val_binary_accuracy: 0.8742\n",
      "Epoch 48/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2774 - binary_accuracy: 0.8814 - val_loss: 0.3155 - val_binary_accuracy: 0.8751\n",
      "Epoch 49/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2773 - binary_accuracy: 0.8890 - val_loss: 0.3152 - val_binary_accuracy: 0.8787\n",
      "Epoch 50/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2724 - binary_accuracy: 0.8861 - val_loss: 0.3164 - val_binary_accuracy: 0.8832\n",
      "Epoch 51/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2790 - binary_accuracy: 0.8854 - val_loss: 0.3182 - val_binary_accuracy: 0.8751\n",
      "Epoch 52/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.2675 - binary_accuracy: 0.8942 - val_loss: 0.3192 - val_binary_accuracy: 0.8751\n",
      "Epoch 53/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2684 - binary_accuracy: 0.8886 - val_loss: 0.3214 - val_binary_accuracy: 0.8661\n",
      "Epoch 54/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2702 - binary_accuracy: 0.8852 - val_loss: 0.3181 - val_binary_accuracy: 0.8742\n",
      "Epoch 55/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2638 - binary_accuracy: 0.8886 - val_loss: 0.3186 - val_binary_accuracy: 0.8769\n",
      "Epoch 55: early stopping\n",
      "107/107 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8786808009422851\n",
      "F1 Score: 0.8598639455782313\n"
     ]
    }
   ],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'ITA_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "featType = 1; #Acoustic or Acoustic+context\n",
    "if featType == 1:\n",
    "  original_dim = 19\n",
    "else:\n",
    "  original_dim = 38\n",
    "\n",
    "# print('Classification with::::::',os.path.basename(filee))\n",
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "# print('test file:::::::',os.path.basename(test_path))\n",
    "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
    "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
    "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "\n",
    "\n",
    "xtest_a = normalization(xtest)\n",
    "xtest_ac = normalization(xtest1)\n",
    "xtrain = normalization(xtrain)\n",
    "xtrain1 = normalization(xtrain1)\n",
    "\n",
    "woPP=[]; wPP=[]\n",
    "input_shape1 = (19,1)\n",
    "input_shape2 = (38,1)\n",
    "temperature = 0.03\n",
    "learning_rate=0.001\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# Splitting xtrain and ytrain into training and validation sets\n",
    "xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting xtrain1 and ytrain1 into training and validation sets\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)\n",
    "def create_classifier(hidden_units):\n",
    "\n",
    "    # for layer in encoder.layers:\n",
    "    #     layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # features = encoder(inputs)\n",
    "    features=layers.Dense(hidden_units, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "units=64\n",
    "\n",
    "classifier = create_classifier(units)\n",
    "\n",
    "# history = classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "classifier.summary()\n",
    "classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(xtest_ac)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "accuracy = accuracy_score(ytest, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(ytest, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "data = scipy.io.loadmat(filee)\n",
    "print(data.keys())\n",
    "AF = data['AF']\n",
    "x1 = AF[:-2]\n",
    "y = AF[-2]\n",
    "w = AF[-1]\n",
    "x2 = data['CF']\n",
    "x = np.concatenate((x1, x2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'ITA_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "data = scipy.io.loadmat(filee)\n",
    "print(data.keys())\n",
    "AF = data['AF']\n",
    "x3 = AF[:-2]\n",
    "y1 = AF[-2]\n",
    "w1 = AF[-1]\n",
    "x4 = data['CF']\n",
    "x5 = np.concatenate((x3, x4), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfinal = np.concatenate((x, x5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfinal=np.concatenate((y, y1), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfinal=xfinal.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(xfinal, yfinal, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "Model: \"classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 38)]              0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 64)                2496      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,833\n",
      "Trainable params: 8,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573/573 [==============================] - 7s 8ms/step - loss: 0.6046 - binary_accuracy: 0.6737 - val_loss: 0.5197 - val_binary_accuracy: 0.7617\n",
      "Epoch 2/200\n",
      "573/573 [==============================] - 4s 8ms/step - loss: 0.5252 - binary_accuracy: 0.7491 - val_loss: 0.4825 - val_binary_accuracy: 0.7787\n",
      "Epoch 3/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.4936 - binary_accuracy: 0.7648 - val_loss: 0.4577 - val_binary_accuracy: 0.7931\n",
      "Epoch 4/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.4660 - binary_accuracy: 0.7880 - val_loss: 0.4351 - val_binary_accuracy: 0.8136\n",
      "Epoch 5/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.4437 - binary_accuracy: 0.8000 - val_loss: 0.4225 - val_binary_accuracy: 0.8154\n",
      "Epoch 6/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.4337 - binary_accuracy: 0.8037 - val_loss: 0.4095 - val_binary_accuracy: 0.8241\n",
      "Epoch 7/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.4296 - binary_accuracy: 0.8072 - val_loss: 0.4087 - val_binary_accuracy: 0.8223\n",
      "Epoch 8/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.4168 - binary_accuracy: 0.8145 - val_loss: 0.4066 - val_binary_accuracy: 0.8250\n",
      "Epoch 9/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.4132 - binary_accuracy: 0.8175 - val_loss: 0.3967 - val_binary_accuracy: 0.8285\n",
      "Epoch 10/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.4046 - binary_accuracy: 0.8247 - val_loss: 0.3865 - val_binary_accuracy: 0.8341\n",
      "Epoch 11/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3976 - binary_accuracy: 0.8269 - val_loss: 0.3801 - val_binary_accuracy: 0.8394\n",
      "Epoch 12/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3926 - binary_accuracy: 0.8258 - val_loss: 0.3855 - val_binary_accuracy: 0.8302\n",
      "Epoch 13/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3865 - binary_accuracy: 0.8334 - val_loss: 0.3821 - val_binary_accuracy: 0.8341\n",
      "Epoch 14/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3898 - binary_accuracy: 0.8265 - val_loss: 0.3716 - val_binary_accuracy: 0.8385\n",
      "Epoch 15/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3798 - binary_accuracy: 0.8364 - val_loss: 0.3854 - val_binary_accuracy: 0.8333\n",
      "Epoch 16/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3802 - binary_accuracy: 0.8330 - val_loss: 0.3668 - val_binary_accuracy: 0.8411\n",
      "Epoch 17/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3758 - binary_accuracy: 0.8365 - val_loss: 0.3720 - val_binary_accuracy: 0.8359\n",
      "Epoch 18/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3731 - binary_accuracy: 0.8394 - val_loss: 0.3634 - val_binary_accuracy: 0.8433\n",
      "Epoch 19/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3671 - binary_accuracy: 0.8460 - val_loss: 0.3614 - val_binary_accuracy: 0.8464\n",
      "Epoch 20/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3677 - binary_accuracy: 0.8410 - val_loss: 0.3582 - val_binary_accuracy: 0.8468\n",
      "Epoch 21/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3645 - binary_accuracy: 0.8432 - val_loss: 0.3614 - val_binary_accuracy: 0.8429\n",
      "Epoch 22/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3605 - binary_accuracy: 0.8445 - val_loss: 0.3717 - val_binary_accuracy: 0.8424\n",
      "Epoch 23/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3632 - binary_accuracy: 0.8413 - val_loss: 0.3628 - val_binary_accuracy: 0.8398\n",
      "Epoch 24/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3580 - binary_accuracy: 0.8462 - val_loss: 0.3553 - val_binary_accuracy: 0.8481\n",
      "Epoch 25/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3507 - binary_accuracy: 0.8490 - val_loss: 0.3474 - val_binary_accuracy: 0.8529\n",
      "Epoch 26/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3495 - binary_accuracy: 0.8501 - val_loss: 0.3533 - val_binary_accuracy: 0.8503\n",
      "Epoch 27/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3524 - binary_accuracy: 0.8504 - val_loss: 0.3536 - val_binary_accuracy: 0.8472\n",
      "Epoch 28/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3549 - binary_accuracy: 0.8461 - val_loss: 0.3537 - val_binary_accuracy: 0.8555\n",
      "Epoch 29/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3461 - binary_accuracy: 0.8536 - val_loss: 0.3512 - val_binary_accuracy: 0.8472\n",
      "Epoch 30/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3453 - binary_accuracy: 0.8526 - val_loss: 0.3538 - val_binary_accuracy: 0.8573\n",
      "Epoch 31/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3464 - binary_accuracy: 0.8516 - val_loss: 0.3494 - val_binary_accuracy: 0.8512\n",
      "Epoch 32/200\n",
      "573/573 [==============================] - 4s 6ms/step - loss: 0.3397 - binary_accuracy: 0.8553 - val_loss: 0.3451 - val_binary_accuracy: 0.8573\n",
      "Epoch 33/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3408 - binary_accuracy: 0.8546 - val_loss: 0.3441 - val_binary_accuracy: 0.8464\n",
      "Epoch 34/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3419 - binary_accuracy: 0.8536 - val_loss: 0.3392 - val_binary_accuracy: 0.8560\n",
      "Epoch 35/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3399 - binary_accuracy: 0.8553 - val_loss: 0.3434 - val_binary_accuracy: 0.8529\n",
      "Epoch 36/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3352 - binary_accuracy: 0.8555 - val_loss: 0.3459 - val_binary_accuracy: 0.8512\n",
      "Epoch 37/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3345 - binary_accuracy: 0.8563 - val_loss: 0.3497 - val_binary_accuracy: 0.8529\n",
      "Epoch 38/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3387 - binary_accuracy: 0.8566 - val_loss: 0.3335 - val_binary_accuracy: 0.8677\n",
      "Epoch 39/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3327 - binary_accuracy: 0.8605 - val_loss: 0.3336 - val_binary_accuracy: 0.8629\n",
      "Epoch 40/200\n",
      "573/573 [==============================] - 4s 6ms/step - loss: 0.3292 - binary_accuracy: 0.8597 - val_loss: 0.3498 - val_binary_accuracy: 0.8564\n",
      "Epoch 41/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3332 - binary_accuracy: 0.8557 - val_loss: 0.3273 - val_binary_accuracy: 0.8664\n",
      "Epoch 42/200\n",
      "573/573 [==============================] - 3s 5ms/step - loss: 0.3280 - binary_accuracy: 0.8623 - val_loss: 0.3350 - val_binary_accuracy: 0.8573\n",
      "Epoch 43/200\n",
      "573/573 [==============================] - 4s 6ms/step - loss: 0.3291 - binary_accuracy: 0.8613 - val_loss: 0.3426 - val_binary_accuracy: 0.8546\n",
      "Epoch 44/200\n",
      "573/573 [==============================] - 4s 6ms/step - loss: 0.3334 - binary_accuracy: 0.8585 - val_loss: 0.3320 - val_binary_accuracy: 0.8603\n",
      "Epoch 45/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3299 - binary_accuracy: 0.8598 - val_loss: 0.3333 - val_binary_accuracy: 0.8608\n",
      "Epoch 46/200\n",
      "573/573 [==============================] - 4s 6ms/step - loss: 0.3314 - binary_accuracy: 0.8580 - val_loss: 0.3314 - val_binary_accuracy: 0.8612\n",
      "Epoch 47/200\n",
      "573/573 [==============================] - 3s 6ms/step - loss: 0.3284 - binary_accuracy: 0.8584 - val_loss: 0.3340 - val_binary_accuracy: 0.8581\n",
      "Epoch 48/200\n",
      "573/573 [==============================] - 4s 7ms/step - loss: 0.3263 - binary_accuracy: 0.8598 - val_loss: 0.3241 - val_binary_accuracy: 0.8625\n",
      "Epoch 49/200\n",
      "573/573 [==============================] - 5s 8ms/step - loss: 0.3267 - binary_accuracy: 0.8625 - val_loss: 0.3330 - val_binary_accuracy: 0.8546\n",
      "Epoch 50/200\n",
      "573/573 [==============================] - 5s 8ms/step - loss: 0.3195 - binary_accuracy: 0.8674 - val_loss: 0.3445 - val_binary_accuracy: 0.8516\n",
      "Epoch 51/200\n",
      "573/573 [==============================] - 5s 8ms/step - loss: 0.3218 - binary_accuracy: 0.8610 - val_loss: 0.3268 - val_binary_accuracy: 0.8629\n",
      "Epoch 52/200\n",
      "573/573 [==============================] - 4s 7ms/step - loss: 0.3262 - binary_accuracy: 0.8625 - val_loss: 0.3313 - val_binary_accuracy: 0.8581\n",
      "Epoch 53/200\n",
      "573/573 [==============================] - 4s 8ms/step - loss: 0.3219 - binary_accuracy: 0.8582 - val_loss: 0.3324 - val_binary_accuracy: 0.8651\n",
      "Epoch 54/200\n",
      "573/573 [==============================] - 4s 7ms/step - loss: 0.3179 - binary_accuracy: 0.8676 - val_loss: 0.3275 - val_binary_accuracy: 0.8669\n",
      "Epoch 55/200\n",
      "573/573 [==============================] - 4s 7ms/step - loss: 0.3191 - binary_accuracy: 0.8625 - val_loss: 0.3269 - val_binary_accuracy: 0.8651\n",
      "Epoch 56/200\n",
      "573/573 [==============================] - 4s 8ms/step - loss: 0.3166 - binary_accuracy: 0.8632 - val_loss: 0.3197 - val_binary_accuracy: 0.8712\n",
      "Epoch 57/200\n",
      "573/573 [==============================] - 4s 8ms/step - loss: 0.3046 - binary_accuracy: 0.8755 - val_loss: 0.3220 - val_binary_accuracy: 0.8616\n",
      "Epoch 58/200\n",
      "573/573 [==============================] - 4s 7ms/step - loss: 0.3163 - binary_accuracy: 0.8656 - val_loss: 0.3207 - val_binary_accuracy: 0.8638\n",
      "Epoch 59/200\n",
      "573/573 [==============================] - 4s 7ms/step - loss: 0.3119 - binary_accuracy: 0.8699 - val_loss: 0.3257 - val_binary_accuracy: 0.8625\n",
      "Epoch 60/200\n",
      "573/573 [==============================] - 4s 7ms/step - loss: 0.3122 - binary_accuracy: 0.8673 - val_loss: 0.3249 - val_binary_accuracy: 0.8647\n",
      "Epoch 61/200\n",
      "573/573 [==============================] - 4s 7ms/step - loss: 0.3121 - binary_accuracy: 0.8688 - val_loss: 0.3221 - val_binary_accuracy: 0.8677\n",
      "Epoch 62/200\n",
      "573/573 [==============================] - 4s 7ms/step - loss: 0.3181 - binary_accuracy: 0.8653 - val_loss: 0.3231 - val_binary_accuracy: 0.8634\n",
      "Epoch 63/200\n",
      "573/573 [==============================] - 4s 8ms/step - loss: 0.3138 - binary_accuracy: 0.8664 - val_loss: 0.3260 - val_binary_accuracy: 0.8686\n",
      "Epoch 64/200\n",
      "573/573 [==============================] - 4s 8ms/step - loss: 0.3147 - binary_accuracy: 0.8667 - val_loss: 0.3203 - val_binary_accuracy: 0.8664\n",
      "Epoch 65/200\n",
      "573/573 [==============================] - 4s 8ms/step - loss: 0.3114 - binary_accuracy: 0.8701 - val_loss: 0.3438 - val_binary_accuracy: 0.8538\n",
      "Epoch 66/200\n",
      "573/573 [==============================] - 4s 7ms/step - loss: 0.3063 - binary_accuracy: 0.8669 - val_loss: 0.3302 - val_binary_accuracy: 0.8682\n",
      "Epoch 66: early stopping\n",
      "107/107 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.7458775029446407\n",
      "F1 Score: 0.6985679357317499\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "# print('test file:::::::',os.path.basename(test_path))\n",
    "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
    "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
    "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "\n",
    "\n",
    "xtest_a = normalization(xtest)\n",
    "xtest_ac = normalization(xtest1)\n",
    "xtrain = normalization(xtrain)\n",
    "xtrain1 = normalization(xfinal)\n",
    "\n",
    "input_shape1 = (19,)\n",
    "input_shape2 = (38,)\n",
    "temperature = 0.03\n",
    "learning_rate=0.001\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xfinal, yfinal, test_size=0.2, random_state=42)\n",
    "def create_classifier(hidden_units):\n",
    "\n",
    "    # for layer in encoder.layers:\n",
    "    #     layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # features = encoder(inputs)\n",
    "    features=layers.Dense(hidden_units, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "units=64\n",
    "\n",
    "classifier = create_classifier(units)\n",
    "classifier.summary()\n",
    "classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_ac), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(xtest_ac)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "accuracy = accuracy_score(ytest, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(ytest, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "original_dim=38\n",
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "xtrain1G, ytrain1G, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest1G, ytest1G, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "xtest_acG = normalization(xtest1G)\n",
    "xtrain1G = normalization(xtrain1G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'ITA_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "original_dim=38\n",
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "xtrain1I, ytrain1I, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest1I, ytest1I, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "xtest_acI = normalization(xtest1I)\n",
    "xtrain1I = normalization(xtrain1I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.7458775029446407\n",
      "F1 Score: 0.6985679357317499\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(xtest_acI)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "accuracy = accuracy_score(ytest1I, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(ytest1I, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.7544372520359156\n",
      "F1 Score: 0.7010676156583631\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(xtest_acG)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "accuracy = accuracy_score(ytest1G, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(ytest1G, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 38)]              0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 64)                2496      \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,833\n",
      "Trainable params: 8,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284/295 [===========================>..] - ETA: 0s - loss: 0.5727 - binary_accuracy: 0.6974"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 4s 6ms/step - loss: 0.5691 - binary_accuracy: 0.7001 - val_loss: 0.4835 - val_binary_accuracy: 0.7786\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4940 - binary_accuracy: 0.7685 - val_loss: 0.4375 - val_binary_accuracy: 0.8083\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.4513 - binary_accuracy: 0.7935 - val_loss: 0.4104 - val_binary_accuracy: 0.8261\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4251 - binary_accuracy: 0.8118 - val_loss: 0.3891 - val_binary_accuracy: 0.8363\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4160 - binary_accuracy: 0.8169 - val_loss: 0.3762 - val_binary_accuracy: 0.8482\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.4006 - binary_accuracy: 0.8253 - val_loss: 0.3702 - val_binary_accuracy: 0.8473\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3881 - binary_accuracy: 0.8319 - val_loss: 0.3626 - val_binary_accuracy: 0.8533\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3717 - binary_accuracy: 0.8349 - val_loss: 0.3480 - val_binary_accuracy: 0.8601\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3755 - binary_accuracy: 0.8379 - val_loss: 0.3423 - val_binary_accuracy: 0.8651\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3626 - binary_accuracy: 0.8489 - val_loss: 0.3327 - val_binary_accuracy: 0.8745\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3470 - binary_accuracy: 0.8508 - val_loss: 0.3373 - val_binary_accuracy: 0.8626\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3497 - binary_accuracy: 0.8527 - val_loss: 0.3269 - val_binary_accuracy: 0.8694\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3466 - binary_accuracy: 0.8521 - val_loss: 0.3270 - val_binary_accuracy: 0.8728\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3329 - binary_accuracy: 0.8604 - val_loss: 0.3249 - val_binary_accuracy: 0.8719\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3322 - binary_accuracy: 0.8601 - val_loss: 0.3187 - val_binary_accuracy: 0.8753\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3313 - binary_accuracy: 0.8604 - val_loss: 0.3175 - val_binary_accuracy: 0.8745\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3170 - binary_accuracy: 0.8682 - val_loss: 0.3107 - val_binary_accuracy: 0.8762\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3186 - binary_accuracy: 0.8659 - val_loss: 0.3108 - val_binary_accuracy: 0.8711\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3239 - binary_accuracy: 0.8650 - val_loss: 0.3163 - val_binary_accuracy: 0.8753\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3101 - binary_accuracy: 0.8725 - val_loss: 0.3030 - val_binary_accuracy: 0.8787\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3095 - binary_accuracy: 0.8708 - val_loss: 0.3067 - val_binary_accuracy: 0.8830\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3110 - binary_accuracy: 0.8710 - val_loss: 0.3030 - val_binary_accuracy: 0.8821\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2993 - binary_accuracy: 0.8744 - val_loss: 0.2951 - val_binary_accuracy: 0.8830\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3074 - binary_accuracy: 0.8722 - val_loss: 0.2972 - val_binary_accuracy: 0.8813\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2895 - binary_accuracy: 0.8826 - val_loss: 0.2971 - val_binary_accuracy: 0.8855\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2902 - binary_accuracy: 0.8795 - val_loss: 0.3018 - val_binary_accuracy: 0.8787\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2899 - binary_accuracy: 0.8824 - val_loss: 0.2918 - val_binary_accuracy: 0.8830\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2961 - binary_accuracy: 0.8750 - val_loss: 0.2849 - val_binary_accuracy: 0.8889\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2853 - binary_accuracy: 0.8807 - val_loss: 0.2833 - val_binary_accuracy: 0.8906\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2820 - binary_accuracy: 0.8824 - val_loss: 0.2886 - val_binary_accuracy: 0.8880\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2814 - binary_accuracy: 0.8839 - val_loss: 0.2874 - val_binary_accuracy: 0.8762\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2836 - binary_accuracy: 0.8771 - val_loss: 0.2869 - val_binary_accuracy: 0.8872\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2764 - binary_accuracy: 0.8831 - val_loss: 0.2827 - val_binary_accuracy: 0.8872\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2804 - binary_accuracy: 0.8884 - val_loss: 0.2873 - val_binary_accuracy: 0.8974\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.2644 - binary_accuracy: 0.8888 - val_loss: 0.2911 - val_binary_accuracy: 0.8897\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2741 - binary_accuracy: 0.8884 - val_loss: 0.2828 - val_binary_accuracy: 0.8957\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2652 - binary_accuracy: 0.8877 - val_loss: 0.2757 - val_binary_accuracy: 0.8957\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2710 - binary_accuracy: 0.8920 - val_loss: 0.2745 - val_binary_accuracy: 0.8974\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2648 - binary_accuracy: 0.8911 - val_loss: 0.2805 - val_binary_accuracy: 0.8914\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2666 - binary_accuracy: 0.8960 - val_loss: 0.2791 - val_binary_accuracy: 0.8821\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2588 - binary_accuracy: 0.8960 - val_loss: 0.2721 - val_binary_accuracy: 0.8991\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2611 - binary_accuracy: 0.8937 - val_loss: 0.2787 - val_binary_accuracy: 0.8880\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2589 - binary_accuracy: 0.8943 - val_loss: 0.2754 - val_binary_accuracy: 0.8957\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2642 - binary_accuracy: 0.8877 - val_loss: 0.2707 - val_binary_accuracy: 0.9016\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2580 - binary_accuracy: 0.8941 - val_loss: 0.2661 - val_binary_accuracy: 0.8957\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2590 - binary_accuracy: 0.8924 - val_loss: 0.2684 - val_binary_accuracy: 0.8965\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2575 - binary_accuracy: 0.8913 - val_loss: 0.2734 - val_binary_accuracy: 0.8957\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2539 - binary_accuracy: 0.8943 - val_loss: 0.2725 - val_binary_accuracy: 0.8914\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2561 - binary_accuracy: 0.8909 - val_loss: 0.2689 - val_binary_accuracy: 0.8948\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2580 - binary_accuracy: 0.8952 - val_loss: 0.2662 - val_binary_accuracy: 0.8889\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2477 - binary_accuracy: 0.8990 - val_loss: 0.2712 - val_binary_accuracy: 0.8914\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2559 - binary_accuracy: 0.8937 - val_loss: 0.2690 - val_binary_accuracy: 0.8965\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2521 - binary_accuracy: 0.8994 - val_loss: 0.2741 - val_binary_accuracy: 0.8948\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2483 - binary_accuracy: 0.8973 - val_loss: 0.2629 - val_binary_accuracy: 0.8957\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2507 - binary_accuracy: 0.8956 - val_loss: 0.2676 - val_binary_accuracy: 0.8880\n",
      "Epoch 56/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2379 - binary_accuracy: 0.9017 - val_loss: 0.2740 - val_binary_accuracy: 0.8948\n",
      "Epoch 57/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2497 - binary_accuracy: 0.8958 - val_loss: 0.2696 - val_binary_accuracy: 0.8948\n",
      "Epoch 58/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2392 - binary_accuracy: 0.9005 - val_loss: 0.2702 - val_binary_accuracy: 0.8889\n",
      "Epoch 59/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2447 - binary_accuracy: 0.8992 - val_loss: 0.2703 - val_binary_accuracy: 0.8830\n",
      "Epoch 60/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2436 - binary_accuracy: 0.8971 - val_loss: 0.2712 - val_binary_accuracy: 0.8923\n",
      "Epoch 61/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2334 - binary_accuracy: 0.9024 - val_loss: 0.2690 - val_binary_accuracy: 0.8914\n",
      "Epoch 62/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2401 - binary_accuracy: 0.9017 - val_loss: 0.2741 - val_binary_accuracy: 0.8889\n",
      "Epoch 63/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2398 - binary_accuracy: 0.9000 - val_loss: 0.2793 - val_binary_accuracy: 0.8846\n",
      "Epoch 64/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2388 - binary_accuracy: 0.9015 - val_loss: 0.2802 - val_binary_accuracy: 0.8804\n",
      "Epoch 64: early stopping\n",
      "107/107 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.848939929328622\n",
      "F1 Score: 0.8243752139678192\n"
     ]
    }
   ],
   "source": [
    "input_shape = (38,)\n",
    "temperature = 0.03\n",
    "learning_rate=0.001\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1G, ytrain1G, test_size=0.2, random_state=42)\n",
    "def create_classifier(hidden_units):\n",
    "    # for layer in encoder.layers:\n",
    "    #     layer.trainable = trainable\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features=layers.Dense(hidden_units, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "units=64\n",
    "\n",
    "classifier = create_classifier(units)\n",
    "classifier.summary()\n",
    "classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_ac), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(xtest_acI)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "accuracy = accuracy_score(ytest1I, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(ytest1I, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 38)]              0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 64)                2496      \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,833\n",
      "Trainable params: 8,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277/279 [============================>.] - ETA: 0s - loss: 0.5700 - binary_accuracy: 0.7017"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - 4s 6ms/step - loss: 0.5693 - binary_accuracy: 0.7021 - val_loss: 0.4758 - val_binary_accuracy: 0.7799\n",
      "Epoch 2/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.4833 - binary_accuracy: 0.7767 - val_loss: 0.4488 - val_binary_accuracy: 0.7960\n",
      "Epoch 3/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.4492 - binary_accuracy: 0.7960 - val_loss: 0.4149 - val_binary_accuracy: 0.8167\n",
      "Epoch 4/200\n",
      "279/279 [==============================] - 2s 5ms/step - loss: 0.4330 - binary_accuracy: 0.8115 - val_loss: 0.4058 - val_binary_accuracy: 0.8212\n",
      "Epoch 5/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.4125 - binary_accuracy: 0.8189 - val_loss: 0.3947 - val_binary_accuracy: 0.8212\n",
      "Epoch 6/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3961 - binary_accuracy: 0.8313 - val_loss: 0.3880 - val_binary_accuracy: 0.8365\n",
      "Epoch 7/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3957 - binary_accuracy: 0.8270 - val_loss: 0.3832 - val_binary_accuracy: 0.8356\n",
      "Epoch 8/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3902 - binary_accuracy: 0.8297 - val_loss: 0.3731 - val_binary_accuracy: 0.8446\n",
      "Epoch 9/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3755 - binary_accuracy: 0.8391 - val_loss: 0.3689 - val_binary_accuracy: 0.8410\n",
      "Epoch 10/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3787 - binary_accuracy: 0.8369 - val_loss: 0.3840 - val_binary_accuracy: 0.8374\n",
      "Epoch 11/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3600 - binary_accuracy: 0.8506 - val_loss: 0.3617 - val_binary_accuracy: 0.8410\n",
      "Epoch 12/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3580 - binary_accuracy: 0.8441 - val_loss: 0.3589 - val_binary_accuracy: 0.8473\n",
      "Epoch 13/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3578 - binary_accuracy: 0.8461 - val_loss: 0.3567 - val_binary_accuracy: 0.8509\n",
      "Epoch 14/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3519 - binary_accuracy: 0.8551 - val_loss: 0.3541 - val_binary_accuracy: 0.8482\n",
      "Epoch 15/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3491 - binary_accuracy: 0.8542 - val_loss: 0.3422 - val_binary_accuracy: 0.8580\n",
      "Epoch 16/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3386 - binary_accuracy: 0.8591 - val_loss: 0.3461 - val_binary_accuracy: 0.8544\n",
      "Epoch 17/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3337 - binary_accuracy: 0.8564 - val_loss: 0.3502 - val_binary_accuracy: 0.8535\n",
      "Epoch 18/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3324 - binary_accuracy: 0.8614 - val_loss: 0.3443 - val_binary_accuracy: 0.8571\n",
      "Epoch 19/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3291 - binary_accuracy: 0.8645 - val_loss: 0.3332 - val_binary_accuracy: 0.8598\n",
      "Epoch 20/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3270 - binary_accuracy: 0.8652 - val_loss: 0.3344 - val_binary_accuracy: 0.8535\n",
      "Epoch 21/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3284 - binary_accuracy: 0.8636 - val_loss: 0.3457 - val_binary_accuracy: 0.8607\n",
      "Epoch 22/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3333 - binary_accuracy: 0.8605 - val_loss: 0.3414 - val_binary_accuracy: 0.8580\n",
      "Epoch 23/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3175 - binary_accuracy: 0.8656 - val_loss: 0.3374 - val_binary_accuracy: 0.8571\n",
      "Epoch 24/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3233 - binary_accuracy: 0.8625 - val_loss: 0.3366 - val_binary_accuracy: 0.8643\n",
      "Epoch 25/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3125 - binary_accuracy: 0.8744 - val_loss: 0.3269 - val_binary_accuracy: 0.8706\n",
      "Epoch 26/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3140 - binary_accuracy: 0.8708 - val_loss: 0.3301 - val_binary_accuracy: 0.8634\n",
      "Epoch 27/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3174 - binary_accuracy: 0.8681 - val_loss: 0.3308 - val_binary_accuracy: 0.8562\n",
      "Epoch 28/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3163 - binary_accuracy: 0.8708 - val_loss: 0.3305 - val_binary_accuracy: 0.8589\n",
      "Epoch 29/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3055 - binary_accuracy: 0.8771 - val_loss: 0.3345 - val_binary_accuracy: 0.8634\n",
      "Epoch 30/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3018 - binary_accuracy: 0.8751 - val_loss: 0.3329 - val_binary_accuracy: 0.8607\n",
      "Epoch 31/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3033 - binary_accuracy: 0.8746 - val_loss: 0.3345 - val_binary_accuracy: 0.8571\n",
      "Epoch 32/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.3007 - binary_accuracy: 0.8737 - val_loss: 0.3245 - val_binary_accuracy: 0.8634\n",
      "Epoch 33/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.3018 - binary_accuracy: 0.8717 - val_loss: 0.3177 - val_binary_accuracy: 0.8679\n",
      "Epoch 34/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.2959 - binary_accuracy: 0.8820 - val_loss: 0.3220 - val_binary_accuracy: 0.8661\n",
      "Epoch 35/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2916 - binary_accuracy: 0.8782 - val_loss: 0.3202 - val_binary_accuracy: 0.8688\n",
      "Epoch 36/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2961 - binary_accuracy: 0.8764 - val_loss: 0.3248 - val_binary_accuracy: 0.8661\n",
      "Epoch 37/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2889 - binary_accuracy: 0.8854 - val_loss: 0.3221 - val_binary_accuracy: 0.8742\n",
      "Epoch 38/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2955 - binary_accuracy: 0.8771 - val_loss: 0.3182 - val_binary_accuracy: 0.8751\n",
      "Epoch 39/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2891 - binary_accuracy: 0.8834 - val_loss: 0.3268 - val_binary_accuracy: 0.8670\n",
      "Epoch 40/200\n",
      "279/279 [==============================] - 1s 4ms/step - loss: 0.2856 - binary_accuracy: 0.8827 - val_loss: 0.3187 - val_binary_accuracy: 0.8778\n",
      "Epoch 41/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2855 - binary_accuracy: 0.8836 - val_loss: 0.3183 - val_binary_accuracy: 0.8751\n",
      "Epoch 42/200\n",
      "279/279 [==============================] - 1s 5ms/step - loss: 0.2877 - binary_accuracy: 0.8812 - val_loss: 0.3244 - val_binary_accuracy: 0.8679\n",
      "Epoch 43/200\n",
      "279/279 [==============================] - 2s 6ms/step - loss: 0.2802 - binary_accuracy: 0.8863 - val_loss: 0.3238 - val_binary_accuracy: 0.8733\n",
      "Epoch 43: early stopping\n",
      "150/150 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.8481937774065567\n",
      "F1 Score: 0.8297025064417897\n"
     ]
    }
   ],
   "source": [
    "input_shape = (38,)\n",
    "temperature = 0.03\n",
    "learning_rate=0.001\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1I, ytrain1I, test_size=0.2, random_state=42)\n",
    "def create_classifier(hidden_units):\n",
    "    # for layer in encoder.layers:\n",
    "    #     layer.trainable = trainable\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features=layers.Dense(hidden_units, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "units=64\n",
    "\n",
    "classifier = create_classifier(units)\n",
    "classifier.summary()\n",
    "classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_ac), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(xtest_acG)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "accuracy = accuracy_score(ytest1G, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(ytest1G, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
