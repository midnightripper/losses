{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Reshape\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint ,EarlyStopping\n",
    "temperature = 0.03\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(path, featType):\n",
    "    \"\"\"\n",
    "    Load data from a MATLAB file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the MATLAB file.\n",
    "        featType (int): Type of features to load.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Tuple containing input features, labels, weights, and additional information.\n",
    "    \"\"\"\n",
    "    data = scipy.io.loadmat(path)\n",
    "    print(data.keys())\n",
    "\n",
    "    AF = data['AF']\n",
    "    x1 = AF[:-2]\n",
    "    y = AF[-2]\n",
    "    w = AF[-1]\n",
    "\n",
    "    if featType == 1:\n",
    "        x = x1\n",
    "    else:\n",
    "        x2 = data['CF']\n",
    "        x = np.concatenate((x1, x2), axis=0)\n",
    "    return x.T, y.T, w.T, data['CF_info']\n",
    "\n",
    "def calculate_accuracy(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy between two arrays.\n",
    "\n",
    "    Args:\n",
    "        arr1 (array): First array.\n",
    "        arr2 (array): Second array.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy between the two arrays.\n",
    "    \"\"\"\n",
    "    count = sum(1 for itr1, itr2 in zip(arr1, arr2) if itr1 == itr2)\n",
    "    return count / len(arr1)\n",
    "\n",
    "def normalization(feats):\n",
    "\n",
    "    \"\"\"\n",
    "    Normalize the input features using standard scaling.\n",
    "\n",
    "    Args:\n",
    "        feats (array): Input features.\n",
    "\n",
    "    Returns:\n",
    "        array: Normalized features.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(feats)\n",
    "    scaler = StandardScaler()\n",
    "    x_new = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return x_new\n",
    "\n",
    "def make_partitions(arr_words, arr_labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Create partitions based on word boundaries and labels.\n",
    "\n",
    "    Args:\n",
    "        arr_words (array): Array of words.\n",
    "        arr_labels (array): Array of labels.\n",
    "\n",
    "    Returns:\n",
    "        array: Partitions based on word boundaries and labels.\n",
    "    \"\"\"\n",
    "    v = []\n",
    "    temp = []\n",
    "\n",
    "    for i in range(len(arr_words) - 1):\n",
    "        word = arr_words[i]\n",
    "        next_word = arr_words[i + 1]\n",
    "        temp.append(arr_labels[i])\n",
    "\n",
    "        if word != next_word or i == len(arr_words) - 2:\n",
    "            if i == len(arr_words) - 2:\n",
    "                temp.append(arr_labels[i + 1])\n",
    "\n",
    "            numpy_temp = np.array(temp)\n",
    "            temp_max = np.amax(numpy_temp)\n",
    "            numpy_temp = np.divide(numpy_temp, temp_max)\n",
    "            v = np.concatenate((v, numpy_temp), axis=None)\n",
    "            temp.clear()\n",
    "\n",
    "    v1 = [1 if i == 1 else 0 for i in v]\n",
    "    return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "featType = 1; #Acoustic or Acoustic+context\n",
    "if featType == 1:\n",
    "  original_dim = 19\n",
    "else:\n",
    "  original_dim = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "# print('Classification with::::::',os.path.basename(filee))\n",
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "# print('test file:::::::',os.path.basename(test_path))\n",
    "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
    "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
    "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_a = normalization(xtest)\n",
    "xtest_ac = normalization(xtest1)\n",
    "xtrain = normalization(xtrain)\n",
    "xtrain1 = normalization(xtrain1)\n",
    "\n",
    "woPP=[]; wPP=[]\n",
    "input_shape1 = (19,1)\n",
    "input_shape2 = (38,1)\n",
    "temperature = 0.03\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# Splitting xtrain and ytrain into training and validation sets\n",
    "xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting xtrain1 and ytrain1 into training and validation sets\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(hidden_units):\n",
    "\n",
    "    # for layer in encoder.layers:\n",
    "    #     layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # features = encoder(inputs)\n",
    "    features=layers.Dense(hidden_units, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    features = layers.Dense(hidden_units//2, activation=\"relu\")(features)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "units=64\n",
    "\n",
    "classifier = create_classifier(units)\n",
    "\n",
    "# history = classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10-classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_70 (InputLayer)       [(None, 38)]              0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 64)                2496      \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,833\n",
      "Trainable params: 8,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 4s 5ms/step - loss: 0.5763 - binary_accuracy: 0.6933 - val_loss: 0.4817 - val_binary_accuracy: 0.7820\n",
      "Epoch 2/200\n",
      "  1/295 [..............................] - ETA: 1s - loss: 0.5974 - binary_accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 1s 5ms/step - loss: 0.4898 - binary_accuracy: 0.7716 - val_loss: 0.4311 - val_binary_accuracy: 0.8134\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.4473 - binary_accuracy: 0.7950 - val_loss: 0.4103 - val_binary_accuracy: 0.8202\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.4371 - binary_accuracy: 0.8094 - val_loss: 0.4003 - val_binary_accuracy: 0.8287\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.4129 - binary_accuracy: 0.8145 - val_loss: 0.3727 - val_binary_accuracy: 0.8422\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3933 - binary_accuracy: 0.8283 - val_loss: 0.3662 - val_binary_accuracy: 0.8431\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3897 - binary_accuracy: 0.8262 - val_loss: 0.3499 - val_binary_accuracy: 0.8541\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3850 - binary_accuracy: 0.8328 - val_loss: 0.3398 - val_binary_accuracy: 0.8626\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3762 - binary_accuracy: 0.8376 - val_loss: 0.3446 - val_binary_accuracy: 0.8660\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3562 - binary_accuracy: 0.8442 - val_loss: 0.3224 - val_binary_accuracy: 0.8702\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3623 - binary_accuracy: 0.8455 - val_loss: 0.3350 - val_binary_accuracy: 0.8643\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3494 - binary_accuracy: 0.8483 - val_loss: 0.3393 - val_binary_accuracy: 0.8643\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3387 - binary_accuracy: 0.8621 - val_loss: 0.3316 - val_binary_accuracy: 0.8643\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3370 - binary_accuracy: 0.8565 - val_loss: 0.3313 - val_binary_accuracy: 0.8702\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3417 - binary_accuracy: 0.8570 - val_loss: 0.3164 - val_binary_accuracy: 0.8753\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3289 - binary_accuracy: 0.8597 - val_loss: 0.3186 - val_binary_accuracy: 0.8762\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3350 - binary_accuracy: 0.8555 - val_loss: 0.3141 - val_binary_accuracy: 0.8719\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3314 - binary_accuracy: 0.8565 - val_loss: 0.3087 - val_binary_accuracy: 0.8728\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3169 - binary_accuracy: 0.8618 - val_loss: 0.3053 - val_binary_accuracy: 0.8728\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3189 - binary_accuracy: 0.8629 - val_loss: 0.3006 - val_binary_accuracy: 0.8813\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3092 - binary_accuracy: 0.8727 - val_loss: 0.3014 - val_binary_accuracy: 0.8804\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3050 - binary_accuracy: 0.8716 - val_loss: 0.3045 - val_binary_accuracy: 0.8813\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3040 - binary_accuracy: 0.8720 - val_loss: 0.3050 - val_binary_accuracy: 0.8728\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3125 - binary_accuracy: 0.8648 - val_loss: 0.3029 - val_binary_accuracy: 0.8745\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3008 - binary_accuracy: 0.8710 - val_loss: 0.3121 - val_binary_accuracy: 0.8821\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2997 - binary_accuracy: 0.8752 - val_loss: 0.2990 - val_binary_accuracy: 0.8796\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2965 - binary_accuracy: 0.8752 - val_loss: 0.2940 - val_binary_accuracy: 0.8804\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2940 - binary_accuracy: 0.8758 - val_loss: 0.2994 - val_binary_accuracy: 0.8813\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2891 - binary_accuracy: 0.8763 - val_loss: 0.2902 - val_binary_accuracy: 0.8813\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2905 - binary_accuracy: 0.8812 - val_loss: 0.2880 - val_binary_accuracy: 0.8821\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2951 - binary_accuracy: 0.8788 - val_loss: 0.2836 - val_binary_accuracy: 0.8796\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2893 - binary_accuracy: 0.8780 - val_loss: 0.2890 - val_binary_accuracy: 0.8830\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2803 - binary_accuracy: 0.8814 - val_loss: 0.2952 - val_binary_accuracy: 0.8779\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2783 - binary_accuracy: 0.8826 - val_loss: 0.2855 - val_binary_accuracy: 0.8897\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2733 - binary_accuracy: 0.8862 - val_loss: 0.2842 - val_binary_accuracy: 0.8821\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2754 - binary_accuracy: 0.8812 - val_loss: 0.2822 - val_binary_accuracy: 0.8897\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2714 - binary_accuracy: 0.8873 - val_loss: 0.2813 - val_binary_accuracy: 0.8813\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2783 - binary_accuracy: 0.8843 - val_loss: 0.2803 - val_binary_accuracy: 0.8830\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2750 - binary_accuracy: 0.8814 - val_loss: 0.2775 - val_binary_accuracy: 0.8906\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2608 - binary_accuracy: 0.8901 - val_loss: 0.2898 - val_binary_accuracy: 0.8914\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2673 - binary_accuracy: 0.8926 - val_loss: 0.2764 - val_binary_accuracy: 0.8846\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2671 - binary_accuracy: 0.8850 - val_loss: 0.2801 - val_binary_accuracy: 0.8821\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2573 - binary_accuracy: 0.8918 - val_loss: 0.2886 - val_binary_accuracy: 0.8846\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2653 - binary_accuracy: 0.8856 - val_loss: 0.2771 - val_binary_accuracy: 0.8855\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2562 - binary_accuracy: 0.8907 - val_loss: 0.2742 - val_binary_accuracy: 0.8931\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2592 - binary_accuracy: 0.8905 - val_loss: 0.2720 - val_binary_accuracy: 0.8965\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2522 - binary_accuracy: 0.8964 - val_loss: 0.2768 - val_binary_accuracy: 0.8872\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2544 - binary_accuracy: 0.8930 - val_loss: 0.2655 - val_binary_accuracy: 0.8889\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2535 - binary_accuracy: 0.8994 - val_loss: 0.2691 - val_binary_accuracy: 0.8889\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2541 - binary_accuracy: 0.8930 - val_loss: 0.2770 - val_binary_accuracy: 0.8889\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2495 - binary_accuracy: 0.8947 - val_loss: 0.2738 - val_binary_accuracy: 0.8897\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2431 - binary_accuracy: 0.9000 - val_loss: 0.2753 - val_binary_accuracy: 0.8872\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2414 - binary_accuracy: 0.8979 - val_loss: 0.2683 - val_binary_accuracy: 0.8940\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2333 - binary_accuracy: 0.9026 - val_loss: 0.2719 - val_binary_accuracy: 0.8906\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2509 - binary_accuracy: 0.8956 - val_loss: 0.2734 - val_binary_accuracy: 0.8897\n",
      "Epoch 56/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2405 - binary_accuracy: 0.8977 - val_loss: 0.2638 - val_binary_accuracy: 0.8974\n",
      "Epoch 57/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2425 - binary_accuracy: 0.9011 - val_loss: 0.2615 - val_binary_accuracy: 0.8948\n",
      "Epoch 58/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2532 - binary_accuracy: 0.8930 - val_loss: 0.2579 - val_binary_accuracy: 0.8991\n",
      "Epoch 59/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2373 - binary_accuracy: 0.9062 - val_loss: 0.2614 - val_binary_accuracy: 0.8948\n",
      "Epoch 60/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2353 - binary_accuracy: 0.9047 - val_loss: 0.2568 - val_binary_accuracy: 0.9025\n",
      "Epoch 61/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2390 - binary_accuracy: 0.9026 - val_loss: 0.2626 - val_binary_accuracy: 0.8982\n",
      "Epoch 62/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2277 - binary_accuracy: 0.9062 - val_loss: 0.2542 - val_binary_accuracy: 0.8999\n",
      "Epoch 63/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2341 - binary_accuracy: 0.9024 - val_loss: 0.2548 - val_binary_accuracy: 0.8999\n",
      "Epoch 64/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2400 - binary_accuracy: 0.8996 - val_loss: 0.2567 - val_binary_accuracy: 0.9008\n",
      "Epoch 65/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2378 - binary_accuracy: 0.8981 - val_loss: 0.2541 - val_binary_accuracy: 0.8965\n",
      "Epoch 66/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2342 - binary_accuracy: 0.9005 - val_loss: 0.2606 - val_binary_accuracy: 0.8974\n",
      "Epoch 67/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2287 - binary_accuracy: 0.9051 - val_loss: 0.2564 - val_binary_accuracy: 0.8948\n",
      "Epoch 68/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2346 - binary_accuracy: 0.8977 - val_loss: 0.2533 - val_binary_accuracy: 0.9042\n",
      "Epoch 69/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2305 - binary_accuracy: 0.9058 - val_loss: 0.2585 - val_binary_accuracy: 0.8982\n",
      "Epoch 70/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2371 - binary_accuracy: 0.9037 - val_loss: 0.2539 - val_binary_accuracy: 0.8982\n",
      "Epoch 71/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2276 - binary_accuracy: 0.9051 - val_loss: 0.2509 - val_binary_accuracy: 0.9016\n",
      "Epoch 72/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2210 - binary_accuracy: 0.9090 - val_loss: 0.2579 - val_binary_accuracy: 0.9025\n",
      "Epoch 73/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2296 - binary_accuracy: 0.9096 - val_loss: 0.2570 - val_binary_accuracy: 0.9033\n",
      "Epoch 74/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2282 - binary_accuracy: 0.9070 - val_loss: 0.2497 - val_binary_accuracy: 0.9016\n",
      "Epoch 75/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2230 - binary_accuracy: 0.9102 - val_loss: 0.2571 - val_binary_accuracy: 0.9008\n",
      "Epoch 76/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2251 - binary_accuracy: 0.9100 - val_loss: 0.2517 - val_binary_accuracy: 0.9008\n",
      "Epoch 77/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2243 - binary_accuracy: 0.9100 - val_loss: 0.2472 - val_binary_accuracy: 0.8991\n",
      "Epoch 78/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2280 - binary_accuracy: 0.9058 - val_loss: 0.2472 - val_binary_accuracy: 0.9008\n",
      "Epoch 79/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2223 - binary_accuracy: 0.9102 - val_loss: 0.2536 - val_binary_accuracy: 0.8948\n",
      "Epoch 80/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.2160 - binary_accuracy: 0.9147 - val_loss: 0.2510 - val_binary_accuracy: 0.8965\n",
      "Epoch 81/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2172 - binary_accuracy: 0.9147 - val_loss: 0.2514 - val_binary_accuracy: 0.8999\n",
      "Epoch 82/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2218 - binary_accuracy: 0.9070 - val_loss: 0.2495 - val_binary_accuracy: 0.8982\n",
      "Epoch 83/200\n",
      "295/295 [==============================] - 3s 9ms/step - loss: 0.2171 - binary_accuracy: 0.9124 - val_loss: 0.2496 - val_binary_accuracy: 0.8914\n",
      "Epoch 84/200\n",
      "295/295 [==============================] - 3s 9ms/step - loss: 0.2204 - binary_accuracy: 0.9102 - val_loss: 0.2540 - val_binary_accuracy: 0.9025\n",
      "Epoch 85/200\n",
      "295/295 [==============================] - 3s 9ms/step - loss: 0.2196 - binary_accuracy: 0.9098 - val_loss: 0.2514 - val_binary_accuracy: 0.8991\n",
      "Epoch 86/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2071 - binary_accuracy: 0.9160 - val_loss: 0.2576 - val_binary_accuracy: 0.9025\n",
      "Epoch 87/200\n",
      "295/295 [==============================] - 3s 10ms/step - loss: 0.2185 - binary_accuracy: 0.9109 - val_loss: 0.2502 - val_binary_accuracy: 0.8991\n",
      "Epoch 88/200\n",
      "295/295 [==============================] - 3s 9ms/step - loss: 0.2085 - binary_accuracy: 0.9134 - val_loss: 0.2603 - val_binary_accuracy: 0.8982\n",
      "Epoch 88: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f38b2a460>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.summary()\n",
    "classifier.fit(x=xtra_ac, y=ytra_ac, validation_data =(xval_ac,yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.8993526832324076\n",
      "F1 Score: 0.8856193640246796\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(xtest_ac)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "accuracy = accuracy_score(ytest, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(ytest, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
