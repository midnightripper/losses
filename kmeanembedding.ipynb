{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (3.7.1)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.2.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.local/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (9.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (5.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.39.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.0.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 2)) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 06:10:36.353404: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/project/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2023-06-09 06:10:40.021458: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-06-09 06:10:40.021490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: 3ac0bb55e8cb\n",
      "2023-06-09 06:10:40.021499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: 3ac0bb55e8cb\n",
      "2023-06-09 06:10:40.021592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2023-06-09 06:10:40.021617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.105.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Reshape\n",
    "temperature = 0.03\n",
    "learning_rate=0.001\n",
    "\n",
    "def dataloader(path, featType):\n",
    "    \"\"\"\n",
    "    Load data from a MATLAB file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the MATLAB file.\n",
    "        featType (int): Type of features to load.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Tuple containing input features, labels, weights, and additional information.\n",
    "    \"\"\"\n",
    "    data = scipy.io.loadmat(path)\n",
    "    print(data.keys())\n",
    "\n",
    "    AF = data['AF']\n",
    "    x1 = AF[:-2]\n",
    "    y = AF[-2]\n",
    "    w = AF[-1]\n",
    "\n",
    "    if featType == 1:\n",
    "        x = x1\n",
    "    else:\n",
    "        x2 = data['CF']\n",
    "        x = np.concatenate((x1, x2), axis=0)\n",
    "    return x.T, y.T, w.T, data['CF_info']\n",
    "\n",
    "def calculate_accuracy(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy between two arrays.\n",
    "\n",
    "    Args:\n",
    "        arr1 (array): First array.\n",
    "        arr2 (array): Second array.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy between the two arrays.\n",
    "    \"\"\"\n",
    "    count = sum(1 for itr1, itr2 in zip(arr1, arr2) if itr1 == itr2)\n",
    "    return count / len(arr1)\n",
    "\n",
    "def normalization(feats):\n",
    "\n",
    "    \"\"\"\n",
    "    Normalize the input features using standard scaling.\n",
    "\n",
    "    Args:\n",
    "        feats (array): Input features.\n",
    "\n",
    "    Returns:\n",
    "        array: Normalized features.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(feats)\n",
    "    scaler = StandardScaler()\n",
    "    x_new = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return x_new\n",
    "\n",
    "def make_partitions(arr_words, arr_labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Create partitions based on word boundaries and labels.\n",
    "\n",
    "    Args:\n",
    "        arr_words (array): Array of words.\n",
    "        arr_labels (array): Array of labels.\n",
    "\n",
    "    Returns:\n",
    "        array: Partitions based on word boundaries and labels.\n",
    "    \"\"\"\n",
    "    v = []\n",
    "    temp = []\n",
    "\n",
    "    for i in range(len(arr_words) - 1):\n",
    "        word = arr_words[i]\n",
    "        next_word = arr_words[i + 1]\n",
    "        temp.append(arr_labels[i])\n",
    "\n",
    "        if word != next_word or i == len(arr_words) - 2:\n",
    "            if i == len(arr_words) - 2:\n",
    "                temp.append(arr_labels[i + 1])\n",
    "\n",
    "            numpy_temp = np.array(temp)\n",
    "            temp_max = np.amax(numpy_temp)\n",
    "            numpy_temp = np.divide(numpy_temp, temp_max)\n",
    "            v = np.concatenate((v, numpy_temp), axis=None)\n",
    "            temp.clear()\n",
    "\n",
    "    v1 = [1 if i == 1 else 0 for i in v]\n",
    "    return v1\n",
    "    \n",
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "featType = 1; #Acoustic or Acoustic+context\n",
    "if featType == 1:\n",
    "  original_dim = 19\n",
    "else:\n",
    "  original_dim = 38\n",
    "# print('Classification with::::::',os.path.basename(filee))\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "# print('test file:::::::',os.path.basename(test_path))\n",
    "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
    "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
    "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "xtest_a = normalization(xtest)\n",
    "xtest_ac = normalization(xtest1)\n",
    "xtrain = normalization(xtrain)\n",
    "xtrain1 = normalization(xtrain1)\n",
    "\n",
    "woPP=[]; wPP=[]\n",
    "input_shape1 = (19,1)\n",
    "input_shape2 = (38,1)\n",
    "temperature = 0.03\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
