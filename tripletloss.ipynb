{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m388.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (3.7.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m357.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m218.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m391.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.39.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (5.12.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (9.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.11.0)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m299.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.10.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 2)) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.14.0)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, joblib, scikit-learn, pandas\n",
      "Successfully installed joblib-1.2.0 pandas-2.0.2 pytz-2023.3 scikit-learn-1.2.2 threadpoolctl-3.1.0 tzdata-2023.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 06:49:26.019748: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in ./.local/lib/python3.8/site-packages (0.20.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in ./.local/lib/python3.8/site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-addons --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14618623979168308874\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4545708032\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15074625132866972223\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 06:49:36.074756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.083914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.084283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.865326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.865738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.865919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.866077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 4335 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2023-06-07 06:49:41.921386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.921696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.921856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.924987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.925514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.925740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.926039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.926282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.926548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4335 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Reshape\n",
    "temperature = 0.03\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.keras' from '/usr/local/lib/python3.8/dist-packages/keras/api/_v2/keras/__init__.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Mar 13 2023, 10:26:41) \n",
      "[GCC 9.4.0]\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in ./.local/lib/python3.8/site-packages (0.20.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in ./.local/lib/python3.8/site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-addons --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(path, featType):\n",
    "    \"\"\"\n",
    "    Load data from a MATLAB file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the MATLAB file.\n",
    "        featType (int): Type of features to load.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Tuple containing input features, labels, weights, and additional information.\n",
    "    \"\"\"\n",
    "    data = scipy.io.loadmat(path)\n",
    "    print(data.keys())\n",
    "\n",
    "    AF = data['AF']\n",
    "    x1 = AF[:-2]\n",
    "    y = AF[-2]\n",
    "    w = AF[-1]\n",
    "\n",
    "    if featType == 1:\n",
    "        x = x1\n",
    "    else:\n",
    "        x2 = data['CF']\n",
    "        x = np.concatenate((x1, x2), axis=0)\n",
    "    return x.T, y.T, w.T, data['CF_info']\n",
    "\n",
    "def calculate_accuracy(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy between two arrays.\n",
    "\n",
    "    Args:\n",
    "        arr1 (array): First array.\n",
    "        arr2 (array): Second array.\n",
    "\n",
    "    Returns:ModuleNotFoundError: No module named 'keras'\n",
    "        float: Accuracy between the two arrays.\n",
    "    \"\"\"\n",
    "    count = sum(1 for itr1, itr2 in zip(arr1, arr2) if itr1 == itr2)\n",
    "    return count / len(arr1)\n",
    "\n",
    "def normalization(feats):\n",
    "\n",
    "    \"\"\"\n",
    "    Normalize the input features using standard scaling.\n",
    "\n",
    "    Args:\n",
    "        feats (array): Input features.\n",
    "\n",
    "    Returns:\n",
    "        array: Normalized features.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(feats)\n",
    "    scaler = StandardScaler()\n",
    "    x_new = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return x_new\n",
    "\n",
    "def make_partitions(arr_words, arr_labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Create partitions based on word boundaries and labels.\n",
    "\n",
    "    Args:\n",
    "        arr_words (array): Array of words.\n",
    "        arr_labels (array): Array of labels.\n",
    "\n",
    "    Returns:\n",
    "        array: Partitions based on word boundaries and labels.\n",
    "    \"\"\"\n",
    "    v = []\n",
    "    temp = []\n",
    "\n",
    "    for i in range(len(arr_words) - 1):\n",
    "        word = arr_words[i]\n",
    "        next_word = arr_words[i + 1]\n",
    "        temp.append(arr_labels[i])\n",
    "\n",
    "        if word != next_word or i == len(arr_words) - 2:\n",
    "            if i == len(arr_words) - 2:\n",
    "                temp.append(arr_labels[i + 1])\n",
    "\n",
    "            numpy_temp = np.array(temp)\n",
    "            temp_max = np.amax(numpy_temp)\n",
    "            numpy_temp = np.divide(numpy_temp, temp_max)\n",
    "            v = np.concatenate((v, numpy_temp), axis=None)\n",
    "            temp.clear()\n",
    "\n",
    "    v1 = [1 if i == 1 else 0 for i in v]\n",
    "    return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "featType = 1; #Acoustic or Acoustic+context\n",
    "if featType == 1:\n",
    "  original_dim = 19\n",
    "else:\n",
    "  original_dim = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "# print('Classification with::::::',os.path.basename(filee))\n",
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "# print('test file:::::::',os.path.basename(test_path))\n",
    "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
    "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
    "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_a = normalization(xtest)\n",
    "xtest_ac = normalization(xtest1)\n",
    "xtrain = normalization(xtrain)\n",
    "xtrain1 = normalization(xtrain1)\n",
    "\n",
    "woPP=[]; wPP=[]\n",
    "input_shape1 = (19,1)\n",
    "input_shape2 = (38,1)\n",
    "temperature = 0.03\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "#     def __init__(self, temperature=temperature, name=None):\n",
    "#         super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "#         self.temperature = temperature\n",
    "\n",
    "#     def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "#         # Normalize feature vectors\n",
    "#         print(feature_vectors.shape)\n",
    "#         # labels = tf.keras.layers.Concatenate(axis=0)([labels, labels])\n",
    "#         feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "#         # Compute logits\n",
    "#         logits = tf.divide(tf.matmul(feature_vectors_normalized, tf.transpose(feature_vectors_normalized)), self.temperature)\n",
    "        \n",
    "#         # print(feature_vectors.shape)\n",
    "#         # print(labels.shape)\n",
    "#         # print('loss:::::::', tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
    "#         return 0.35*(tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
    "#         #find out more about why 0.35 is used\n",
    "\n",
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=temperature, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        # Compute pairwise distances\n",
    "        pairwise_distances = tfa.losses.TripletSemiHardLoss()(labels, feature_vectors_normalized)\n",
    "\n",
    "        # Scale distances by temperature\n",
    "        scaled_distances = pairwise_distances / self.temperature\n",
    "        # Apply a hinge loss\n",
    "        triplet_loss = tf.maximum(scaled_distances - 1.0, 0)\n",
    "        return tf.reduce_mean(triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 19, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 38, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " encoder1 (Encoder1)            (None, 30)           20670       ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " encoder2 (Encoder2)            (None, 30)           38910       ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 60)           0           ['encoder1[0][0]',               \n",
      "                                                                  'encoder2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 16)           976         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 06:50:11.440274: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'y_pred' with dtype float and shape [?,16]\n",
      "\t [[{{node y_pred}}]]\n",
      "2023-06-07 06:50:11.721130: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/Max_1_grad/Sum/Max_1/reduction_indices' with dtype int32\n",
      "\t [[{{node gradients/Max_1_grad/Sum/Max_1/reduction_indices}}]]\n",
      "2023-06-07 06:50:11.728510: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/Min_grad/Sum/Min/reduction_indices' with dtype int32\n",
      "\t [[{{node gradients/Min_grad/Sum/Min/reduction_indices}}]]\n",
      "2023-06-07 06:50:11.758971: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/Min_1_grad/Sum/Min_1/reduction_indices' with dtype int32\n",
      "\t [[{{node gradients/Min_1_grad/Sum/Min_1/reduction_indices}}]]\n",
      "2023-06-07 06:50:11.766012: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/Max_grad/Sum/Max/reduction_indices' with dtype int32\n",
      "\t [[{{node gradients/Max_grad/Sum/Max/reduction_indices}}]]\n",
      "2023-06-07 06:50:11.855991: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall}}]]\n",
      "2023-06-07 06:50:11.856120: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_1' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_1}}]]\n",
      "2023-06-07 06:50:11.856225: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_2' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_2}}]]\n",
      "2023-06-07 06:50:11.856327: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_3' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_3}}]]\n",
      "2023-06-07 06:50:11.856427: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_4' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_4}}]]\n",
      "2023-06-07 06:50:11.856529: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_5' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_5}}]]\n",
      "2023-06-07 06:50:11.856628: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_6' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_6}}]]\n",
      "2023-06-07 06:50:11.856735: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_8' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_8}}]]\n",
      "2023-06-07 06:50:11.856858: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_9' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_9}}]]\n",
      "2023-06-07 06:50:11.856953: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_10' with dtype float and shape [?,1]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_10}}]]\n",
      "2023-06-07 06:50:11.857091: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_11' with dtype float and shape [1,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_11}}]]\n",
      "2023-06-07 06:50:11.857196: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_12' with dtype float\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_12}}]]\n",
      "2023-06-07 06:50:11.857300: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_13' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_13}}]]\n",
      "2023-06-07 06:50:11.857396: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_14' with dtype float and shape [?,16]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_14}}]]\n",
      "2023-06-07 06:50:11.857517: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_15' with dtype float and shape [16,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_15}}]]\n",
      "2023-06-07 06:50:11.857641: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_19' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_19}}]]\n",
      "2023-06-07 06:50:11.857766: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_20' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_20}}]]\n",
      "2023-06-07 06:50:15.192103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-07 06:50:16.855653: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f4d0b820720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-07 06:50:16.855770: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2023-06-07 06:50:16.898963: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-07 06:50:17.265345: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 11s 12ms/step - loss: 28.6964 - val_loss: 26.7255\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 26.3310 - val_loss: 26.5430\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 26.1408 - val_loss: 26.1444\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 26.3614 - val_loss: 26.5792\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 24.2594 - val_loss: 26.0884\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 24.8694 - val_loss: 24.0756\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 24.5575 - val_loss: 25.2481\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 2s 12ms/step - loss: 25.0431 - val_loss: 24.3037\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 23.3173 - val_loss: 23.2460\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 24.6874 - val_loss: 23.6735\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 24.1131 - val_loss: 23.7084\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 23.4859 - val_loss: 22.5974\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 23.8153 - val_loss: 23.5040\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 23.0564 - val_loss: 23.1015\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 22.8201 - val_loss: 21.3909\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 23.2112 - val_loss: 24.4882\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 22.8719 - val_loss: 22.7818\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 21.7014 - val_loss: 21.7138\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 21.3588 - val_loss: 20.8050\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 21.2908 - val_loss: 21.6268\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 20.4949 - val_loss: 22.0887\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 20.2498 - val_loss: 21.6878\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 20.8278 - val_loss: 21.5193\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 20.6678 - val_loss: 20.2215\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 20.8962 - val_loss: 21.0404\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 19.8080 - val_loss: 19.1531\n",
      "Epoch 27/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 18.0043 - val_loss: 17.4227\n",
      "Epoch 28/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 17.4727 - val_loss: 17.9981\n",
      "Epoch 29/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 17.0150 - val_loss: 17.2271\n",
      "Epoch 30/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 17.2190 - val_loss: 18.0019\n",
      "Epoch 31/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 17.0902 - val_loss: 17.2792\n",
      "Epoch 32/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 17.5300 - val_loss: 18.2404\n",
      "Epoch 33/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 17.6042 - val_loss: 17.3723\n",
      "Epoch 34/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 16.5765 - val_loss: 15.6505\n",
      "Epoch 35/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 15.0939 - val_loss: 16.1027\n",
      "Epoch 36/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 14.6871 - val_loss: 14.5096\n",
      "Epoch 37/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 15.4015 - val_loss: 15.8524\n",
      "Epoch 38/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 17.0087 - val_loss: 16.8447\n",
      "Epoch 39/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 15.7270 - val_loss: 15.0563\n",
      "Epoch 40/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 14.3086 - val_loss: 15.3111\n",
      "Epoch 41/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 14.3340 - val_loss: 15.1696\n",
      "Epoch 42/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 13.7446 - val_loss: 13.9174\n",
      "Epoch 43/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 12.8945 - val_loss: 13.7829\n",
      "Epoch 44/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 13.2084 - val_loss: 13.8919\n",
      "Epoch 45/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 14.0533 - val_loss: 14.5269\n",
      "Epoch 46/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 13.6680 - val_loss: 13.7934\n",
      "Epoch 47/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 12.8556 - val_loss: 9.7665\n",
      "Epoch 48/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.8200 - val_loss: 9.2591\n",
      "Epoch 49/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.3825 - val_loss: 9.4069\n",
      "Epoch 50/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.6682 - val_loss: 9.3226\n",
      "Epoch 51/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.9526 - val_loss: 9.3215\n",
      "Epoch 52/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.5726 - val_loss: 7.3630\n",
      "Epoch 53/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 8.8500 - val_loss: 7.3930\n",
      "Epoch 54/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.3411 - val_loss: 7.4366\n",
      "Epoch 55/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.7342 - val_loss: 7.4367\n",
      "Epoch 56/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 8.4804 - val_loss: 7.3489\n",
      "Epoch 57/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.9838 - val_loss: 7.4128\n",
      "Epoch 58/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.2298 - val_loss: 7.3727\n",
      "Epoch 59/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 7.9247 - val_loss: 8.0488\n",
      "Epoch 60/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.2508 - val_loss: 8.0488\n",
      "Epoch 61/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.4230 - val_loss: 8.0488\n",
      "Epoch 62/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 8.5357 - val_loss: 8.0488\n",
      "Epoch 63/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.1413 - val_loss: 8.0488\n",
      "Epoch 64/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.1597 - val_loss: 8.0488\n",
      "Epoch 65/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.5796 - val_loss: 8.0488\n",
      "Epoch 66/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.5785 - val_loss: 8.0488\n",
      "Epoch 66: early stopping\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1 (Encoder1)            (None, 30)           20670       ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " encoder2 (Encoder2)            (None, 30)           38910       ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 60)           0           ['encoder1[1][0]',               \n",
      "                                                                  'encoder2[1][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           3904        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64)          256         ['dense_3[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           2080        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 32)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 16)           528         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4)            68          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 2)            10          ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 7s 9ms/step - loss: 0.5824 - sparse_categorical_accuracy: 0.6664 - val_loss: 0.4585 - val_sparse_categorical_accuracy: 0.8015\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4670 - sparse_categorical_accuracy: 0.7814 - val_loss: 0.4090 - val_sparse_categorical_accuracy: 0.8168\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4421 - sparse_categorical_accuracy: 0.7984 - val_loss: 0.3962 - val_sparse_categorical_accuracy: 0.8295\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4306 - sparse_categorical_accuracy: 0.8058 - val_loss: 0.3778 - val_sparse_categorical_accuracy: 0.8346\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4231 - sparse_categorical_accuracy: 0.8054 - val_loss: 0.3697 - val_sparse_categorical_accuracy: 0.8422\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4190 - sparse_categorical_accuracy: 0.8011 - val_loss: 0.3680 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4162 - sparse_categorical_accuracy: 0.8156 - val_loss: 0.3639 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4149 - sparse_categorical_accuracy: 0.8105 - val_loss: 0.3609 - val_sparse_categorical_accuracy: 0.8482\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3961 - sparse_categorical_accuracy: 0.8247 - val_loss: 0.3510 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3982 - sparse_categorical_accuracy: 0.8241 - val_loss: 0.3527 - val_sparse_categorical_accuracy: 0.8490\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4008 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.3553 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3921 - sparse_categorical_accuracy: 0.8258 - val_loss: 0.3553 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3889 - sparse_categorical_accuracy: 0.8258 - val_loss: 0.3508 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3840 - sparse_categorical_accuracy: 0.8236 - val_loss: 0.3487 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3840 - sparse_categorical_accuracy: 0.8285 - val_loss: 0.3495 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3735 - sparse_categorical_accuracy: 0.8345 - val_loss: 0.3494 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3809 - sparse_categorical_accuracy: 0.8256 - val_loss: 0.3485 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3781 - sparse_categorical_accuracy: 0.8368 - val_loss: 0.3479 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3843 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.3455 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3782 - sparse_categorical_accuracy: 0.8283 - val_loss: 0.3451 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3755 - sparse_categorical_accuracy: 0.8292 - val_loss: 0.3401 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3717 - sparse_categorical_accuracy: 0.8366 - val_loss: 0.3389 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3608 - sparse_categorical_accuracy: 0.8398 - val_loss: 0.3390 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3672 - sparse_categorical_accuracy: 0.8385 - val_loss: 0.3351 - val_sparse_categorical_accuracy: 0.8626\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3680 - sparse_categorical_accuracy: 0.8368 - val_loss: 0.3384 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3664 - sparse_categorical_accuracy: 0.8374 - val_loss: 0.3353 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3736 - sparse_categorical_accuracy: 0.8345 - val_loss: 0.3412 - val_sparse_categorical_accuracy: 0.8592\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3602 - sparse_categorical_accuracy: 0.8370 - val_loss: 0.3368 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3696 - sparse_categorical_accuracy: 0.8336 - val_loss: 0.3371 - val_sparse_categorical_accuracy: 0.8592\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3516 - sparse_categorical_accuracy: 0.8461 - val_loss: 0.3353 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3625 - sparse_categorical_accuracy: 0.8381 - val_loss: 0.3330 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3603 - sparse_categorical_accuracy: 0.8391 - val_loss: 0.3317 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3598 - sparse_categorical_accuracy: 0.8451 - val_loss: 0.3335 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3608 - sparse_categorical_accuracy: 0.8421 - val_loss: 0.3339 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3681 - sparse_categorical_accuracy: 0.8406 - val_loss: 0.3354 - val_sparse_categorical_accuracy: 0.8660\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3563 - sparse_categorical_accuracy: 0.8355 - val_loss: 0.3305 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3614 - sparse_categorical_accuracy: 0.8406 - val_loss: 0.3322 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3557 - sparse_categorical_accuracy: 0.8400 - val_loss: 0.3317 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3601 - sparse_categorical_accuracy: 0.8376 - val_loss: 0.3377 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3474 - sparse_categorical_accuracy: 0.8485 - val_loss: 0.3394 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3640 - sparse_categorical_accuracy: 0.8387 - val_loss: 0.3324 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3576 - sparse_categorical_accuracy: 0.8406 - val_loss: 0.3314 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3601 - sparse_categorical_accuracy: 0.8434 - val_loss: 0.3278 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3606 - sparse_categorical_accuracy: 0.8436 - val_loss: 0.3353 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3518 - sparse_categorical_accuracy: 0.8472 - val_loss: 0.3255 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3460 - sparse_categorical_accuracy: 0.8508 - val_loss: 0.3356 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3408 - sparse_categorical_accuracy: 0.8474 - val_loss: 0.3276 - val_sparse_categorical_accuracy: 0.8660\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3579 - sparse_categorical_accuracy: 0.8419 - val_loss: 0.3314 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3474 - sparse_categorical_accuracy: 0.8504 - val_loss: 0.3313 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3447 - sparse_categorical_accuracy: 0.8553 - val_loss: 0.3317 - val_sparse_categorical_accuracy: 0.8694\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3502 - sparse_categorical_accuracy: 0.8480 - val_loss: 0.3310 - val_sparse_categorical_accuracy: 0.8626\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3359 - sparse_categorical_accuracy: 0.8514 - val_loss: 0.3293 - val_sparse_categorical_accuracy: 0.8660\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3426 - sparse_categorical_accuracy: 0.8504 - val_loss: 0.3339 - val_sparse_categorical_accuracy: 0.8609\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3502 - sparse_categorical_accuracy: 0.8463 - val_loss: 0.3342 - val_sparse_categorical_accuracy: 0.8668\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3480 - sparse_categorical_accuracy: 0.8476 - val_loss: 0.3265 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 55: early stopping\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.3189 - sparse_categorical_accuracy: 0.8659\n",
      "150/150 [==============================] - 0s 2ms/step\n",
      "Test accuracy: 86.594%\n",
      "Postprocessing Test accuracy: 90.812%\n",
      "Test F1_score: 84.858%\n",
      "Postprocessing F1_score: 89.549%\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 30\n",
    "\n",
    "class Encoder1(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Encoder1, self).__init__()\n",
    "    self.latent_dim = latent_dim \n",
    "    inputs = Input(shape=(19,1))\n",
    "    outputs = inputs  \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      inputs,\n",
    "      \n",
    "      layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    return encoded\n",
    "\n",
    "class Encoder2(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Encoder2, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    inputs = Input(shape=(38,1))\n",
    "    outputs = inputs  \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      inputs,\n",
    "      layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    return encoded\n",
    "  \n",
    "def create_encoder1():\n",
    "    return Encoder1(latent_dim)\n",
    "\n",
    "def create_encoder2():\n",
    "    return Encoder2(latent_dim)\n",
    "\n",
    "def add_projection_head1(Encoder1, Encoder2):\n",
    "    inp1 = keras.Input(shape=input_shape1)\n",
    "    inp2 = keras.Input(shape=input_shape2)\n",
    "    hidden3a  = Encoder1(inp1)\n",
    "    hidden3b = Encoder2(inp2)\n",
    "    features = tf.keras.layers.Concatenate(axis=1)([hidden3a, hidden3b])\n",
    "    features = layers.Dense(16, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=[inp1, inp2], outputs=features, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_classifier(encoder, trainable):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs1 = keras.Input(shape=input_shape1)\n",
    "    inputs2 = keras.Input(shape=input_shape2)\n",
    "    features1 = encoder1(inputs1)\n",
    "    features2 = encoder2(inputs2)\n",
    "    features = tf.keras.layers.Concatenate(axis=1)([features1, features2])\n",
    "    # features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    \n",
    "    features = layers.BatchNormalization()(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(32, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(16, activation=\"relu\")(features)\n",
    "    # features = layers.BatchNormalization()(features)\n",
    "    # features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(4, activation=\"relu\")(features)\n",
    "    # features = layers.BatchNormalization()(features)\n",
    "    # features = layers.Dropout(0.1)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=[inputs1,inputs2], outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# Splitting xtrain and ytrain into training and validation sets\n",
    "xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting xtrain1 and ytrain1 into training and validation sets\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "y = np.concatenate((ytra_a,ytra_a), axis=0)\n",
    "yv = np.concatenate((yval_a,yval_a), axis=0)\n",
    "\n",
    "encoder1 = create_encoder1()\n",
    "encoder2 = create_encoder2()\n",
    "encoder_with_projection_head = add_projection_head1(encoder1, encoder2)\n",
    "encoder_with_projection_head.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),loss=SupervisedContrastiveLoss(temperature))\n",
    "encoder_with_projection_head.summary()\n",
    "                                                            #ytra_a                                  #yval_a\n",
    "history = encoder_with_projection_head.fit([xtra_a,xtra_ac], ytra_ac , validation_data =([xval_a,xval_ac],yval_ac), batch_size=32, epochs=100, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "\n",
    "learning_rate = 0.0005\n",
    "batch_size = 16\n",
    "hidden_units = 64\n",
    "projection_units = 128\n",
    "num_epochs = 200\n",
    "dropout_rate = 0.3\n",
    "num_classes = 2\n",
    "input_shape1 = (19,)\n",
    "input_shape2 = (38,)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint ,EarlyStopping\n",
    "classifier = create_classifier(encoder_with_projection_head, trainable=False)\n",
    "classifier.summary()\n",
    "history = classifier.fit(x=[xtra_a,xtra_ac], y=ytra_a, validation_data =([xval_a,xval_ac],yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "accuracy = classifier.evaluate([xtest_a,xtest_ac], ytest)[1]\n",
    "\n",
    "##  Accuracy on post processed labels (each word should contain only one stressed syllable)\n",
    "pred_output= classifier.predict([xtest_a,xtest_ac])\n",
    "# pred_labels= pred_output.argmax(axis =1)\n",
    "pred1_labels = pred_output[:,1]\n",
    "post_labels = make_partitions(wtest, pred1_labels)\n",
    "post_accuracy = calculate_accuracy(post_labels, ytest)\n",
    "\n",
    "F1_score_WoPP = f1_score(ytest, pred_output.argmax(axis =1))\n",
    "F1_score_WPP = f1_score(ytest, post_labels)\n",
    "\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 3)}%\")\n",
    "print(f\"Postprocessing Test accuracy: {round(post_accuracy * 100, 3)}%\")\n",
    "print(f\"Test F1_score: {round(F1_score_WoPP * 100, 3)}%\")\n",
    "print(f\"Postprocessing F1_score: {round(F1_score_WPP * 100, 3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_1 (Encoder1)          (None, 30)           20670       ['input_9[0][0]']                \n",
      "                                                                                                  \n",
      " encoder2_1 (Encoder2)          (None, 30)           38910       ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 60)           0           ['encoder1_1[0][0]',             \n",
      "                                                                  'encoder2_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 16)           976         ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "148/148 [==============================] - 4s 8ms/step - loss: 14.2842 - val_loss: 13.8505\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 13.6963 - val_loss: 13.2037\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 13.1363 - val_loss: 12.6002\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 12.9996 - val_loss: 12.1998\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 12.6369 - val_loss: 12.1098\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 12.9127 - val_loss: 12.2998\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 12.6321 - val_loss: 11.7853\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 12.2379 - val_loss: 11.3444\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 12.0581 - val_loss: 11.4270\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 11.5877 - val_loss: 12.3016\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 12.1563 - val_loss: 12.2459\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 11.9455 - val_loss: 12.9952\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 12.3259 - val_loss: 11.8083\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 11.5830 - val_loss: 11.4295\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 11.4699 - val_loss: 11.2214\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 11.5845 - val_loss: 11.4409\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 11.1868 - val_loss: 11.1275\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 11.1336 - val_loss: 11.3962\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.9207 - val_loss: 10.7278\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.4236 - val_loss: 10.9820\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.3953 - val_loss: 10.1652\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.3230 - val_loss: 9.8188\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.4025 - val_loss: 10.3100\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.5819 - val_loss: 10.3325\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.3785 - val_loss: 9.7674\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.3346 - val_loss: 10.6632\n",
      "Epoch 27/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.6535 - val_loss: 10.6682\n",
      "Epoch 28/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.5098 - val_loss: 10.5341\n",
      "Epoch 29/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.2454 - val_loss: 9.9802\n",
      "Epoch 30/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.0829 - val_loss: 10.4055\n",
      "Epoch 31/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.0205 - val_loss: 9.3601\n",
      "Epoch 32/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 9.9400 - val_loss: 9.5012\n",
      "Epoch 33/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 9.9098 - val_loss: 10.2409\n",
      "Epoch 34/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 9.2246 - val_loss: 10.0716\n",
      "Epoch 35/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 10.2492 - val_loss: 9.8835\n",
      "Epoch 36/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.1991 - val_loss: 9.2469\n",
      "Epoch 37/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 9.2432 - val_loss: 9.0039\n",
      "Epoch 38/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 9.0319 - val_loss: 9.0500\n",
      "Epoch 39/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 9.0283 - val_loss: 8.4901\n",
      "Epoch 40/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.5726 - val_loss: 8.5037\n",
      "Epoch 41/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.8459 - val_loss: 8.9972\n",
      "Epoch 42/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.5894 - val_loss: 8.5821\n",
      "Epoch 43/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 8.4908 - val_loss: 9.4675\n",
      "Epoch 44/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 8.7515 - val_loss: 8.3230\n",
      "Epoch 45/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.3829 - val_loss: 8.5690\n",
      "Epoch 46/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 8.5208 - val_loss: 9.3017\n",
      "Epoch 47/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 9.0803 - val_loss: 8.9272\n",
      "Epoch 48/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.4647 - val_loss: 8.9260\n",
      "Epoch 49/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.2317 - val_loss: 8.3688\n",
      "Epoch 50/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.8597 - val_loss: 8.1578\n",
      "Epoch 51/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.6904 - val_loss: 7.9177\n",
      "Epoch 52/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.2345 - val_loss: 7.4717\n",
      "Epoch 53/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.7495 - val_loss: 7.3958\n",
      "Epoch 54/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.9080 - val_loss: 7.3914\n",
      "Epoch 55/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.7479 - val_loss: 7.3237\n",
      "Epoch 56/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.7632 - val_loss: 7.0553\n",
      "Epoch 57/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.4713 - val_loss: 6.9048\n",
      "Epoch 58/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.4144 - val_loss: 6.9775\n",
      "Epoch 59/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.4338 - val_loss: 6.6939\n",
      "Epoch 60/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 6.4050 - val_loss: 7.1569\n",
      "Epoch 61/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.4059 - val_loss: 6.5986\n",
      "Epoch 62/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.1628 - val_loss: 6.9172\n",
      "Epoch 63/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.2710 - val_loss: 7.1743\n",
      "Epoch 64/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 6.4409 - val_loss: 7.0274\n",
      "Epoch 65/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.0571 - val_loss: 6.5299\n",
      "Epoch 66/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 5.8009 - val_loss: 6.5487\n",
      "Epoch 67/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 6.2291 - val_loss: 6.5995\n",
      "Epoch 68/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.0951 - val_loss: 6.4351\n",
      "Epoch 69/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 6.1565 - val_loss: 6.5555\n",
      "Epoch 70/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.0607 - val_loss: 6.2842\n",
      "Epoch 71/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.0157 - val_loss: 6.3433\n",
      "Epoch 72/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 6.0972 - val_loss: 6.0148\n",
      "Epoch 73/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 5.6816 - val_loss: 6.0993\n",
      "Epoch 74/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 5.9900 - val_loss: 6.4768\n",
      "Epoch 75/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 6.4793 - val_loss: 5.5807\n",
      "Epoch 76/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 6.1310 - val_loss: 6.0305\n",
      "Epoch 77/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 6.9440 - val_loss: 6.0305\n",
      "Epoch 78/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 6.4779 - val_loss: 6.0305\n",
      "Epoch 79/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 7.2389 - val_loss: 6.2072\n",
      "Epoch 80/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.8673 - val_loss: 6.2494\n",
      "Epoch 81/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.6494 - val_loss: 6.2582\n",
      "Epoch 82/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.0105 - val_loss: 6.2343\n",
      "Epoch 83/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 7.1984 - val_loss: 6.2188\n",
      "Epoch 84/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.8277 - val_loss: 6.4646\n",
      "Epoch 85/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.0493 - val_loss: 6.5368\n",
      "Epoch 85: early stopping\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_1 (Encoder1)          (None, 30)           20670       ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_1 (Encoder2)          (None, 30)           38910       ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 60)           0           ['encoder1_1[1][0]',             \n",
      "                                                                  'encoder2_1[1][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64)           3904        ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64)          256         ['dense_11[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 64)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 32)           2080        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 32)           0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 16)           528         ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4)            68          ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 2)            10          ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 4s 7ms/step - loss: 0.5596 - sparse_categorical_accuracy: 0.6870 - val_loss: 0.4533 - val_sparse_categorical_accuracy: 0.7998\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4628 - sparse_categorical_accuracy: 0.7880 - val_loss: 0.4031 - val_sparse_categorical_accuracy: 0.8312\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.4204 - sparse_categorical_accuracy: 0.8154 - val_loss: 0.3818 - val_sparse_categorical_accuracy: 0.8388\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.4033 - sparse_categorical_accuracy: 0.8224 - val_loss: 0.3696 - val_sparse_categorical_accuracy: 0.8465\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3893 - sparse_categorical_accuracy: 0.8304 - val_loss: 0.3646 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3729 - sparse_categorical_accuracy: 0.8374 - val_loss: 0.3561 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3810 - sparse_categorical_accuracy: 0.8330 - val_loss: 0.3532 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3702 - sparse_categorical_accuracy: 0.8440 - val_loss: 0.3468 - val_sparse_categorical_accuracy: 0.8567\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3686 - sparse_categorical_accuracy: 0.8427 - val_loss: 0.3425 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.3701 - sparse_categorical_accuracy: 0.8381 - val_loss: 0.3467 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3603 - sparse_categorical_accuracy: 0.8455 - val_loss: 0.3427 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3574 - sparse_categorical_accuracy: 0.8472 - val_loss: 0.3409 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3498 - sparse_categorical_accuracy: 0.8463 - val_loss: 0.3382 - val_sparse_categorical_accuracy: 0.8567\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3543 - sparse_categorical_accuracy: 0.8468 - val_loss: 0.3416 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3452 - sparse_categorical_accuracy: 0.8523 - val_loss: 0.3442 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3475 - sparse_categorical_accuracy: 0.8512 - val_loss: 0.3363 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3534 - sparse_categorical_accuracy: 0.8493 - val_loss: 0.3363 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3401 - sparse_categorical_accuracy: 0.8557 - val_loss: 0.3358 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3448 - sparse_categorical_accuracy: 0.8527 - val_loss: 0.3264 - val_sparse_categorical_accuracy: 0.8668\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3484 - sparse_categorical_accuracy: 0.8525 - val_loss: 0.3265 - val_sparse_categorical_accuracy: 0.8685\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3367 - sparse_categorical_accuracy: 0.8610 - val_loss: 0.3263 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3333 - sparse_categorical_accuracy: 0.8565 - val_loss: 0.3279 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3302 - sparse_categorical_accuracy: 0.8614 - val_loss: 0.3249 - val_sparse_categorical_accuracy: 0.8626\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3314 - sparse_categorical_accuracy: 0.8565 - val_loss: 0.3254 - val_sparse_categorical_accuracy: 0.8626\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3385 - sparse_categorical_accuracy: 0.8561 - val_loss: 0.3244 - val_sparse_categorical_accuracy: 0.8609\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.3265 - sparse_categorical_accuracy: 0.8591 - val_loss: 0.3228 - val_sparse_categorical_accuracy: 0.8685\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3376 - sparse_categorical_accuracy: 0.8536 - val_loss: 0.3217 - val_sparse_categorical_accuracy: 0.8702\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3355 - sparse_categorical_accuracy: 0.8580 - val_loss: 0.3251 - val_sparse_categorical_accuracy: 0.8711\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3264 - sparse_categorical_accuracy: 0.8663 - val_loss: 0.3202 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3308 - sparse_categorical_accuracy: 0.8565 - val_loss: 0.3176 - val_sparse_categorical_accuracy: 0.8762\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3292 - sparse_categorical_accuracy: 0.8597 - val_loss: 0.3210 - val_sparse_categorical_accuracy: 0.8685\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3279 - sparse_categorical_accuracy: 0.8578 - val_loss: 0.3207 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3330 - sparse_categorical_accuracy: 0.8508 - val_loss: 0.3240 - val_sparse_categorical_accuracy: 0.8677\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3232 - sparse_categorical_accuracy: 0.8618 - val_loss: 0.3200 - val_sparse_categorical_accuracy: 0.8702\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3149 - sparse_categorical_accuracy: 0.8701 - val_loss: 0.3182 - val_sparse_categorical_accuracy: 0.8626\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3256 - sparse_categorical_accuracy: 0.8584 - val_loss: 0.3173 - val_sparse_categorical_accuracy: 0.8685\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3288 - sparse_categorical_accuracy: 0.8567 - val_loss: 0.3219 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3178 - sparse_categorical_accuracy: 0.8650 - val_loss: 0.3180 - val_sparse_categorical_accuracy: 0.8711\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3252 - sparse_categorical_accuracy: 0.8610 - val_loss: 0.3163 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3245 - sparse_categorical_accuracy: 0.8616 - val_loss: 0.3184 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3109 - sparse_categorical_accuracy: 0.8684 - val_loss: 0.3231 - val_sparse_categorical_accuracy: 0.8745\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3165 - sparse_categorical_accuracy: 0.8633 - val_loss: 0.3137 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3151 - sparse_categorical_accuracy: 0.8701 - val_loss: 0.3144 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3237 - sparse_categorical_accuracy: 0.8610 - val_loss: 0.3114 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3156 - sparse_categorical_accuracy: 0.8627 - val_loss: 0.3129 - val_sparse_categorical_accuracy: 0.8779\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3128 - sparse_categorical_accuracy: 0.8661 - val_loss: 0.3137 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3149 - sparse_categorical_accuracy: 0.8680 - val_loss: 0.3122 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3098 - sparse_categorical_accuracy: 0.8674 - val_loss: 0.3149 - val_sparse_categorical_accuracy: 0.8753\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3146 - sparse_categorical_accuracy: 0.8646 - val_loss: 0.3158 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3141 - sparse_categorical_accuracy: 0.8631 - val_loss: 0.3129 - val_sparse_categorical_accuracy: 0.8702\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3128 - sparse_categorical_accuracy: 0.8746 - val_loss: 0.3124 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3194 - sparse_categorical_accuracy: 0.8657 - val_loss: 0.3086 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3081 - sparse_categorical_accuracy: 0.8699 - val_loss: 0.3090 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3123 - sparse_categorical_accuracy: 0.8674 - val_loss: 0.3111 - val_sparse_categorical_accuracy: 0.8753\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3085 - sparse_categorical_accuracy: 0.8671 - val_loss: 0.3128 - val_sparse_categorical_accuracy: 0.8745\n",
      "Epoch 56/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3098 - sparse_categorical_accuracy: 0.8697 - val_loss: 0.3141 - val_sparse_categorical_accuracy: 0.8745\n",
      "Epoch 57/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3056 - sparse_categorical_accuracy: 0.8661 - val_loss: 0.3161 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 58/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3023 - sparse_categorical_accuracy: 0.8718 - val_loss: 0.3148 - val_sparse_categorical_accuracy: 0.8694\n",
      "Epoch 59/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3097 - sparse_categorical_accuracy: 0.8682 - val_loss: 0.3114 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 60/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2982 - sparse_categorical_accuracy: 0.8748 - val_loss: 0.3134 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 61/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3043 - sparse_categorical_accuracy: 0.8720 - val_loss: 0.3097 - val_sparse_categorical_accuracy: 0.8745\n",
      "Epoch 62/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3005 - sparse_categorical_accuracy: 0.8754 - val_loss: 0.3072 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 63/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3033 - sparse_categorical_accuracy: 0.8750 - val_loss: 0.3019 - val_sparse_categorical_accuracy: 0.8779\n",
      "Epoch 64/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2967 - sparse_categorical_accuracy: 0.8737 - val_loss: 0.3062 - val_sparse_categorical_accuracy: 0.8711\n",
      "Epoch 65/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3040 - sparse_categorical_accuracy: 0.8671 - val_loss: 0.3052 - val_sparse_categorical_accuracy: 0.8745\n",
      "Epoch 66/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3045 - sparse_categorical_accuracy: 0.8699 - val_loss: 0.3102 - val_sparse_categorical_accuracy: 0.8677\n",
      "Epoch 67/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3011 - sparse_categorical_accuracy: 0.8763 - val_loss: 0.3073 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 68/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2962 - sparse_categorical_accuracy: 0.8727 - val_loss: 0.3038 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 69/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3101 - sparse_categorical_accuracy: 0.8712 - val_loss: 0.3008 - val_sparse_categorical_accuracy: 0.8787\n",
      "Epoch 70/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3104 - sparse_categorical_accuracy: 0.8729 - val_loss: 0.3031 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 71/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2963 - sparse_categorical_accuracy: 0.8720 - val_loss: 0.3024 - val_sparse_categorical_accuracy: 0.8804\n",
      "Epoch 72/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2979 - sparse_categorical_accuracy: 0.8744 - val_loss: 0.3035 - val_sparse_categorical_accuracy: 0.8796\n",
      "Epoch 73/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3020 - sparse_categorical_accuracy: 0.8695 - val_loss: 0.3016 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 74/200\n",
      "295/295 [==============================] - 3s 9ms/step - loss: 0.2961 - sparse_categorical_accuracy: 0.8761 - val_loss: 0.3055 - val_sparse_categorical_accuracy: 0.8796\n",
      "Epoch 75/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2921 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.3069 - val_sparse_categorical_accuracy: 0.8762\n",
      "Epoch 76/200\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.3047 - sparse_categorical_accuracy: 0.8699 - val_loss: 0.3047 - val_sparse_categorical_accuracy: 0.8745\n",
      "Epoch 77/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2947 - sparse_categorical_accuracy: 0.8790 - val_loss: 0.3049 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 78/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2988 - sparse_categorical_accuracy: 0.8720 - val_loss: 0.3024 - val_sparse_categorical_accuracy: 0.8745\n",
      "Epoch 79/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2935 - sparse_categorical_accuracy: 0.8769 - val_loss: 0.2993 - val_sparse_categorical_accuracy: 0.8796\n",
      "Epoch 80/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3013 - sparse_categorical_accuracy: 0.8746 - val_loss: 0.3018 - val_sparse_categorical_accuracy: 0.8813\n",
      "Epoch 81/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2836 - sparse_categorical_accuracy: 0.8792 - val_loss: 0.3075 - val_sparse_categorical_accuracy: 0.8779\n",
      "Epoch 82/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2849 - sparse_categorical_accuracy: 0.8748 - val_loss: 0.2981 - val_sparse_categorical_accuracy: 0.8855\n",
      "Epoch 83/200\n",
      "295/295 [==============================] - 3s 9ms/step - loss: 0.2953 - sparse_categorical_accuracy: 0.8716 - val_loss: 0.3068 - val_sparse_categorical_accuracy: 0.8796\n",
      "Epoch 84/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2899 - sparse_categorical_accuracy: 0.8816 - val_loss: 0.3055 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 85/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2920 - sparse_categorical_accuracy: 0.8769 - val_loss: 0.3030 - val_sparse_categorical_accuracy: 0.8813\n",
      "Epoch 86/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2922 - sparse_categorical_accuracy: 0.8763 - val_loss: 0.3048 - val_sparse_categorical_accuracy: 0.8796\n",
      "Epoch 87/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2937 - sparse_categorical_accuracy: 0.8799 - val_loss: 0.3059 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 88/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2959 - sparse_categorical_accuracy: 0.8729 - val_loss: 0.3045 - val_sparse_categorical_accuracy: 0.8762\n",
      "Epoch 89/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2958 - sparse_categorical_accuracy: 0.8727 - val_loss: 0.3099 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 90/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2957 - sparse_categorical_accuracy: 0.8814 - val_loss: 0.2974 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 91/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2870 - sparse_categorical_accuracy: 0.8750 - val_loss: 0.3069 - val_sparse_categorical_accuracy: 0.8804\n",
      "Epoch 92/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2928 - sparse_categorical_accuracy: 0.8784 - val_loss: 0.3017 - val_sparse_categorical_accuracy: 0.8753\n",
      "Epoch 93/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2899 - sparse_categorical_accuracy: 0.8765 - val_loss: 0.3036 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 94/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2906 - sparse_categorical_accuracy: 0.8790 - val_loss: 0.3017 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 95/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2893 - sparse_categorical_accuracy: 0.8807 - val_loss: 0.2978 - val_sparse_categorical_accuracy: 0.8838\n",
      "Epoch 96/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2896 - sparse_categorical_accuracy: 0.8814 - val_loss: 0.3026 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 97/200\n",
      "295/295 [==============================] - 3s 9ms/step - loss: 0.2822 - sparse_categorical_accuracy: 0.8812 - val_loss: 0.3026 - val_sparse_categorical_accuracy: 0.8796\n",
      "Epoch 98/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2834 - sparse_categorical_accuracy: 0.8826 - val_loss: 0.3042 - val_sparse_categorical_accuracy: 0.8813\n",
      "Epoch 99/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2969 - sparse_categorical_accuracy: 0.8784 - val_loss: 0.3031 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 100/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2882 - sparse_categorical_accuracy: 0.8795 - val_loss: 0.2965 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 101/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2857 - sparse_categorical_accuracy: 0.8856 - val_loss: 0.2988 - val_sparse_categorical_accuracy: 0.8830\n",
      "Epoch 102/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2890 - sparse_categorical_accuracy: 0.8803 - val_loss: 0.2920 - val_sparse_categorical_accuracy: 0.8863\n",
      "Epoch 103/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2782 - sparse_categorical_accuracy: 0.8816 - val_loss: 0.3039 - val_sparse_categorical_accuracy: 0.8838\n",
      "Epoch 104/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2817 - sparse_categorical_accuracy: 0.8824 - val_loss: 0.3060 - val_sparse_categorical_accuracy: 0.8753\n",
      "Epoch 105/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2898 - sparse_categorical_accuracy: 0.8775 - val_loss: 0.2982 - val_sparse_categorical_accuracy: 0.8830\n",
      "Epoch 106/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2907 - sparse_categorical_accuracy: 0.8758 - val_loss: 0.2951 - val_sparse_categorical_accuracy: 0.8796\n",
      "Epoch 107/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2912 - sparse_categorical_accuracy: 0.8784 - val_loss: 0.2916 - val_sparse_categorical_accuracy: 0.8863\n",
      "Epoch 108/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2845 - sparse_categorical_accuracy: 0.8797 - val_loss: 0.2888 - val_sparse_categorical_accuracy: 0.8863\n",
      "Epoch 109/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2960 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.2900 - val_sparse_categorical_accuracy: 0.8872\n",
      "Epoch 110/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2873 - sparse_categorical_accuracy: 0.8807 - val_loss: 0.3022 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 111/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2833 - sparse_categorical_accuracy: 0.8812 - val_loss: 0.2924 - val_sparse_categorical_accuracy: 0.8863\n",
      "Epoch 112/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2875 - sparse_categorical_accuracy: 0.8801 - val_loss: 0.2956 - val_sparse_categorical_accuracy: 0.8838\n",
      "Epoch 113/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2813 - sparse_categorical_accuracy: 0.8807 - val_loss: 0.2959 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 114/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2808 - sparse_categorical_accuracy: 0.8795 - val_loss: 0.2970 - val_sparse_categorical_accuracy: 0.8838\n",
      "Epoch 115/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2865 - sparse_categorical_accuracy: 0.8754 - val_loss: 0.2936 - val_sparse_categorical_accuracy: 0.8813\n",
      "Epoch 116/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2811 - sparse_categorical_accuracy: 0.8786 - val_loss: 0.3067 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 117/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2808 - sparse_categorical_accuracy: 0.8807 - val_loss: 0.2988 - val_sparse_categorical_accuracy: 0.8813\n",
      "Epoch 118/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2850 - sparse_categorical_accuracy: 0.8833 - val_loss: 0.2959 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 118: early stopping\n",
      "150/150 [==============================] - 1s 3ms/step - loss: 0.2961 - sparse_categorical_accuracy: 0.8824\n",
      "150/150 [==============================] - 1s 3ms/step\n",
      "Test accuracy: 88.244%\n",
      "Postprocessing Test accuracy: 91.522%\n",
      "Test F1_score: 86.934%\n",
      "Postprocessing F1_score: 90.356%\n"
     ]
    }
   ],
   "source": [
    "elements = {1, 0.5,0.35,0.3,0.25,0.2,0.15,0.1,0.05}\n",
    "accuracies = []\n",
    "f1scores= []\n",
    "\n",
    "# Iterate through the set and print each element\n",
    "for element in elements:\n",
    "  class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=temperature, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        # Compute pairwise distances\n",
    "        pairwise_distances = tfa.losses.TripletSemiHardLoss()(labels, feature_vectors_normalized)\n",
    "\n",
    "        # Scale distances by temperature\n",
    "        scaled_distances = pairwise_distances / self.temperature\n",
    "        # Apply a hinge loss\n",
    "        triplet_loss = tf.maximum(scaled_distances - 1.0, 0)\n",
    "        return (element)*(tf.reduce_mean(triplet_loss))\n",
    "\n",
    "  latent_dim = 30\n",
    "  class Encoder1(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Encoder1, self).__init__()\n",
    "    self.latent_dim = latent_dim \n",
    "    inputs = Input(shape=(19,1))\n",
    "    outputs = inputs  \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      inputs,\n",
    "      \n",
    "      layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    return encoded\n",
    "\n",
    "  class Encoder2(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Encoder2, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    inputs = Input(shape=(38,1))\n",
    "    outputs = inputs  \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      inputs,\n",
    "      layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    return encoded\n",
    "\n",
    "  def create_encoder1():\n",
    "    return Encoder1(latent_dim)\n",
    "\n",
    "  def create_encoder2():\n",
    "    return Encoder2(latent_dim)\n",
    "\n",
    "  def add_projection_head1(Encoder1, Encoder2):\n",
    "    inp1 = keras.Input(shape=input_shape1)\n",
    "    inp2 = keras.Input(shape=input_shape2)\n",
    "    hidden3a  = Encoder1(inp1)\n",
    "    hidden3b = Encoder2(inp2)\n",
    "    features = tf.keras.layers.Concatenate(axis=1)([hidden3a, hidden3b])\n",
    "    features = layers.Dense(16, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=[inp1, inp2], outputs=features, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "  def create_classifier(encoder, trainable):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs1 = keras.Input(shape=input_shape1)\n",
    "    inputs2 = keras.Input(shape=input_shape2)\n",
    "    features1 = encoder1(inputs1)\n",
    "    features2 = encoder2(inputs2)\n",
    "    features = tf.keras.layers.Concatenate(axis=1)([features1, features2])\n",
    "    # features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    \n",
    "    features = layers.BatchNormalization()(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(32, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(16, activation=\"relu\")(features)\n",
    "    # features = layers.BatchNormalization()(features)\n",
    "    # features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(4, activation=\"relu\")(features)\n",
    "    # features = layers.BatchNormalization()(features)\n",
    "    # features = layers.Dropout(0.1)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=[inputs1,inputs2], outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  from sklearn.metrics import f1_score\n",
    "  # Splitting xtrain and ytrain into training and validation sets\n",
    "  xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Splitting xtrain1 and ytrain1 into training and validation sets\n",
    "  xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "  y = np.concatenate((ytra_a,ytra_a), axis=0)\n",
    "  yv = np.concatenate((yval_a,yval_a), axis=0)\n",
    "\n",
    "  encoder1 = create_encoder1()\n",
    "  encoder2 = create_encoder2()\n",
    "  encoder_with_projection_head = add_projection_head1(encoder1, encoder2)\n",
    "  encoder_with_projection_head.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),loss=SupervisedContrastiveLoss(temperature))\n",
    "  encoder_with_projection_head.summary()\n",
    "                                                            #ytra_a                                  #yval_a\n",
    "  history = encoder_with_projection_head.fit([xtra_a,xtra_ac], ytra_ac , validation_data =([xval_a,xval_ac],yval_ac), batch_size=32, epochs=100, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "\n",
    "  learning_rate = 0.0005\n",
    "  batch_size = 16\n",
    "  hidden_units = 64\n",
    "  projection_units = 128\n",
    "  num_epochs = 200\n",
    "  dropout_rate = 0.3\n",
    "  num_classes = 2\n",
    "  input_shape1 = (19,)\n",
    "  input_shape2 = (38,)\n",
    "\n",
    "  from keras.callbacks import ModelCheckpoint ,EarlyStopping\n",
    "  classifier = create_classifier(encoder_with_projection_head, trainable=False)\n",
    "  classifier.summary()\n",
    "  history = classifier.fit(x=[xtra_a,xtra_ac], y=ytra_a, validation_data =([xval_a,xval_ac],yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "  accuracy = classifier.evaluate([xtest_a,xtest_ac], ytest)[1]\n",
    "\n",
    "  ##  Accuracy on post processed labels (each word should contain only one stressed syllable)\n",
    "  pred_output= classifier.predict([xtest_a,xtest_ac])\n",
    "  # pred_labels= pred_output.argmax(axis =1)\n",
    "  pred1_labels = pred_output[:,1]\n",
    "  post_labels = make_partitions(wtest, pred1_labels)\n",
    "  post_accuracy = calculate_accuracy(post_labels, ytest)\n",
    "\n",
    "  F1_score_WoPP = f1_score(ytest, pred_output.argmax(axis =1))\n",
    "  F1_score_WPP = f1_score(ytest, post_labels)\n",
    "\n",
    "  print(f\"Test accuracy: {round(accuracy * 100, 3)}%\")\n",
    "  print(f\"Postprocessing Test accuracy: {round(post_accuracy * 100, 3)}%\")\n",
    "  print(f\"Test F1_score: {round(F1_score_WoPP * 100, 3)}%\")\n",
    "  print(f\"Postprocessing F1_score: {round(F1_score_WPP * 100, 3)}%\")\n",
    "  accuracies.append(round(accuracy * 100, 3))\n",
    "  f1scores.append(round(accuracy * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
