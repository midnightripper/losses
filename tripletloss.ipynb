{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 14:04:03.330276: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in ./.local/lib/python3.8/site-packages (0.20.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in ./.local/lib/python3.8/site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-addons --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1263625906176704716\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4613603328\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13048804928024639337\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 14:04:10.571498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:10.599045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:10.599240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:11.710787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:11.710979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:11.711135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:11.711254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 4399 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2023-06-06 14:04:16.553090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:16.553327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:16.553464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:16.556330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:16.556630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:16.556767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:16.556938: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:16.557076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-06 14:04:16.557188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4399 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Reshape\n",
    "temperature = 0.03\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.keras' from '/usr/local/lib/python3.8/dist-packages/keras/api/_v2/keras/__init__.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Mar 13 2023, 10:26:41) \n",
      "[GCC 9.4.0]\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in ./.local/lib/python3.8/site-packages (0.20.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in ./.local/lib/python3.8/site-packages (from tensorflow-addons) (2.13.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(path, featType):\n",
    "    \"\"\"\n",
    "    Load data from a MATLAB file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the MATLAB file.\n",
    "        featType (int): Type of features to load.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Tuple containing input features, labels, weights, and additional information.\n",
    "    \"\"\"\n",
    "    data = scipy.io.loadmat(path)\n",
    "    print(data.keys())\n",
    "\n",
    "    AF = data['AF']\n",
    "    x1 = AF[:-2]\n",
    "    y = AF[-2]\n",
    "    w = AF[-1]\n",
    "\n",
    "    if featType == 1:\n",
    "        x = x1\n",
    "    else:\n",
    "        x2 = data['CF']\n",
    "        x = np.concatenate((x1, x2), axis=0)\n",
    "    return x.T, y.T, w.T, data['CF_info']\n",
    "\n",
    "def calculate_accuracy(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy between two arrays.\n",
    "\n",
    "    Args:\n",
    "        arr1 (array): First array.\n",
    "        arr2 (array): Second array.\n",
    "\n",
    "    Returns:ModuleNotFoundError: No module named 'keras'\n",
    "        float: Accuracy between the two arrays.\n",
    "    \"\"\"\n",
    "    count = sum(1 for itr1, itr2 in zip(arr1, arr2) if itr1 == itr2)\n",
    "    return count / len(arr1)\n",
    "\n",
    "def normalization(feats):\n",
    "\n",
    "    \"\"\"\n",
    "    Normalize the input features using standard scaling.\n",
    "\n",
    "    Args:\n",
    "        feats (array): Input features.\n",
    "\n",
    "    Returns:\n",
    "        array: Normalized features.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(feats)\n",
    "    scaler = StandardScaler()\n",
    "    x_new = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return x_new\n",
    "\n",
    "def make_partitions(arr_words, arr_labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Create partitions based on word boundaries and labels.\n",
    "\n",
    "    Args:\n",
    "        arr_words (array): Array of words.\n",
    "        arr_labels (array): Array of labels.\n",
    "\n",
    "    Returns:\n",
    "        array: Partitions based on word boundaries and labels.\n",
    "    \"\"\"\n",
    "    v = []\n",
    "    temp = []\n",
    "\n",
    "    for i in range(len(arr_words) - 1):\n",
    "        word = arr_words[i]\n",
    "        next_word = arr_words[i + 1]\n",
    "        temp.append(arr_labels[i])\n",
    "\n",
    "        if word != next_word or i == len(arr_words) - 2:\n",
    "            if i == len(arr_words) - 2:\n",
    "                temp.append(arr_labels[i + 1])\n",
    "\n",
    "            numpy_temp = np.array(temp)\n",
    "            temp_max = np.amax(numpy_temp)\n",
    "            numpy_temp = np.divide(numpy_temp, temp_max)\n",
    "            v = np.concatenate((v, numpy_temp), axis=None)\n",
    "            temp.clear()\n",
    "\n",
    "    v1 = [1 if i == 1 else 0 for i in v]\n",
    "    return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "featType = 1; #Acoustic or Acoustic+context\n",
    "if featType == 1:\n",
    "  original_dim = 19\n",
    "else:\n",
    "  original_dim = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "# print('Classification with::::::',os.path.basename(filee))\n",
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "# print('test file:::::::',os.path.basename(test_path))\n",
    "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
    "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
    "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_a = normalization(xtest)\n",
    "xtest_ac = normalization(xtest1)\n",
    "xtrain = normalization(xtrain)\n",
    "xtrain1 = normalization(xtrain1)\n",
    "\n",
    "woPP=[]; wPP=[]\n",
    "input_shape1 = (19,1)\n",
    "input_shape2 = (38,1)\n",
    "temperature = 0.03\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "#     def __init__(self, temperature=temperature, name=None):\n",
    "#         super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "#         self.temperature = temperature\n",
    "\n",
    "#     def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "#         # Normalize feature vectors\n",
    "#         print(feature_vectors.shape)\n",
    "#         # labels = tf.keras.layers.Concatenate(axis=0)([labels, labels])\n",
    "#         feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "#         # Compute logits\n",
    "#         logits = tf.divide(tf.matmul(feature_vectors_normalized, tf.transpose(feature_vectors_normalized)), self.temperature)\n",
    "        \n",
    "#         # print(feature_vectors.shape)\n",
    "#         # print(labels.shape)\n",
    "#         # print('loss:::::::', tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
    "#         return 0.35*(tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
    "#         #find out more about why 0.35 is used\n",
    "\n",
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=temperature, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        # Compute pairwise distances\n",
    "        pairwise_distances = tfa.losses.TripletSemiHardLoss()(labels, feature_vectors_normalized)\n",
    "\n",
    "        # Scale distances by temperature\n",
    "        scaled_distances = pairwise_distances / self.temperature\n",
    "        # Apply a hinge loss\n",
    "        triplet_loss = tf.maximum(scaled_distances - 1.0, 0)\n",
    "        return tf.reduce_mean(triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_2 (Encoder1)          (None, 30)           20670       ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_2 (Encoder2)          (None, 30)           38910       ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 60)           0           ['encoder1_2[0][0]',             \n",
      "                                                                  'encoder2_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 16)           976         ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "(None, 16)\n",
      "(None, 16)\n",
      "148/148 [==============================] - ETA: 0s - loss: 1.2983(None, 16)\n",
      "148/148 [==============================] - 6s 9ms/step - loss: 1.2983 - val_loss: 1.2005\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.2013 - val_loss: 1.1904\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1925 - val_loss: 1.1866\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1874 - val_loss: 1.1800\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1845 - val_loss: 1.1774\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.1796 - val_loss: 1.1757\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.1779 - val_loss: 1.1687\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.1735 - val_loss: 1.1656\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.1700 - val_loss: 1.1635\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 1.1640 - val_loss: 1.1575\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 1.1588 - val_loss: 1.1544\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 1.1571 - val_loss: 1.1492\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 1.1522 - val_loss: 1.1428\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.1477 - val_loss: 1.1475\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 1.1476 - val_loss: 1.1416\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 1.1418 - val_loss: 1.1392\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1380 - val_loss: 1.1422\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1320 - val_loss: 1.1483\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.1333 - val_loss: 1.1354\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1304 - val_loss: 1.1354\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 1.1256 - val_loss: 1.1355\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 1.1252 - val_loss: 1.1337\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.1217 - val_loss: 1.1289\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1200 - val_loss: 1.1288\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1181 - val_loss: 1.1302\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1148 - val_loss: 1.1342\n",
      "Epoch 27/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1141 - val_loss: 1.1350\n",
      "Epoch 28/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.1134 - val_loss: 1.1291\n",
      "Epoch 29/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.1092 - val_loss: 1.1254\n",
      "Epoch 30/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.1073 - val_loss: 1.1315\n",
      "Epoch 31/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.1071 - val_loss: 1.1289\n",
      "Epoch 32/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 1.1049 - val_loss: 1.1288\n",
      "Epoch 33/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1034 - val_loss: 1.1236\n",
      "Epoch 34/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1012 - val_loss: 1.1309\n",
      "Epoch 35/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1006 - val_loss: 1.1348\n",
      "Epoch 36/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0995 - val_loss: 1.1221\n",
      "Epoch 37/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0973 - val_loss: 1.1239\n",
      "Epoch 38/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0946 - val_loss: 1.1244\n",
      "Epoch 39/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0947 - val_loss: 1.1294\n",
      "Epoch 40/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0928 - val_loss: 1.1251\n",
      "Epoch 41/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0904 - val_loss: 1.1301\n",
      "Epoch 42/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0906 - val_loss: 1.1274\n",
      "Epoch 43/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0885 - val_loss: 1.1238\n",
      "Epoch 44/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0874 - val_loss: 1.1251\n",
      "Epoch 45/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0883 - val_loss: 1.1223\n",
      "Epoch 46/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0859 - val_loss: 1.1176\n",
      "Epoch 47/100\n",
      "148/148 [==============================] - 1s 6ms/step - loss: 1.0827 - val_loss: 1.1209\n",
      "Epoch 48/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0804 - val_loss: 1.1261\n",
      "Epoch 49/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0816 - val_loss: 1.1242\n",
      "Epoch 50/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0780 - val_loss: 1.1231\n",
      "Epoch 51/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0807 - val_loss: 1.1236\n",
      "Epoch 52/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0787 - val_loss: 1.1172\n",
      "Epoch 53/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0746 - val_loss: 1.1229\n",
      "Epoch 54/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0730 - val_loss: 1.1229\n",
      "Epoch 55/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0725 - val_loss: 1.1205\n",
      "Epoch 56/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0710 - val_loss: 1.1285\n",
      "Epoch 57/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0673 - val_loss: 1.1292\n",
      "Epoch 58/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0650 - val_loss: 1.1301\n",
      "Epoch 59/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.0688 - val_loss: 1.1261\n",
      "Epoch 60/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0641 - val_loss: 1.1249\n",
      "Epoch 61/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0652 - val_loss: 1.1305\n",
      "Epoch 62/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.0619 - val_loss: 1.1278\n",
      "Epoch 62: early stopping\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_2 (Encoder1)          (None, 30)           20670       ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_2 (Encoder2)          (None, 30)           38910       ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 60)           0           ['encoder1_2[1][0]',             \n",
      "                                                                  'encoder2_2[1][0]']             \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 64)           3904        ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64)          256         ['dense_14[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 64)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 32)           2080        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 32)           0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 16)           528         ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 4)            68          ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 2)            10          ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 4s 6ms/step - loss: 0.5491 - sparse_categorical_accuracy: 0.7067 - val_loss: 0.3808 - val_sparse_categorical_accuracy: 0.8753\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3199 - sparse_categorical_accuracy: 0.8674 - val_loss: 0.2760 - val_sparse_categorical_accuracy: 0.8957\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2480 - sparse_categorical_accuracy: 0.8998 - val_loss: 0.2746 - val_sparse_categorical_accuracy: 0.8965\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2227 - sparse_categorical_accuracy: 0.9172 - val_loss: 0.2704 - val_sparse_categorical_accuracy: 0.8982\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2149 - sparse_categorical_accuracy: 0.9211 - val_loss: 0.2692 - val_sparse_categorical_accuracy: 0.9008\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.2086 - sparse_categorical_accuracy: 0.9228 - val_loss: 0.2615 - val_sparse_categorical_accuracy: 0.9016\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.1625 - sparse_categorical_accuracy: 0.9402 - val_loss: 0.2805 - val_sparse_categorical_accuracy: 0.9016\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.1745 - sparse_categorical_accuracy: 0.9380 - val_loss: 0.2746 - val_sparse_categorical_accuracy: 0.9042\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.1702 - sparse_categorical_accuracy: 0.9382 - val_loss: 0.2801 - val_sparse_categorical_accuracy: 0.9033\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.1679 - sparse_categorical_accuracy: 0.9374 - val_loss: 0.2803 - val_sparse_categorical_accuracy: 0.8982\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.1628 - sparse_categorical_accuracy: 0.9408 - val_loss: 0.2906 - val_sparse_categorical_accuracy: 0.8965\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.1610 - sparse_categorical_accuracy: 0.9442 - val_loss: 0.2879 - val_sparse_categorical_accuracy: 0.9008\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.1664 - sparse_categorical_accuracy: 0.9431 - val_loss: 0.2899 - val_sparse_categorical_accuracy: 0.9033\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.1567 - sparse_categorical_accuracy: 0.9414 - val_loss: 0.2906 - val_sparse_categorical_accuracy: 0.9033\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.1494 - sparse_categorical_accuracy: 0.9457 - val_loss: 0.2919 - val_sparse_categorical_accuracy: 0.9059\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.1607 - sparse_categorical_accuracy: 0.9410 - val_loss: 0.2902 - val_sparse_categorical_accuracy: 0.8999\n",
      "Epoch 16: early stopping\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.2932 - sparse_categorical_accuracy: 0.9019\n",
      "150/150 [==============================] - 0s 2ms/step\n",
      "Test accuracy: 90.186%\n",
      "Postprocessing Test accuracy: 93.36%\n",
      "Test F1_score: 88.767%\n",
      "Postprocessing F1_score: 92.447%\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 30\n",
    "\n",
    "class Encoder1(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Encoder1, self).__init__()\n",
    "    self.latent_dim = latent_dim \n",
    "    inputs = Input(shape=(19,1))\n",
    "    outputs = inputs  \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      inputs,\n",
    "      \n",
    "      layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    return encoded\n",
    "\n",
    "class Encoder2(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Encoder2, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    inputs = Input(shape=(38,1))\n",
    "    outputs = inputs  \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      inputs,\n",
    "      layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    return encoded\n",
    "  \n",
    "def create_encoder1():\n",
    "    return Encoder1(latent_dim)\n",
    "\n",
    "def create_encoder2():\n",
    "    return Encoder2(latent_dim)\n",
    "\n",
    "def add_projection_head1(Encoder1, Encoder2):\n",
    "    inp1 = keras.Input(shape=input_shape1)\n",
    "    inp2 = keras.Input(shape=input_shape2)\n",
    "    hidden3a  = Encoder1(inp1)\n",
    "    hidden3b = Encoder2(inp2)\n",
    "    features = tf.keras.layers.Concatenate(axis=1)([hidden3a, hidden3b])\n",
    "    features = layers.Dense(16, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=[inp1, inp2], outputs=features, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_classifier(encoder, trainable):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs1 = keras.Input(shape=input_shape1)\n",
    "    inputs2 = keras.Input(shape=input_shape2)\n",
    "    features1 = encoder1(inputs1)\n",
    "    features2 = encoder2(inputs2)\n",
    "    features = tf.keras.layers.Concatenate(axis=1)([features1, features2])\n",
    "    # features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    \n",
    "    features = layers.BatchNormalization()(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(32, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(16, activation=\"relu\")(features)\n",
    "    # features = layers.BatchNormalization()(features)\n",
    "    # features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(4, activation=\"relu\")(features)\n",
    "    # features = layers.BatchNormalization()(features)\n",
    "    # features = layers.Dropout(0.1)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=[inputs1,inputs2], outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# Splitting xtrain and ytrain into training and validation sets\n",
    "xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting xtrain1 and ytrain1 into training and validation sets\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "y = np.concatenate((ytra_a,ytra_a), axis=0)\n",
    "yv = np.concatenate((yval_a,yval_a), axis=0)\n",
    "\n",
    "encoder1 = create_encoder1()\n",
    "encoder2 = create_encoder2()\n",
    "encoder_with_projection_head = add_projection_head1(encoder1, encoder2)\n",
    "encoder_with_projection_head.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),loss=SupervisedContrastiveLoss(temperature))\n",
    "encoder_with_projection_head.summary()\n",
    "                                                            #ytra_a                                  #yval_a\n",
    "history = encoder_with_projection_head.fit([xtra_a,xtra_ac], ytra_ac , validation_data =([xval_a,xval_ac],yval_ac), batch_size=32, epochs=100, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "\n",
    "learning_rate = 0.0005\n",
    "batch_size = 16\n",
    "hidden_units = 64\n",
    "projection_units = 128\n",
    "num_epochs = 200\n",
    "dropout_rate = 0.3\n",
    "num_classes = 2\n",
    "input_shape1 = (19,)\n",
    "input_shape2 = (38,)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint ,EarlyStopping\n",
    "classifier = create_classifier(encoder_with_projection_head, trainable=False)\n",
    "classifier.summary()\n",
    "history = classifier.fit(x=[xtra_a,xtra_ac], y=ytra_a, validation_data =([xval_a,xval_ac],yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "accuracy = classifier.evaluate([xtest_a,xtest_ac], ytest)[1]\n",
    "\n",
    "##  Accuracy on post processed labels (each word should contain only one stressed syllable)\n",
    "pred_output= classifier.predict([xtest_a,xtest_ac])\n",
    "# pred_labels= pred_output.argmax(axis =1)\n",
    "pred1_labels = pred_output[:,1]\n",
    "post_labels = make_partitions(wtest, pred1_labels)\n",
    "post_accuracy = calculate_accuracy(post_labels, ytest)\n",
    "\n",
    "F1_score_WoPP = f1_score(ytest, pred_output.argmax(axis =1))\n",
    "F1_score_WPP = f1_score(ytest, post_labels)\n",
    "\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 3)}%\")\n",
    "print(f\"Postprocessing Test accuracy: {round(post_accuracy * 100, 3)}%\")\n",
    "print(f\"Test F1_score: {round(F1_score_WoPP * 100, 3)}%\")\n",
    "print(f\"Postprocessing F1_score: {round(F1_score_WPP * 100, 3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
