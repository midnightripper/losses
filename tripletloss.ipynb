{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m388.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (3.7.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m357.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m218.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m391.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.39.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (5.12.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (9.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.11.0)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m299.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.10.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 2)) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.14.0)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, joblib, scikit-learn, pandas\n",
      "Successfully installed joblib-1.2.0 pandas-2.0.2 pytz-2023.3 scikit-learn-1.2.2 threadpoolctl-3.1.0 tzdata-2023.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 06:49:26.019748: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in ./.local/lib/python3.8/site-packages (0.20.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in ./.local/lib/python3.8/site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-addons --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14618623979168308874\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4545708032\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15074625132866972223\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 06:49:36.074756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.083914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.084283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.865326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.865738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.865919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:36.866077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 4335 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2023-06-07 06:49:41.921386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.921696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.921856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.924987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.925514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.925740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.926039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.926282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-07 06:49:41.926548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4335 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Reshape\n",
    "temperature = 0.03\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.keras' from '/usr/local/lib/python3.8/dist-packages/keras/api/_v2/keras/__init__.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Mar 13 2023, 10:26:41) \n",
      "[GCC 9.4.0]\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in ./.local/lib/python3.8/site-packages (0.20.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in ./.local/lib/python3.8/site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-addons --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(path, featType):\n",
    "    \"\"\"\n",
    "    Load data from a MATLAB file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the MATLAB file.\n",
    "        featType (int): Type of features to load.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Tuple containing input features, labels, weights, and additional information.\n",
    "    \"\"\"\n",
    "    data = scipy.io.loadmat(path)\n",
    "    print(data.keys())\n",
    "\n",
    "    AF = data['AF']\n",
    "    x1 = AF[:-2]\n",
    "    y = AF[-2]\n",
    "    w = AF[-1]\n",
    "\n",
    "    if featType == 1:\n",
    "        x = x1\n",
    "    else:\n",
    "        x2 = data['CF']\n",
    "        x = np.concatenate((x1, x2), axis=0)\n",
    "    return x.T, y.T, w.T, data['CF_info']\n",
    "\n",
    "def calculate_accuracy(arr1, arr2):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy between two arrays.\n",
    "\n",
    "    Args:\n",
    "        arr1 (array): First array.\n",
    "        arr2 (array): Second array.\n",
    "\n",
    "    Returns:ModuleNotFoundError: No module named 'keras'\n",
    "        float: Accuracy between the two arrays.\n",
    "    \"\"\"\n",
    "    count = sum(1 for itr1, itr2 in zip(arr1, arr2) if itr1 == itr2)\n",
    "    return count / len(arr1)\n",
    "\n",
    "def normalization(feats):\n",
    "\n",
    "    \"\"\"\n",
    "    Normalize the input features using standard scaling.\n",
    "\n",
    "    Args:\n",
    "        feats (array): Input features.\n",
    "\n",
    "    Returns:\n",
    "        array: Normalized features.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(feats)\n",
    "    scaler = StandardScaler()\n",
    "    x_new = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return x_new\n",
    "\n",
    "def make_partitions(arr_words, arr_labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Create partitions based on word boundaries and labels.\n",
    "\n",
    "    Args:\n",
    "        arr_words (array): Array of words.\n",
    "        arr_labels (array): Array of labels.\n",
    "\n",
    "    Returns:\n",
    "        array: Partitions based on word boundaries and labels.\n",
    "    \"\"\"\n",
    "    v = []\n",
    "    temp = []\n",
    "\n",
    "    for i in range(len(arr_words) - 1):\n",
    "        word = arr_words[i]\n",
    "        next_word = arr_words[i + 1]\n",
    "        temp.append(arr_labels[i])\n",
    "\n",
    "        if word != next_word or i == len(arr_words) - 2:\n",
    "            if i == len(arr_words) - 2:\n",
    "                temp.append(arr_labels[i + 1])\n",
    "\n",
    "            numpy_temp = np.array(temp)\n",
    "            temp_max = np.amax(numpy_temp)\n",
    "            numpy_temp = np.divide(numpy_temp, temp_max)\n",
    "            v = np.concatenate((v, numpy_temp), axis=None)\n",
    "            temp.clear()\n",
    "\n",
    "    v1 = [1 if i == 1 else 0 for i in v]\n",
    "    return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatyp = 'TypicalFA_comb1'\n",
    "drivepath = 'finalData/'+ fatyp +'/';\n",
    "#featFiles = 'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress'               #glob.glob(drivepath + '*train*')\n",
    "filee = drivepath+'GER_train_fisher-2000_FA_GT_ESTphnTrans_estStress.mat'\n",
    "featType = 1; #Acoustic or Acoustic+context\n",
    "if featType == 1:\n",
    "  original_dim = 19\n",
    "else:\n",
    "  original_dim = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'AF', 'AF_info', 'CF', 'CF_info'])\n"
     ]
    }
   ],
   "source": [
    "# print('Classification with::::::',os.path.basename(filee))\n",
    "\n",
    "train_path = filee; test_path = filee.replace('train','test')\n",
    "# print('test file:::::::',os.path.basename(test_path))\n",
    "xtrain, ytrain, wtrain, info_train = dataloader(train_path, featType); \n",
    "xtrain1, ytrain1, wtrain1, info_train1 = dataloader(train_path, featType=2);\n",
    "xtest, ytest, wtest ,info_test = dataloader(test_path, featType)\n",
    "xtest1, ytest1, wtest1, info_test1 = dataloader(test_path, featType=2);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_a = normalization(xtest)\n",
    "xtest_ac = normalization(xtest1)\n",
    "xtrain = normalization(xtrain)\n",
    "xtrain1 = normalization(xtrain1)\n",
    "\n",
    "woPP=[]; wPP=[]\n",
    "input_shape1 = (19,1)\n",
    "input_shape2 = (38,1)\n",
    "temperature = 0.03\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "#     def __init__(self, temperature=temperature, name=None):\n",
    "#         super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "#         self.temperature = temperature\n",
    "\n",
    "#     def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "#         # Normalize feature vectors\n",
    "#         print(feature_vectors.shape)\n",
    "#         # labels = tf.keras.layers.Concatenate(axis=0)([labels, labels])\n",
    "#         feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "#         # Compute logits\n",
    "#         logits = tf.divide(tf.matmul(feature_vectors_normalized, tf.transpose(feature_vectors_normalized)), self.temperature)\n",
    "        \n",
    "#         # print(feature_vectors.shape)\n",
    "#         # print(labels.shape)\n",
    "#         # print('loss:::::::', tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
    "#         return 0.35*(tfa.losses.npairs_loss(tf.squeeze(labels), logits))\n",
    "#         #find out more about why 0.35 is used\n",
    "\n",
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=temperature, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        # Compute pairwise distances\n",
    "        pairwise_distances = tfa.losses.TripletSemiHardLoss()(labels, feature_vectors_normalized)\n",
    "\n",
    "        # Scale distances by temperature\n",
    "        scaled_distances = pairwise_distances / self.temperature\n",
    "        # Apply a hinge loss\n",
    "        triplet_loss = tf.maximum(scaled_distances - 1.0, 0)\n",
    "        return tf.reduce_mean(triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 19, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 38, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " encoder1 (Encoder1)            (None, 30)           20670       ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " encoder2 (Encoder2)            (None, 30)           38910       ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 60)           0           ['encoder1[0][0]',               \n",
      "                                                                  'encoder2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 16)           976         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 06:50:11.440274: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'y_pred' with dtype float and shape [?,16]\n",
      "\t [[{{node y_pred}}]]\n",
      "2023-06-07 06:50:11.721130: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/Max_1_grad/Sum/Max_1/reduction_indices' with dtype int32\n",
      "\t [[{{node gradients/Max_1_grad/Sum/Max_1/reduction_indices}}]]\n",
      "2023-06-07 06:50:11.728510: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/Min_grad/Sum/Min/reduction_indices' with dtype int32\n",
      "\t [[{{node gradients/Min_grad/Sum/Min/reduction_indices}}]]\n",
      "2023-06-07 06:50:11.758971: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/Min_1_grad/Sum/Min_1/reduction_indices' with dtype int32\n",
      "\t [[{{node gradients/Min_1_grad/Sum/Min_1/reduction_indices}}]]\n",
      "2023-06-07 06:50:11.766012: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/Max_grad/Sum/Max/reduction_indices' with dtype int32\n",
      "\t [[{{node gradients/Max_grad/Sum/Max/reduction_indices}}]]\n",
      "2023-06-07 06:50:11.855991: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall}}]]\n",
      "2023-06-07 06:50:11.856120: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_1' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_1}}]]\n",
      "2023-06-07 06:50:11.856225: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_2' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_2}}]]\n",
      "2023-06-07 06:50:11.856327: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_3' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_3}}]]\n",
      "2023-06-07 06:50:11.856427: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_4' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_4}}]]\n",
      "2023-06-07 06:50:11.856529: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_5' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_5}}]]\n",
      "2023-06-07 06:50:11.856628: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_6' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_6}}]]\n",
      "2023-06-07 06:50:11.856735: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_8' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_8}}]]\n",
      "2023-06-07 06:50:11.856858: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_9' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_9}}]]\n",
      "2023-06-07 06:50:11.856953: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_10' with dtype float and shape [?,1]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_10}}]]\n",
      "2023-06-07 06:50:11.857091: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_11' with dtype float and shape [1,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_11}}]]\n",
      "2023-06-07 06:50:11.857196: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_12' with dtype float\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_12}}]]\n",
      "2023-06-07 06:50:11.857300: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_13' with dtype float and shape [?,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_13}}]]\n",
      "2023-06-07 06:50:11.857396: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_14' with dtype float and shape [?,16]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_14}}]]\n",
      "2023-06-07 06:50:11.857517: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_15' with dtype float and shape [16,?]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_15}}]]\n",
      "2023-06-07 06:50:11.857641: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_19' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_19}}]]\n",
      "2023-06-07 06:50:11.857766: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/PartitionedCall_grad/PartitionedCall_20' with dtype int32 and shape [2]\n",
      "\t [[{{node gradients/PartitionedCall_grad/PartitionedCall_20}}]]\n",
      "2023-06-07 06:50:15.192103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-07 06:50:16.855653: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f4d0b820720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-07 06:50:16.855770: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2023-06-07 06:50:16.898963: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-07 06:50:17.265345: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 11s 12ms/step - loss: 28.6964 - val_loss: 26.7255\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 26.3310 - val_loss: 26.5430\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 26.1408 - val_loss: 26.1444\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 26.3614 - val_loss: 26.5792\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 24.2594 - val_loss: 26.0884\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 24.8694 - val_loss: 24.0756\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 24.5575 - val_loss: 25.2481\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 2s 12ms/step - loss: 25.0431 - val_loss: 24.3037\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 23.3173 - val_loss: 23.2460\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 24.6874 - val_loss: 23.6735\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 24.1131 - val_loss: 23.7084\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 23.4859 - val_loss: 22.5974\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 23.8153 - val_loss: 23.5040\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 23.0564 - val_loss: 23.1015\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 22.8201 - val_loss: 21.3909\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 23.2112 - val_loss: 24.4882\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 22.8719 - val_loss: 22.7818\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 21.7014 - val_loss: 21.7138\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 21.3588 - val_loss: 20.8050\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 21.2908 - val_loss: 21.6268\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 20.4949 - val_loss: 22.0887\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 20.2498 - val_loss: 21.6878\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 20.8278 - val_loss: 21.5193\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 20.6678 - val_loss: 20.2215\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 20.8962 - val_loss: 21.0404\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 19.8080 - val_loss: 19.1531\n",
      "Epoch 27/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 18.0043 - val_loss: 17.4227\n",
      "Epoch 28/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 17.4727 - val_loss: 17.9981\n",
      "Epoch 29/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 17.0150 - val_loss: 17.2271\n",
      "Epoch 30/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 17.2190 - val_loss: 18.0019\n",
      "Epoch 31/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 17.0902 - val_loss: 17.2792\n",
      "Epoch 32/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 17.5300 - val_loss: 18.2404\n",
      "Epoch 33/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 17.6042 - val_loss: 17.3723\n",
      "Epoch 34/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 16.5765 - val_loss: 15.6505\n",
      "Epoch 35/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 15.0939 - val_loss: 16.1027\n",
      "Epoch 36/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 14.6871 - val_loss: 14.5096\n",
      "Epoch 37/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 15.4015 - val_loss: 15.8524\n",
      "Epoch 38/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 17.0087 - val_loss: 16.8447\n",
      "Epoch 39/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 15.7270 - val_loss: 15.0563\n",
      "Epoch 40/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 14.3086 - val_loss: 15.3111\n",
      "Epoch 41/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 14.3340 - val_loss: 15.1696\n",
      "Epoch 42/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 13.7446 - val_loss: 13.9174\n",
      "Epoch 43/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 12.8945 - val_loss: 13.7829\n",
      "Epoch 44/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 13.2084 - val_loss: 13.8919\n",
      "Epoch 45/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 14.0533 - val_loss: 14.5269\n",
      "Epoch 46/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 13.6680 - val_loss: 13.7934\n",
      "Epoch 47/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 12.8556 - val_loss: 9.7665\n",
      "Epoch 48/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.8200 - val_loss: 9.2591\n",
      "Epoch 49/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.3825 - val_loss: 9.4069\n",
      "Epoch 50/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.6682 - val_loss: 9.3226\n",
      "Epoch 51/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.9526 - val_loss: 9.3215\n",
      "Epoch 52/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.5726 - val_loss: 7.3630\n",
      "Epoch 53/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 8.8500 - val_loss: 7.3930\n",
      "Epoch 54/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.3411 - val_loss: 7.4366\n",
      "Epoch 55/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.7342 - val_loss: 7.4367\n",
      "Epoch 56/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 8.4804 - val_loss: 7.3489\n",
      "Epoch 57/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.9838 - val_loss: 7.4128\n",
      "Epoch 58/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.2298 - val_loss: 7.3727\n",
      "Epoch 59/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 7.9247 - val_loss: 8.0488\n",
      "Epoch 60/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.2508 - val_loss: 8.0488\n",
      "Epoch 61/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.4230 - val_loss: 8.0488\n",
      "Epoch 62/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 8.5357 - val_loss: 8.0488\n",
      "Epoch 63/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.1413 - val_loss: 8.0488\n",
      "Epoch 64/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.1597 - val_loss: 8.0488\n",
      "Epoch 65/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.5796 - val_loss: 8.0488\n",
      "Epoch 66/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.5785 - val_loss: 8.0488\n",
      "Epoch 66: early stopping\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1 (Encoder1)            (None, 30)           20670       ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " encoder2 (Encoder2)            (None, 30)           38910       ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 60)           0           ['encoder1[1][0]',               \n",
      "                                                                  'encoder2[1][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           3904        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64)          256         ['dense_3[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           2080        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 32)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 16)           528         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4)            68          ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 2)            10          ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 7s 9ms/step - loss: 0.5824 - sparse_categorical_accuracy: 0.6664 - val_loss: 0.4585 - val_sparse_categorical_accuracy: 0.8015\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4670 - sparse_categorical_accuracy: 0.7814 - val_loss: 0.4090 - val_sparse_categorical_accuracy: 0.8168\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4421 - sparse_categorical_accuracy: 0.7984 - val_loss: 0.3962 - val_sparse_categorical_accuracy: 0.8295\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4306 - sparse_categorical_accuracy: 0.8058 - val_loss: 0.3778 - val_sparse_categorical_accuracy: 0.8346\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4231 - sparse_categorical_accuracy: 0.8054 - val_loss: 0.3697 - val_sparse_categorical_accuracy: 0.8422\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4190 - sparse_categorical_accuracy: 0.8011 - val_loss: 0.3680 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4162 - sparse_categorical_accuracy: 0.8156 - val_loss: 0.3639 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4149 - sparse_categorical_accuracy: 0.8105 - val_loss: 0.3609 - val_sparse_categorical_accuracy: 0.8482\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3961 - sparse_categorical_accuracy: 0.8247 - val_loss: 0.3510 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3982 - sparse_categorical_accuracy: 0.8241 - val_loss: 0.3527 - val_sparse_categorical_accuracy: 0.8490\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4008 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.3553 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3921 - sparse_categorical_accuracy: 0.8258 - val_loss: 0.3553 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3889 - sparse_categorical_accuracy: 0.8258 - val_loss: 0.3508 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3840 - sparse_categorical_accuracy: 0.8236 - val_loss: 0.3487 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3840 - sparse_categorical_accuracy: 0.8285 - val_loss: 0.3495 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3735 - sparse_categorical_accuracy: 0.8345 - val_loss: 0.3494 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3809 - sparse_categorical_accuracy: 0.8256 - val_loss: 0.3485 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3781 - sparse_categorical_accuracy: 0.8368 - val_loss: 0.3479 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3843 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.3455 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3782 - sparse_categorical_accuracy: 0.8283 - val_loss: 0.3451 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3755 - sparse_categorical_accuracy: 0.8292 - val_loss: 0.3401 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3717 - sparse_categorical_accuracy: 0.8366 - val_loss: 0.3389 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3608 - sparse_categorical_accuracy: 0.8398 - val_loss: 0.3390 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3672 - sparse_categorical_accuracy: 0.8385 - val_loss: 0.3351 - val_sparse_categorical_accuracy: 0.8626\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3680 - sparse_categorical_accuracy: 0.8368 - val_loss: 0.3384 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3664 - sparse_categorical_accuracy: 0.8374 - val_loss: 0.3353 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3736 - sparse_categorical_accuracy: 0.8345 - val_loss: 0.3412 - val_sparse_categorical_accuracy: 0.8592\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3602 - sparse_categorical_accuracy: 0.8370 - val_loss: 0.3368 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3696 - sparse_categorical_accuracy: 0.8336 - val_loss: 0.3371 - val_sparse_categorical_accuracy: 0.8592\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3516 - sparse_categorical_accuracy: 0.8461 - val_loss: 0.3353 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3625 - sparse_categorical_accuracy: 0.8381 - val_loss: 0.3330 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3603 - sparse_categorical_accuracy: 0.8391 - val_loss: 0.3317 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3598 - sparse_categorical_accuracy: 0.8451 - val_loss: 0.3335 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3608 - sparse_categorical_accuracy: 0.8421 - val_loss: 0.3339 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3681 - sparse_categorical_accuracy: 0.8406 - val_loss: 0.3354 - val_sparse_categorical_accuracy: 0.8660\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3563 - sparse_categorical_accuracy: 0.8355 - val_loss: 0.3305 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3614 - sparse_categorical_accuracy: 0.8406 - val_loss: 0.3322 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3557 - sparse_categorical_accuracy: 0.8400 - val_loss: 0.3317 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3601 - sparse_categorical_accuracy: 0.8376 - val_loss: 0.3377 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3474 - sparse_categorical_accuracy: 0.8485 - val_loss: 0.3394 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3640 - sparse_categorical_accuracy: 0.8387 - val_loss: 0.3324 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3576 - sparse_categorical_accuracy: 0.8406 - val_loss: 0.3314 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3601 - sparse_categorical_accuracy: 0.8434 - val_loss: 0.3278 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3606 - sparse_categorical_accuracy: 0.8436 - val_loss: 0.3353 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3518 - sparse_categorical_accuracy: 0.8472 - val_loss: 0.3255 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3460 - sparse_categorical_accuracy: 0.8508 - val_loss: 0.3356 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3408 - sparse_categorical_accuracy: 0.8474 - val_loss: 0.3276 - val_sparse_categorical_accuracy: 0.8660\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3579 - sparse_categorical_accuracy: 0.8419 - val_loss: 0.3314 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3474 - sparse_categorical_accuracy: 0.8504 - val_loss: 0.3313 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3447 - sparse_categorical_accuracy: 0.8553 - val_loss: 0.3317 - val_sparse_categorical_accuracy: 0.8694\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3502 - sparse_categorical_accuracy: 0.8480 - val_loss: 0.3310 - val_sparse_categorical_accuracy: 0.8626\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 1s 5ms/step - loss: 0.3359 - sparse_categorical_accuracy: 0.8514 - val_loss: 0.3293 - val_sparse_categorical_accuracy: 0.8660\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3426 - sparse_categorical_accuracy: 0.8504 - val_loss: 0.3339 - val_sparse_categorical_accuracy: 0.8609\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3502 - sparse_categorical_accuracy: 0.8463 - val_loss: 0.3342 - val_sparse_categorical_accuracy: 0.8668\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 1s 4ms/step - loss: 0.3480 - sparse_categorical_accuracy: 0.8476 - val_loss: 0.3265 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 55: early stopping\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.3189 - sparse_categorical_accuracy: 0.8659\n",
      "150/150 [==============================] - 0s 2ms/step\n",
      "Test accuracy: 86.594%\n",
      "Postprocessing Test accuracy: 90.812%\n",
      "Test F1_score: 84.858%\n",
      "Postprocessing F1_score: 89.549%\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 30\n",
    "\n",
    "class Encoder1(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Encoder1, self).__init__()\n",
    "    self.latent_dim = latent_dim \n",
    "    inputs = Input(shape=(19,1))\n",
    "    outputs = inputs  \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      inputs,\n",
    "      \n",
    "      layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    return encoded\n",
    "\n",
    "class Encoder2(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Encoder2, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    inputs = Input(shape=(38,1))\n",
    "    outputs = inputs  \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      inputs,\n",
    "      layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "      layers.MaxPooling1D(pool_size=1),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    return encoded\n",
    "  \n",
    "def create_encoder1():\n",
    "    return Encoder1(latent_dim)\n",
    "\n",
    "def create_encoder2():\n",
    "    return Encoder2(latent_dim)\n",
    "\n",
    "def add_projection_head1(Encoder1, Encoder2):\n",
    "    inp1 = keras.Input(shape=input_shape1)\n",
    "    inp2 = keras.Input(shape=input_shape2)\n",
    "    hidden3a  = Encoder1(inp1)\n",
    "    hidden3b = Encoder2(inp2)\n",
    "    features = tf.keras.layers.Concatenate(axis=1)([hidden3a, hidden3b])\n",
    "    features = layers.Dense(16, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=[inp1, inp2], outputs=features, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_classifier(encoder, trainable):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs1 = keras.Input(shape=input_shape1)\n",
    "    inputs2 = keras.Input(shape=input_shape2)\n",
    "    features1 = encoder1(inputs1)\n",
    "    features2 = encoder2(inputs2)\n",
    "    features = tf.keras.layers.Concatenate(axis=1)([features1, features2])\n",
    "    # features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    \n",
    "    features = layers.BatchNormalization()(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(32, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(16, activation=\"relu\")(features)\n",
    "    # features = layers.BatchNormalization()(features)\n",
    "    # features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(4, activation=\"relu\")(features)\n",
    "    # features = layers.BatchNormalization()(features)\n",
    "    # features = layers.Dropout(0.1)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=[inputs1,inputs2], outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# Splitting xtrain and ytrain into training and validation sets\n",
    "xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting xtrain1 and ytrain1 into training and validation sets\n",
    "xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "y = np.concatenate((ytra_a,ytra_a), axis=0)\n",
    "yv = np.concatenate((yval_a,yval_a), axis=0)\n",
    "\n",
    "encoder1 = create_encoder1()\n",
    "encoder2 = create_encoder2()\n",
    "encoder_with_projection_head = add_projection_head1(encoder1, encoder2)\n",
    "encoder_with_projection_head.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),loss=SupervisedContrastiveLoss(temperature))\n",
    "encoder_with_projection_head.summary()\n",
    "                                                            #ytra_a                                  #yval_a\n",
    "history = encoder_with_projection_head.fit([xtra_a,xtra_ac], ytra_ac , validation_data =([xval_a,xval_ac],yval_ac), batch_size=32, epochs=100, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "\n",
    "learning_rate = 0.0005\n",
    "batch_size = 16\n",
    "hidden_units = 64\n",
    "projection_units = 128\n",
    "num_epochs = 200\n",
    "dropout_rate = 0.3\n",
    "num_classes = 2\n",
    "input_shape1 = (19,)\n",
    "input_shape2 = (38,)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint ,EarlyStopping\n",
    "classifier = create_classifier(encoder_with_projection_head, trainable=False)\n",
    "classifier.summary()\n",
    "history = classifier.fit(x=[xtra_a,xtra_ac], y=ytra_a, validation_data =([xval_a,xval_ac],yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "accuracy = classifier.evaluate([xtest_a,xtest_ac], ytest)[1]\n",
    "\n",
    "##  Accuracy on post processed labels (each word should contain only one stressed syllable)\n",
    "pred_output= classifier.predict([xtest_a,xtest_ac])\n",
    "# pred_labels= pred_output.argmax(axis =1)\n",
    "pred1_labels = pred_output[:,1]\n",
    "post_labels = make_partitions(wtest, pred1_labels)\n",
    "post_accuracy = calculate_accuracy(post_labels, ytest)\n",
    "\n",
    "F1_score_WoPP = f1_score(ytest, pred_output.argmax(axis =1))\n",
    "F1_score_WPP = f1_score(ytest, post_labels)\n",
    "\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 3)}%\")\n",
    "print(f\"Postprocessing Test accuracy: {round(post_accuracy * 100, 3)}%\")\n",
    "print(f\"Test F1_score: {round(F1_score_WoPP * 100, 3)}%\")\n",
    "print(f\"Postprocessing F1_score: {round(F1_score_WPP * 100, 3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_16 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_2 (Encoder1)          (None, 30)           20670       ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_2 (Encoder2)          (None, 30)           38910       ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 60)           0           ['encoder1_2[0][0]',             \n",
      "                                                                  'encoder2_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 16)           976         ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "148/148 [==============================] - 5s 12ms/step - loss: 14.5468 - val_loss: 13.9428\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 13.7184 - val_loss: 13.5566\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 13.4479 - val_loss: 13.4833\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 13.1149 - val_loss: 13.2025\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 13.2398 - val_loss: 13.0588\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 13.2521 - val_loss: 12.9147\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 12.9987 - val_loss: 12.5735\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 12.8410 - val_loss: 12.4163\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 12.5212 - val_loss: 12.4075\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 12.6452 - val_loss: 11.7662\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 12.2671 - val_loss: 12.0780\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 12.1837 - val_loss: 12.0727\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 12.2242 - val_loss: 12.1869\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 12.0008 - val_loss: 11.5903\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 12.1866 - val_loss: 11.6037\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 12.1260 - val_loss: 11.5382\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 12.0727 - val_loss: 11.6391\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 11.6734 - val_loss: 11.5912\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 11.5178 - val_loss: 11.1101\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 11.5048 - val_loss: 10.9123\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 11.2093 - val_loss: 10.7948\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 11.6657 - val_loss: 11.0745\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 11.6895 - val_loss: 11.5660\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 11.3031 - val_loss: 10.9613\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 11.3286 - val_loss: 11.4167\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 10.7173 - val_loss: 10.6045\n",
      "Epoch 27/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 10.6197 - val_loss: 11.1617\n",
      "Epoch 28/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 10.8751 - val_loss: 10.7823\n",
      "Epoch 29/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 10.7239 - val_loss: 10.1348\n",
      "Epoch 30/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 10.5061 - val_loss: 10.7786\n",
      "Epoch 31/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 10.6700 - val_loss: 10.9889\n",
      "Epoch 32/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 10.5720 - val_loss: 10.9623\n",
      "Epoch 33/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 10.3899 - val_loss: 11.3614\n",
      "Epoch 34/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 10.4719 - val_loss: 10.5850\n",
      "Epoch 35/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 10.2173 - val_loss: 10.7056\n",
      "Epoch 36/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 10.3528 - val_loss: 9.7093\n",
      "Epoch 37/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 10.1778 - val_loss: 10.1431\n",
      "Epoch 38/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 10.0628 - val_loss: 10.5153\n",
      "Epoch 39/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 10.1921 - val_loss: 10.2617\n",
      "Epoch 40/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.8674 - val_loss: 10.2030\n",
      "Epoch 41/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.9862 - val_loss: 10.5768\n",
      "Epoch 42/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.5216 - val_loss: 10.0842\n",
      "Epoch 43/100\n",
      "148/148 [==============================] - 2s 12ms/step - loss: 9.8732 - val_loss: 10.5735\n",
      "Epoch 44/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 10.0052 - val_loss: 10.2735\n",
      "Epoch 45/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.6752 - val_loss: 9.5591\n",
      "Epoch 46/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.9813 - val_loss: 10.4291\n",
      "Epoch 47/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.6346 - val_loss: 10.2351\n",
      "Epoch 48/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.5928 - val_loss: 10.2465\n",
      "Epoch 49/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.2537 - val_loss: 11.2031\n",
      "Epoch 50/100\n",
      "148/148 [==============================] - 2s 10ms/step - loss: 9.8079 - val_loss: 10.1781\n",
      "Epoch 51/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.4885 - val_loss: 10.1300\n",
      "Epoch 52/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.7289 - val_loss: 10.5275\n",
      "Epoch 53/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 9.5486 - val_loss: 9.6974\n",
      "Epoch 54/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.0780 - val_loss: 9.4631\n",
      "Epoch 55/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.2410 - val_loss: 10.0943\n",
      "Epoch 56/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.1796 - val_loss: 9.4262\n",
      "Epoch 57/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.0857 - val_loss: 9.7935\n",
      "Epoch 58/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 8.8293 - val_loss: 9.5763\n",
      "Epoch 59/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.1020 - val_loss: 9.5936\n",
      "Epoch 60/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.1808 - val_loss: 10.0360\n",
      "Epoch 61/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.1746 - val_loss: 9.0328\n",
      "Epoch 62/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 9.1982 - val_loss: 9.0034\n",
      "Epoch 63/100\n",
      "148/148 [==============================] - 1s 10ms/step - loss: 9.3071 - val_loss: 9.6787\n",
      "Epoch 64/100\n",
      "148/148 [==============================] - 2s 11ms/step - loss: 9.0421 - val_loss: 9.3845\n",
      "Epoch 65/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.6545 - val_loss: 9.5986\n",
      "Epoch 66/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.1140 - val_loss: 9.5015\n",
      "Epoch 67/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.0400 - val_loss: 9.9058\n",
      "Epoch 68/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.8590 - val_loss: 9.1816\n",
      "Epoch 69/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.4632 - val_loss: 8.7444\n",
      "Epoch 70/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.5823 - val_loss: 9.0285\n",
      "Epoch 71/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.3255 - val_loss: 8.7073\n",
      "Epoch 72/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.3489 - val_loss: 8.8810\n",
      "Epoch 73/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.3108 - val_loss: 9.0640\n",
      "Epoch 74/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.3329 - val_loss: 8.8525\n",
      "Epoch 75/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.0132 - val_loss: 8.3861\n",
      "Epoch 76/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.1353 - val_loss: 8.9145\n",
      "Epoch 77/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.1736 - val_loss: 9.3223\n",
      "Epoch 78/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.5738 - val_loss: 8.7569\n",
      "Epoch 79/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.2934 - val_loss: 9.2490\n",
      "Epoch 80/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.0962 - val_loss: 8.6249\n",
      "Epoch 81/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.1396 - val_loss: 8.4009\n",
      "Epoch 82/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.7157 - val_loss: 9.0133\n",
      "Epoch 83/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.1558 - val_loss: 9.6235\n",
      "Epoch 84/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.9179 - val_loss: 8.6377\n",
      "Epoch 85/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.0091 - val_loss: 8.9049\n",
      "Epoch 85: early stopping\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_2 (Encoder1)          (None, 30)           20670       ['input_17[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_2 (Encoder2)          (None, 30)           38910       ['input_18[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 60)           0           ['encoder1_2[1][0]',             \n",
      "                                                                  'encoder2_2[1][0]']             \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 64)           3904        ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64)          256         ['dense_19[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 64)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 32)           2080        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 32)           0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 16)           528         ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 4)            68          ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 2)            10          ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 4s 7ms/step - loss: 0.5393 - sparse_categorical_accuracy: 0.7390 - val_loss: 0.3864 - val_sparse_categorical_accuracy: 0.8448\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3797 - sparse_categorical_accuracy: 0.8311 - val_loss: 0.3253 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3496 - sparse_categorical_accuracy: 0.8502 - val_loss: 0.3113 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3237 - sparse_categorical_accuracy: 0.8608 - val_loss: 0.3094 - val_sparse_categorical_accuracy: 0.8685\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3168 - sparse_categorical_accuracy: 0.8693 - val_loss: 0.3064 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3053 - sparse_categorical_accuracy: 0.8701 - val_loss: 0.3123 - val_sparse_categorical_accuracy: 0.8677\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3067 - sparse_categorical_accuracy: 0.8758 - val_loss: 0.3084 - val_sparse_categorical_accuracy: 0.8677\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3030 - sparse_categorical_accuracy: 0.8752 - val_loss: 0.3038 - val_sparse_categorical_accuracy: 0.8711\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2982 - sparse_categorical_accuracy: 0.8792 - val_loss: 0.3042 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2888 - sparse_categorical_accuracy: 0.8803 - val_loss: 0.3045 - val_sparse_categorical_accuracy: 0.8702\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2862 - sparse_categorical_accuracy: 0.8784 - val_loss: 0.3071 - val_sparse_categorical_accuracy: 0.8660\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2940 - sparse_categorical_accuracy: 0.8805 - val_loss: 0.3044 - val_sparse_categorical_accuracy: 0.8685\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2886 - sparse_categorical_accuracy: 0.8818 - val_loss: 0.3033 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2810 - sparse_categorical_accuracy: 0.8860 - val_loss: 0.2993 - val_sparse_categorical_accuracy: 0.8694\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2864 - sparse_categorical_accuracy: 0.8790 - val_loss: 0.3000 - val_sparse_categorical_accuracy: 0.8753\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2754 - sparse_categorical_accuracy: 0.8801 - val_loss: 0.3021 - val_sparse_categorical_accuracy: 0.8745\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2790 - sparse_categorical_accuracy: 0.8812 - val_loss: 0.3005 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2729 - sparse_categorical_accuracy: 0.8833 - val_loss: 0.2991 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2694 - sparse_categorical_accuracy: 0.8884 - val_loss: 0.3005 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2727 - sparse_categorical_accuracy: 0.8892 - val_loss: 0.2989 - val_sparse_categorical_accuracy: 0.8787\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2681 - sparse_categorical_accuracy: 0.8824 - val_loss: 0.2988 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2659 - sparse_categorical_accuracy: 0.8928 - val_loss: 0.3011 - val_sparse_categorical_accuracy: 0.8804\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2673 - sparse_categorical_accuracy: 0.8877 - val_loss: 0.3029 - val_sparse_categorical_accuracy: 0.8753\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2681 - sparse_categorical_accuracy: 0.8867 - val_loss: 0.3013 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2674 - sparse_categorical_accuracy: 0.8913 - val_loss: 0.2984 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2689 - sparse_categorical_accuracy: 0.8894 - val_loss: 0.3006 - val_sparse_categorical_accuracy: 0.8753\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.2695 - sparse_categorical_accuracy: 0.8841 - val_loss: 0.2954 - val_sparse_categorical_accuracy: 0.8745\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2636 - sparse_categorical_accuracy: 0.8909 - val_loss: 0.2977 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2544 - sparse_categorical_accuracy: 0.8933 - val_loss: 0.3052 - val_sparse_categorical_accuracy: 0.8787\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2598 - sparse_categorical_accuracy: 0.8930 - val_loss: 0.2990 - val_sparse_categorical_accuracy: 0.8762\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2539 - sparse_categorical_accuracy: 0.8903 - val_loss: 0.2977 - val_sparse_categorical_accuracy: 0.8702\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2581 - sparse_categorical_accuracy: 0.8937 - val_loss: 0.3017 - val_sparse_categorical_accuracy: 0.8694\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2617 - sparse_categorical_accuracy: 0.8899 - val_loss: 0.2954 - val_sparse_categorical_accuracy: 0.8702\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2613 - sparse_categorical_accuracy: 0.8892 - val_loss: 0.3005 - val_sparse_categorical_accuracy: 0.8779\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2624 - sparse_categorical_accuracy: 0.8886 - val_loss: 0.2945 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2544 - sparse_categorical_accuracy: 0.8918 - val_loss: 0.3004 - val_sparse_categorical_accuracy: 0.8779\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2559 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.2959 - val_sparse_categorical_accuracy: 0.8796\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2534 - sparse_categorical_accuracy: 0.8933 - val_loss: 0.3032 - val_sparse_categorical_accuracy: 0.8813\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2526 - sparse_categorical_accuracy: 0.8909 - val_loss: 0.2991 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2538 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.2961 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2542 - sparse_categorical_accuracy: 0.8966 - val_loss: 0.2887 - val_sparse_categorical_accuracy: 0.8804\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2499 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.2938 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2421 - sparse_categorical_accuracy: 0.8962 - val_loss: 0.3015 - val_sparse_categorical_accuracy: 0.8923\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2461 - sparse_categorical_accuracy: 0.8990 - val_loss: 0.2959 - val_sparse_categorical_accuracy: 0.8804\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2413 - sparse_categorical_accuracy: 0.9030 - val_loss: 0.3003 - val_sparse_categorical_accuracy: 0.8872\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2610 - sparse_categorical_accuracy: 0.8911 - val_loss: 0.2974 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2493 - sparse_categorical_accuracy: 0.8975 - val_loss: 0.3028 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2429 - sparse_categorical_accuracy: 0.8973 - val_loss: 0.3041 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2436 - sparse_categorical_accuracy: 0.8960 - val_loss: 0.3043 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2441 - sparse_categorical_accuracy: 0.8969 - val_loss: 0.3005 - val_sparse_categorical_accuracy: 0.8821\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.2556 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.2961 - val_sparse_categorical_accuracy: 0.8880\n",
      "Epoch 51: early stopping\n",
      "150/150 [==============================] - 1s 3ms/step - loss: 0.2911 - sparse_categorical_accuracy: 0.8814\n",
      "150/150 [==============================] - 1s 2ms/step\n",
      "Test accuracy: 88.139%\n",
      "Postprocessing Test accuracy: 91.898%\n",
      "Test F1_score: 86.961%\n",
      "Postprocessing F1_score: 90.784%\n",
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_3 (Encoder1)          (None, 30)           20670       ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_3 (Encoder2)          (None, 30)           38910       ['input_22[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 60)           0           ['encoder1_3[0][0]',             \n",
      "                                                                  'encoder2_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 16)           976         ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "148/148 [==============================] - 4s 9ms/step - loss: 28.4753 - val_loss: 27.7569\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 26.9077 - val_loss: 26.7723\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 25.9421 - val_loss: 26.1921\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 26.1957 - val_loss: 25.8315\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 26.5066 - val_loss: 27.4306\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 25.1916 - val_loss: 21.4637\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 17.9190 - val_loss: 16.6450\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 14.3526 - val_loss: 14.4830\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 12.7954 - val_loss: 14.1166\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 11.4089 - val_loss: 12.8159\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 10.4531 - val_loss: 12.2349\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 10.0322 - val_loss: 12.0215\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 10.9002 - val_loss: 11.6650\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 10.9259 - val_loss: 11.4435\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 11.4788 - val_loss: 11.3690\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 10.3586 - val_loss: 9.8976\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.3858 - val_loss: 10.2788\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.1855 - val_loss: 10.2897\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.7140 - val_loss: 10.2897\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.3448 - val_loss: 10.2610\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.7135 - val_loss: 10.5218\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.7824 - val_loss: 10.5218\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.2704 - val_loss: 10.5218\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.4787 - val_loss: 10.4869\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 9.5450 - val_loss: 10.4869\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.8807 - val_loss: 10.4869\n",
      "Epoch 26: early stopping\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_3 (Encoder1)          (None, 30)           20670       ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_3 (Encoder2)          (None, 30)           38910       ['input_24[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 60)           0           ['encoder1_3[1][0]',             \n",
      "                                                                  'encoder2_3[1][0]']             \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 64)           3904        ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64)          256         ['dense_27[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 64)           0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 32)           2080        ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 32)           0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 16)           528         ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 4)            68          ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 2)            10          ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 5s 8ms/step - loss: 0.6478 - sparse_categorical_accuracy: 0.6549 - val_loss: 0.6100 - val_sparse_categorical_accuracy: 0.7583\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.5971 - sparse_categorical_accuracy: 0.7576 - val_loss: 0.5615 - val_sparse_categorical_accuracy: 0.7778\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.5424 - sparse_categorical_accuracy: 0.7668 - val_loss: 0.4875 - val_sparse_categorical_accuracy: 0.7820\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4953 - sparse_categorical_accuracy: 0.7795 - val_loss: 0.4770 - val_sparse_categorical_accuracy: 0.7812\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.5005 - sparse_categorical_accuracy: 0.7765 - val_loss: 0.4732 - val_sparse_categorical_accuracy: 0.7905\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.5023 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.4712 - val_sparse_categorical_accuracy: 0.7947\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4869 - sparse_categorical_accuracy: 0.7731 - val_loss: 0.4711 - val_sparse_categorical_accuracy: 0.7837\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4837 - sparse_categorical_accuracy: 0.7736 - val_loss: 0.4703 - val_sparse_categorical_accuracy: 0.7854\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4827 - sparse_categorical_accuracy: 0.7803 - val_loss: 0.4687 - val_sparse_categorical_accuracy: 0.7922\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4798 - sparse_categorical_accuracy: 0.7778 - val_loss: 0.4714 - val_sparse_categorical_accuracy: 0.7947\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4799 - sparse_categorical_accuracy: 0.7750 - val_loss: 0.4671 - val_sparse_categorical_accuracy: 0.7973\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4936 - sparse_categorical_accuracy: 0.7695 - val_loss: 0.4669 - val_sparse_categorical_accuracy: 0.7930\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4731 - sparse_categorical_accuracy: 0.7759 - val_loss: 0.4675 - val_sparse_categorical_accuracy: 0.7947\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4805 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.4659 - val_sparse_categorical_accuracy: 0.7964\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4689 - sparse_categorical_accuracy: 0.7852 - val_loss: 0.4644 - val_sparse_categorical_accuracy: 0.7947\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4662 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.4640 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4803 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4645 - val_sparse_categorical_accuracy: 0.7956\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4655 - sparse_categorical_accuracy: 0.7897 - val_loss: 0.4595 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4681 - sparse_categorical_accuracy: 0.7893 - val_loss: 0.4617 - val_sparse_categorical_accuracy: 0.7922\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4715 - sparse_categorical_accuracy: 0.7859 - val_loss: 0.4577 - val_sparse_categorical_accuracy: 0.7947\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4707 - sparse_categorical_accuracy: 0.7874 - val_loss: 0.4565 - val_sparse_categorical_accuracy: 0.7981\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4720 - sparse_categorical_accuracy: 0.7899 - val_loss: 0.4571 - val_sparse_categorical_accuracy: 0.7981\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4688 - sparse_categorical_accuracy: 0.7848 - val_loss: 0.4595 - val_sparse_categorical_accuracy: 0.7956\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4724 - sparse_categorical_accuracy: 0.7852 - val_loss: 0.4558 - val_sparse_categorical_accuracy: 0.7922\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4767 - sparse_categorical_accuracy: 0.7772 - val_loss: 0.4610 - val_sparse_categorical_accuracy: 0.7947\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4766 - sparse_categorical_accuracy: 0.7767 - val_loss: 0.4538 - val_sparse_categorical_accuracy: 0.7939\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4755 - sparse_categorical_accuracy: 0.7837 - val_loss: 0.4549 - val_sparse_categorical_accuracy: 0.7964\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4667 - sparse_categorical_accuracy: 0.7820 - val_loss: 0.4584 - val_sparse_categorical_accuracy: 0.7930\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4698 - sparse_categorical_accuracy: 0.7803 - val_loss: 0.4569 - val_sparse_categorical_accuracy: 0.7930\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4700 - sparse_categorical_accuracy: 0.7806 - val_loss: 0.4569 - val_sparse_categorical_accuracy: 0.7964\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4726 - sparse_categorical_accuracy: 0.7829 - val_loss: 0.4541 - val_sparse_categorical_accuracy: 0.7939\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4625 - sparse_categorical_accuracy: 0.7835 - val_loss: 0.4562 - val_sparse_categorical_accuracy: 0.7973\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4680 - sparse_categorical_accuracy: 0.7778 - val_loss: 0.4596 - val_sparse_categorical_accuracy: 0.7913\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4597 - sparse_categorical_accuracy: 0.7852 - val_loss: 0.4529 - val_sparse_categorical_accuracy: 0.7981\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4661 - sparse_categorical_accuracy: 0.7880 - val_loss: 0.4540 - val_sparse_categorical_accuracy: 0.7981\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4683 - sparse_categorical_accuracy: 0.7854 - val_loss: 0.4558 - val_sparse_categorical_accuracy: 0.7897\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4543 - sparse_categorical_accuracy: 0.7884 - val_loss: 0.4533 - val_sparse_categorical_accuracy: 0.7990\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4655 - sparse_categorical_accuracy: 0.7803 - val_loss: 0.4531 - val_sparse_categorical_accuracy: 0.7973\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4747 - sparse_categorical_accuracy: 0.7791 - val_loss: 0.4537 - val_sparse_categorical_accuracy: 0.7956\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4594 - sparse_categorical_accuracy: 0.7888 - val_loss: 0.4496 - val_sparse_categorical_accuracy: 0.7947\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4579 - sparse_categorical_accuracy: 0.7867 - val_loss: 0.4491 - val_sparse_categorical_accuracy: 0.7964\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4595 - sparse_categorical_accuracy: 0.7867 - val_loss: 0.4475 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4567 - sparse_categorical_accuracy: 0.7878 - val_loss: 0.4505 - val_sparse_categorical_accuracy: 0.7956\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4613 - sparse_categorical_accuracy: 0.7852 - val_loss: 0.4454 - val_sparse_categorical_accuracy: 0.8024\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4587 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.4421 - val_sparse_categorical_accuracy: 0.8032\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4554 - sparse_categorical_accuracy: 0.7874 - val_loss: 0.4424 - val_sparse_categorical_accuracy: 0.7973\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4464 - sparse_categorical_accuracy: 0.8001 - val_loss: 0.4416 - val_sparse_categorical_accuracy: 0.8126\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4564 - sparse_categorical_accuracy: 0.7844 - val_loss: 0.4517 - val_sparse_categorical_accuracy: 0.7964\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4599 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.4450 - val_sparse_categorical_accuracy: 0.8015\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4543 - sparse_categorical_accuracy: 0.7937 - val_loss: 0.4419 - val_sparse_categorical_accuracy: 0.7998\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4564 - sparse_categorical_accuracy: 0.7893 - val_loss: 0.4430 - val_sparse_categorical_accuracy: 0.7998\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4505 - sparse_categorical_accuracy: 0.8018 - val_loss: 0.4419 - val_sparse_categorical_accuracy: 0.8058\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4575 - sparse_categorical_accuracy: 0.7893 - val_loss: 0.4430 - val_sparse_categorical_accuracy: 0.7990\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4528 - sparse_categorical_accuracy: 0.7905 - val_loss: 0.4450 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4549 - sparse_categorical_accuracy: 0.7899 - val_loss: 0.4405 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 56/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4553 - sparse_categorical_accuracy: 0.7965 - val_loss: 0.4406 - val_sparse_categorical_accuracy: 0.8015\n",
      "Epoch 57/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4539 - sparse_categorical_accuracy: 0.7914 - val_loss: 0.4425 - val_sparse_categorical_accuracy: 0.8015\n",
      "Epoch 58/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4554 - sparse_categorical_accuracy: 0.7899 - val_loss: 0.4426 - val_sparse_categorical_accuracy: 0.7990\n",
      "Epoch 59/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4504 - sparse_categorical_accuracy: 0.7935 - val_loss: 0.4388 - val_sparse_categorical_accuracy: 0.7998\n",
      "Epoch 60/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4500 - sparse_categorical_accuracy: 0.7973 - val_loss: 0.4402 - val_sparse_categorical_accuracy: 0.8015\n",
      "Epoch 61/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4500 - sparse_categorical_accuracy: 0.7929 - val_loss: 0.4376 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 62/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4551 - sparse_categorical_accuracy: 0.7893 - val_loss: 0.4465 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 63/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4531 - sparse_categorical_accuracy: 0.7903 - val_loss: 0.4375 - val_sparse_categorical_accuracy: 0.7973\n",
      "Epoch 64/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4550 - sparse_categorical_accuracy: 0.7922 - val_loss: 0.4391 - val_sparse_categorical_accuracy: 0.8032\n",
      "Epoch 65/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4524 - sparse_categorical_accuracy: 0.7920 - val_loss: 0.4360 - val_sparse_categorical_accuracy: 0.8109\n",
      "Epoch 66/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4572 - sparse_categorical_accuracy: 0.7931 - val_loss: 0.4412 - val_sparse_categorical_accuracy: 0.8049\n",
      "Epoch 67/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4488 - sparse_categorical_accuracy: 0.7992 - val_loss: 0.4403 - val_sparse_categorical_accuracy: 0.7964\n",
      "Epoch 68/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4420 - sparse_categorical_accuracy: 0.7969 - val_loss: 0.4388 - val_sparse_categorical_accuracy: 0.8075\n",
      "Epoch 69/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4503 - sparse_categorical_accuracy: 0.7971 - val_loss: 0.4398 - val_sparse_categorical_accuracy: 0.8032\n",
      "Epoch 70/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4458 - sparse_categorical_accuracy: 0.7967 - val_loss: 0.4395 - val_sparse_categorical_accuracy: 0.7981\n",
      "Epoch 71/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4448 - sparse_categorical_accuracy: 0.7980 - val_loss: 0.4354 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 72/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4528 - sparse_categorical_accuracy: 0.7880 - val_loss: 0.4367 - val_sparse_categorical_accuracy: 0.8015\n",
      "Epoch 73/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4485 - sparse_categorical_accuracy: 0.7929 - val_loss: 0.4340 - val_sparse_categorical_accuracy: 0.7998\n",
      "Epoch 74/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4425 - sparse_categorical_accuracy: 0.7948 - val_loss: 0.4397 - val_sparse_categorical_accuracy: 0.8015\n",
      "Epoch 75/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4525 - sparse_categorical_accuracy: 0.7954 - val_loss: 0.4400 - val_sparse_categorical_accuracy: 0.8024\n",
      "Epoch 76/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4520 - sparse_categorical_accuracy: 0.7899 - val_loss: 0.4372 - val_sparse_categorical_accuracy: 0.8024\n",
      "Epoch 77/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4490 - sparse_categorical_accuracy: 0.7929 - val_loss: 0.4361 - val_sparse_categorical_accuracy: 0.8024\n",
      "Epoch 78/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4470 - sparse_categorical_accuracy: 0.7988 - val_loss: 0.4402 - val_sparse_categorical_accuracy: 0.7964\n",
      "Epoch 79/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4457 - sparse_categorical_accuracy: 0.7946 - val_loss: 0.4406 - val_sparse_categorical_accuracy: 0.7981\n",
      "Epoch 80/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4425 - sparse_categorical_accuracy: 0.7973 - val_loss: 0.4374 - val_sparse_categorical_accuracy: 0.8041\n",
      "Epoch 81/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4557 - sparse_categorical_accuracy: 0.7901 - val_loss: 0.4385 - val_sparse_categorical_accuracy: 0.8015\n",
      "Epoch 82/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4418 - sparse_categorical_accuracy: 0.8031 - val_loss: 0.4342 - val_sparse_categorical_accuracy: 0.8058\n",
      "Epoch 83/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4473 - sparse_categorical_accuracy: 0.8024 - val_loss: 0.4369 - val_sparse_categorical_accuracy: 0.8024\n",
      "Epoch 83: early stopping\n",
      "150/150 [==============================] - 1s 3ms/step - loss: 0.4305 - sparse_categorical_accuracy: 0.8052\n",
      "150/150 [==============================] - 1s 2ms/step\n",
      "Test accuracy: 80.518%\n",
      "Postprocessing Test accuracy: 37.356%\n",
      "Test F1_score: 77.025%\n",
      "Postprocessing F1_score: 28.741%\n",
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_4 (Encoder1)          (None, 30)           20670       ['input_27[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_4 (Encoder2)          (None, 30)           38910       ['input_28[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 60)           0           ['encoder1_4[0][0]',             \n",
      "                                                                  'encoder2_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 16)           976         ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "148/148 [==============================] - 5s 9ms/step - loss: 10.1591 - val_loss: 9.4140\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.6220 - val_loss: 9.7123\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.5942 - val_loss: 8.3381\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.2075 - val_loss: 6.2813\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.2401 - val_loss: 7.1054\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.8372 - val_loss: 8.9592\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 9.1097 - val_loss: 9.2118\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.8858 - val_loss: 8.9511\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.6699 - val_loss: 8.4452\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.6669 - val_loss: 8.8802\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.3956 - val_loss: 8.6100\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.5312 - val_loss: 8.3569\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.8440 - val_loss: 8.9378\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.5491 - val_loss: 8.4525\n",
      "Epoch 14: early stopping\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_30 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_4 (Encoder1)          (None, 30)           20670       ['input_29[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_4 (Encoder2)          (None, 30)           38910       ['input_30[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 60)           0           ['encoder1_4[1][0]',             \n",
      "                                                                  'encoder2_4[1][0]']             \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 64)           3904        ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64)          256         ['dense_35[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 64)           0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 32)           2080        ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 32)           0           ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 16)           528         ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 4)            68          ['dense_37[0][0]']               \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 2)            10          ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 5s 7ms/step - loss: 0.5327 - sparse_categorical_accuracy: 0.7303 - val_loss: 0.4488 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4527 - sparse_categorical_accuracy: 0.7903 - val_loss: 0.4191 - val_sparse_categorical_accuracy: 0.8126\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4574 - sparse_categorical_accuracy: 0.7890 - val_loss: 0.4142 - val_sparse_categorical_accuracy: 0.8193\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4395 - sparse_categorical_accuracy: 0.8026 - val_loss: 0.4105 - val_sparse_categorical_accuracy: 0.8227\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4386 - sparse_categorical_accuracy: 0.7997 - val_loss: 0.4062 - val_sparse_categorical_accuracy: 0.8253\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4299 - sparse_categorical_accuracy: 0.8062 - val_loss: 0.4028 - val_sparse_categorical_accuracy: 0.8321\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4378 - sparse_categorical_accuracy: 0.8041 - val_loss: 0.4007 - val_sparse_categorical_accuracy: 0.8210\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4309 - sparse_categorical_accuracy: 0.8073 - val_loss: 0.3990 - val_sparse_categorical_accuracy: 0.8261\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4335 - sparse_categorical_accuracy: 0.8094 - val_loss: 0.4002 - val_sparse_categorical_accuracy: 0.8295\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4239 - sparse_categorical_accuracy: 0.8115 - val_loss: 0.3962 - val_sparse_categorical_accuracy: 0.8329\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4119 - sparse_categorical_accuracy: 0.8164 - val_loss: 0.3930 - val_sparse_categorical_accuracy: 0.8278\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4202 - sparse_categorical_accuracy: 0.8177 - val_loss: 0.3948 - val_sparse_categorical_accuracy: 0.8312\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4226 - sparse_categorical_accuracy: 0.8105 - val_loss: 0.3907 - val_sparse_categorical_accuracy: 0.8261\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4246 - sparse_categorical_accuracy: 0.8101 - val_loss: 0.3880 - val_sparse_categorical_accuracy: 0.8355\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4147 - sparse_categorical_accuracy: 0.8179 - val_loss: 0.3906 - val_sparse_categorical_accuracy: 0.8253\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4164 - sparse_categorical_accuracy: 0.8128 - val_loss: 0.3913 - val_sparse_categorical_accuracy: 0.8321\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4216 - sparse_categorical_accuracy: 0.8130 - val_loss: 0.3919 - val_sparse_categorical_accuracy: 0.8287\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4114 - sparse_categorical_accuracy: 0.8162 - val_loss: 0.3854 - val_sparse_categorical_accuracy: 0.8388\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4043 - sparse_categorical_accuracy: 0.8181 - val_loss: 0.3863 - val_sparse_categorical_accuracy: 0.8321\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4200 - sparse_categorical_accuracy: 0.8126 - val_loss: 0.3793 - val_sparse_categorical_accuracy: 0.8372\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4118 - sparse_categorical_accuracy: 0.8098 - val_loss: 0.3789 - val_sparse_categorical_accuracy: 0.8372\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4069 - sparse_categorical_accuracy: 0.8277 - val_loss: 0.3771 - val_sparse_categorical_accuracy: 0.8372\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4007 - sparse_categorical_accuracy: 0.8192 - val_loss: 0.3769 - val_sparse_categorical_accuracy: 0.8388\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4091 - sparse_categorical_accuracy: 0.8183 - val_loss: 0.3742 - val_sparse_categorical_accuracy: 0.8422\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4051 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.3760 - val_sparse_categorical_accuracy: 0.8388\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4040 - sparse_categorical_accuracy: 0.8160 - val_loss: 0.3749 - val_sparse_categorical_accuracy: 0.8422\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3966 - sparse_categorical_accuracy: 0.8236 - val_loss: 0.3732 - val_sparse_categorical_accuracy: 0.8431\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4012 - sparse_categorical_accuracy: 0.8205 - val_loss: 0.3743 - val_sparse_categorical_accuracy: 0.8439\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4033 - sparse_categorical_accuracy: 0.8232 - val_loss: 0.3724 - val_sparse_categorical_accuracy: 0.8431\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3920 - sparse_categorical_accuracy: 0.8326 - val_loss: 0.3680 - val_sparse_categorical_accuracy: 0.8431\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4018 - sparse_categorical_accuracy: 0.8262 - val_loss: 0.3676 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3938 - sparse_categorical_accuracy: 0.8249 - val_loss: 0.3684 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3946 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.3658 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3982 - sparse_categorical_accuracy: 0.8243 - val_loss: 0.3632 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3985 - sparse_categorical_accuracy: 0.8249 - val_loss: 0.3634 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3861 - sparse_categorical_accuracy: 0.8319 - val_loss: 0.3666 - val_sparse_categorical_accuracy: 0.8448\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3932 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.3665 - val_sparse_categorical_accuracy: 0.8431\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3936 - sparse_categorical_accuracy: 0.8268 - val_loss: 0.3663 - val_sparse_categorical_accuracy: 0.8431\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3951 - sparse_categorical_accuracy: 0.8185 - val_loss: 0.3634 - val_sparse_categorical_accuracy: 0.8414\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3881 - sparse_categorical_accuracy: 0.8281 - val_loss: 0.3693 - val_sparse_categorical_accuracy: 0.8405\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3903 - sparse_categorical_accuracy: 0.8304 - val_loss: 0.3651 - val_sparse_categorical_accuracy: 0.8456\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3833 - sparse_categorical_accuracy: 0.8319 - val_loss: 0.3634 - val_sparse_categorical_accuracy: 0.8422\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3881 - sparse_categorical_accuracy: 0.8298 - val_loss: 0.3634 - val_sparse_categorical_accuracy: 0.8431\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4035 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.3660 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 44: early stopping\n",
      "150/150 [==============================] - 1s 3ms/step - loss: 0.3560 - sparse_categorical_accuracy: 0.8476\n",
      "150/150 [==============================] - 1s 2ms/step\n",
      "Test accuracy: 84.757%\n",
      "Postprocessing Test accuracy: 60.743%\n",
      "Test F1_score: 82.775%\n",
      "Postprocessing F1_score: 55.344%\n",
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_34 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_5 (Encoder1)          (None, 30)           20670       ['input_33[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_5 (Encoder2)          (None, 30)           38910       ['input_34[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 60)           0           ['encoder1_5[0][0]',             \n",
      "                                                                  'encoder2_5[0][0]']             \n",
      "                                                                                                  \n",
      " dense_42 (Dense)               (None, 16)           976         ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "148/148 [==============================] - 4s 10ms/step - loss: 8.7026 - val_loss: 8.1784\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 8.1188 - val_loss: 8.0051\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 8.0364 - val_loss: 7.8691\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 7.8583 - val_loss: 7.9478\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.8346 - val_loss: 7.6274\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 7.7079 - val_loss: 8.0135\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 7.6835 - val_loss: 7.5631\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.6602 - val_loss: 7.3747\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 7.7381 - val_loss: 7.6016\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 7.5094 - val_loss: 7.3722\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 7.3468 - val_loss: 7.4139\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.1900 - val_loss: 7.2115\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 7.0764 - val_loss: 7.1292\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.8290 - val_loss: 6.8318\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 7.2281 - val_loss: 7.3976\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 7.2217 - val_loss: 7.1537\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 7.0917 - val_loss: 6.8935\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.8932 - val_loss: 7.1118\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 7.0044 - val_loss: 6.9362\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.7620 - val_loss: 6.3479\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.5525 - val_loss: 7.0846\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.8463 - val_loss: 6.7249\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.5762 - val_loss: 6.4999\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 6.5710 - val_loss: 7.0365\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.9983 - val_loss: 6.7354\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.8644 - val_loss: 7.4819\n",
      "Epoch 27/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.7742 - val_loss: 6.6225\n",
      "Epoch 28/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.8928 - val_loss: 6.6788\n",
      "Epoch 29/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.6742 - val_loss: 6.5569\n",
      "Epoch 30/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.6839 - val_loss: 6.9041\n",
      "Epoch 30: early stopping\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_36 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_5 (Encoder1)          (None, 30)           20670       ['input_35[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_5 (Encoder2)          (None, 30)           38910       ['input_36[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 60)           0           ['encoder1_5[1][0]',             \n",
      "                                                                  'encoder2_5[1][0]']             \n",
      "                                                                                                  \n",
      " dense_43 (Dense)               (None, 64)           3904        ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64)          256         ['dense_43[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 64)           0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_44 (Dense)               (None, 32)           2080        ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 32)           0           ['dense_44[0][0]']               \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 16)           528         ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 4)            68          ['dense_45[0][0]']               \n",
      "                                                                                                  \n",
      " dense_47 (Dense)               (None, 2)            10          ['dense_46[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 5s 7ms/step - loss: 0.5391 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.4284 - val_sparse_categorical_accuracy: 0.8219\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4115 - sparse_categorical_accuracy: 0.8192 - val_loss: 0.3845 - val_sparse_categorical_accuracy: 0.8448\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3903 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.3757 - val_sparse_categorical_accuracy: 0.8397\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3912 - sparse_categorical_accuracy: 0.8326 - val_loss: 0.3697 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3750 - sparse_categorical_accuracy: 0.8404 - val_loss: 0.3676 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3843 - sparse_categorical_accuracy: 0.8336 - val_loss: 0.3623 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3730 - sparse_categorical_accuracy: 0.8430 - val_loss: 0.3627 - val_sparse_categorical_accuracy: 0.8533\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3598 - sparse_categorical_accuracy: 0.8470 - val_loss: 0.3668 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3717 - sparse_categorical_accuracy: 0.8434 - val_loss: 0.3626 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3571 - sparse_categorical_accuracy: 0.8502 - val_loss: 0.3587 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3683 - sparse_categorical_accuracy: 0.8425 - val_loss: 0.3581 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3595 - sparse_categorical_accuracy: 0.8476 - val_loss: 0.3608 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3501 - sparse_categorical_accuracy: 0.8493 - val_loss: 0.3577 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3583 - sparse_categorical_accuracy: 0.8449 - val_loss: 0.3524 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3512 - sparse_categorical_accuracy: 0.8525 - val_loss: 0.3495 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3552 - sparse_categorical_accuracy: 0.8447 - val_loss: 0.3520 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3477 - sparse_categorical_accuracy: 0.8506 - val_loss: 0.3523 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3500 - sparse_categorical_accuracy: 0.8497 - val_loss: 0.3491 - val_sparse_categorical_accuracy: 0.8609\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3582 - sparse_categorical_accuracy: 0.8472 - val_loss: 0.3512 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3487 - sparse_categorical_accuracy: 0.8472 - val_loss: 0.3513 - val_sparse_categorical_accuracy: 0.8592\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3583 - sparse_categorical_accuracy: 0.8466 - val_loss: 0.3466 - val_sparse_categorical_accuracy: 0.8592\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3397 - sparse_categorical_accuracy: 0.8517 - val_loss: 0.3492 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3398 - sparse_categorical_accuracy: 0.8584 - val_loss: 0.3503 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3362 - sparse_categorical_accuracy: 0.8591 - val_loss: 0.3512 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3406 - sparse_categorical_accuracy: 0.8495 - val_loss: 0.3500 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3385 - sparse_categorical_accuracy: 0.8529 - val_loss: 0.3439 - val_sparse_categorical_accuracy: 0.8592\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3363 - sparse_categorical_accuracy: 0.8514 - val_loss: 0.3443 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3408 - sparse_categorical_accuracy: 0.8582 - val_loss: 0.3426 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3257 - sparse_categorical_accuracy: 0.8621 - val_loss: 0.3443 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3324 - sparse_categorical_accuracy: 0.8540 - val_loss: 0.3450 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3269 - sparse_categorical_accuracy: 0.8572 - val_loss: 0.3446 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.3253 - sparse_categorical_accuracy: 0.8610 - val_loss: 0.3439 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3275 - sparse_categorical_accuracy: 0.8567 - val_loss: 0.3398 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3202 - sparse_categorical_accuracy: 0.8597 - val_loss: 0.3436 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3369 - sparse_categorical_accuracy: 0.8544 - val_loss: 0.3410 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3243 - sparse_categorical_accuracy: 0.8582 - val_loss: 0.3425 - val_sparse_categorical_accuracy: 0.8660\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3318 - sparse_categorical_accuracy: 0.8567 - val_loss: 0.3385 - val_sparse_categorical_accuracy: 0.8626\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3317 - sparse_categorical_accuracy: 0.8597 - val_loss: 0.3349 - val_sparse_categorical_accuracy: 0.8711\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3172 - sparse_categorical_accuracy: 0.8661 - val_loss: 0.3305 - val_sparse_categorical_accuracy: 0.8711\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3354 - sparse_categorical_accuracy: 0.8557 - val_loss: 0.3315 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3271 - sparse_categorical_accuracy: 0.8589 - val_loss: 0.3313 - val_sparse_categorical_accuracy: 0.8702\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3269 - sparse_categorical_accuracy: 0.8565 - val_loss: 0.3306 - val_sparse_categorical_accuracy: 0.8685\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3216 - sparse_categorical_accuracy: 0.8693 - val_loss: 0.3346 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3256 - sparse_categorical_accuracy: 0.8614 - val_loss: 0.3305 - val_sparse_categorical_accuracy: 0.8668\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3332 - sparse_categorical_accuracy: 0.8567 - val_loss: 0.3335 - val_sparse_categorical_accuracy: 0.8634\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3153 - sparse_categorical_accuracy: 0.8631 - val_loss: 0.3259 - val_sparse_categorical_accuracy: 0.8685\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3159 - sparse_categorical_accuracy: 0.8674 - val_loss: 0.3383 - val_sparse_categorical_accuracy: 0.8643\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3272 - sparse_categorical_accuracy: 0.8557 - val_loss: 0.3308 - val_sparse_categorical_accuracy: 0.8660\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3222 - sparse_categorical_accuracy: 0.8616 - val_loss: 0.3257 - val_sparse_categorical_accuracy: 0.8711\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3190 - sparse_categorical_accuracy: 0.8635 - val_loss: 0.3291 - val_sparse_categorical_accuracy: 0.8753\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3195 - sparse_categorical_accuracy: 0.8652 - val_loss: 0.3269 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3134 - sparse_categorical_accuracy: 0.8650 - val_loss: 0.3283 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3169 - sparse_categorical_accuracy: 0.8659 - val_loss: 0.3273 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3120 - sparse_categorical_accuracy: 0.8654 - val_loss: 0.3333 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3216 - sparse_categorical_accuracy: 0.8625 - val_loss: 0.3246 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 56/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3169 - sparse_categorical_accuracy: 0.8674 - val_loss: 0.3255 - val_sparse_categorical_accuracy: 0.8770\n",
      "Epoch 57/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3123 - sparse_categorical_accuracy: 0.8716 - val_loss: 0.3222 - val_sparse_categorical_accuracy: 0.8813\n",
      "Epoch 58/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3062 - sparse_categorical_accuracy: 0.8701 - val_loss: 0.3325 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 59/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3215 - sparse_categorical_accuracy: 0.8635 - val_loss: 0.3256 - val_sparse_categorical_accuracy: 0.8711\n",
      "Epoch 60/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3239 - sparse_categorical_accuracy: 0.8540 - val_loss: 0.3235 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 61/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3090 - sparse_categorical_accuracy: 0.8688 - val_loss: 0.3202 - val_sparse_categorical_accuracy: 0.8813\n",
      "Epoch 62/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3038 - sparse_categorical_accuracy: 0.8667 - val_loss: 0.3245 - val_sparse_categorical_accuracy: 0.8685\n",
      "Epoch 63/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3179 - sparse_categorical_accuracy: 0.8646 - val_loss: 0.3235 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 64/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3166 - sparse_categorical_accuracy: 0.8599 - val_loss: 0.3224 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 65/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3169 - sparse_categorical_accuracy: 0.8657 - val_loss: 0.3220 - val_sparse_categorical_accuracy: 0.8702\n",
      "Epoch 66/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3066 - sparse_categorical_accuracy: 0.8725 - val_loss: 0.3215 - val_sparse_categorical_accuracy: 0.8711\n",
      "Epoch 67/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3014 - sparse_categorical_accuracy: 0.8744 - val_loss: 0.3213 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 68/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3184 - sparse_categorical_accuracy: 0.8663 - val_loss: 0.3196 - val_sparse_categorical_accuracy: 0.8702\n",
      "Epoch 69/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3135 - sparse_categorical_accuracy: 0.8638 - val_loss: 0.3153 - val_sparse_categorical_accuracy: 0.8762\n",
      "Epoch 70/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3024 - sparse_categorical_accuracy: 0.8778 - val_loss: 0.3189 - val_sparse_categorical_accuracy: 0.8745\n",
      "Epoch 71/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3072 - sparse_categorical_accuracy: 0.8716 - val_loss: 0.3199 - val_sparse_categorical_accuracy: 0.8719\n",
      "Epoch 72/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3165 - sparse_categorical_accuracy: 0.8652 - val_loss: 0.3181 - val_sparse_categorical_accuracy: 0.8762\n",
      "Epoch 73/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3058 - sparse_categorical_accuracy: 0.8705 - val_loss: 0.3193 - val_sparse_categorical_accuracy: 0.8753\n",
      "Epoch 74/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3055 - sparse_categorical_accuracy: 0.8674 - val_loss: 0.3197 - val_sparse_categorical_accuracy: 0.8838\n",
      "Epoch 75/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3009 - sparse_categorical_accuracy: 0.8765 - val_loss: 0.3167 - val_sparse_categorical_accuracy: 0.8796\n",
      "Epoch 76/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3080 - sparse_categorical_accuracy: 0.8705 - val_loss: 0.3236 - val_sparse_categorical_accuracy: 0.8736\n",
      "Epoch 77/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3016 - sparse_categorical_accuracy: 0.8688 - val_loss: 0.3170 - val_sparse_categorical_accuracy: 0.8804\n",
      "Epoch 78/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3152 - sparse_categorical_accuracy: 0.8695 - val_loss: 0.3187 - val_sparse_categorical_accuracy: 0.8813\n",
      "Epoch 79/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3043 - sparse_categorical_accuracy: 0.8691 - val_loss: 0.3234 - val_sparse_categorical_accuracy: 0.8804\n",
      "Epoch 79: early stopping\n",
      "150/150 [==============================] - 1s 3ms/step - loss: 0.3058 - sparse_categorical_accuracy: 0.8739\n",
      "150/150 [==============================] - 1s 3ms/step\n",
      "Test accuracy: 87.388%\n",
      "Postprocessing Test accuracy: 90.52%\n",
      "Test F1_score: 85.868%\n",
      "Postprocessing F1_score: 89.216%\n",
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_39 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_40 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_6 (Encoder1)          (None, 30)           20670       ['input_39[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_6 (Encoder2)          (None, 30)           38910       ['input_40[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 60)           0           ['encoder1_6[0][0]',             \n",
      "                                                                  'encoder2_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 16)           976         ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "148/148 [==============================] - 4s 9ms/step - loss: 7.1406 - val_loss: 6.9042\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 6.9466 - val_loss: 6.7488\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.6375 - val_loss: 6.5837\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 6.3078 - val_loss: 6.1276\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 5.2007 - val_loss: 4.7579\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.1345 - val_loss: 3.1549\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.8607 - val_loss: 3.0526\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.8111 - val_loss: 2.9028\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.6442 - val_loss: 2.7372\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.4449 - val_loss: 2.9004\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.6233 - val_loss: 2.8035\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.7307 - val_loss: 2.5750\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 2.3909 - val_loss: 2.2827\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.1456 - val_loss: 2.2580\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2262 - val_loss: 2.2340\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.3941 - val_loss: 2.1952\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 2.2919 - val_loss: 2.2554\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.1737 - val_loss: 2.2554\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2080 - val_loss: 2.2239\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2331 - val_loss: 2.2385\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2445 - val_loss: 2.3142\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2589 - val_loss: 2.2879\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 2.2552 - val_loss: 2.2879\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2447 - val_loss: 2.2853\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2320 - val_loss: 2.3174\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.3629 - val_loss: 2.3055\n",
      "Epoch 26: early stopping\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_41 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_42 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_6 (Encoder1)          (None, 30)           20670       ['input_41[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_6 (Encoder2)          (None, 30)           38910       ['input_42[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 60)           0           ['encoder1_6[1][0]',             \n",
      "                                                                  'encoder2_6[1][0]']             \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 64)           3904        ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64)          256         ['dense_51[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 64)           0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_52 (Dense)               (None, 32)           2080        ['dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 32)           0           ['dense_52[0][0]']               \n",
      "                                                                                                  \n",
      " dense_53 (Dense)               (None, 16)           528         ['dropout_27[0][0]']             \n",
      "                                                                                                  \n",
      " dense_54 (Dense)               (None, 4)            68          ['dense_53[0][0]']               \n",
      "                                                                                                  \n",
      " dense_55 (Dense)               (None, 2)            10          ['dense_54[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 5s 7ms/step - loss: 0.5561 - sparse_categorical_accuracy: 0.7156 - val_loss: 0.5201 - val_sparse_categorical_accuracy: 0.7668\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.5070 - sparse_categorical_accuracy: 0.7581 - val_loss: 0.4778 - val_sparse_categorical_accuracy: 0.7642\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4912 - sparse_categorical_accuracy: 0.7708 - val_loss: 0.4627 - val_sparse_categorical_accuracy: 0.7778\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4913 - sparse_categorical_accuracy: 0.7723 - val_loss: 0.4607 - val_sparse_categorical_accuracy: 0.7829\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4818 - sparse_categorical_accuracy: 0.7780 - val_loss: 0.4577 - val_sparse_categorical_accuracy: 0.7846\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4761 - sparse_categorical_accuracy: 0.7774 - val_loss: 0.4594 - val_sparse_categorical_accuracy: 0.7871\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4762 - sparse_categorical_accuracy: 0.7803 - val_loss: 0.4607 - val_sparse_categorical_accuracy: 0.7769\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4879 - sparse_categorical_accuracy: 0.7761 - val_loss: 0.4542 - val_sparse_categorical_accuracy: 0.7888\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4723 - sparse_categorical_accuracy: 0.7844 - val_loss: 0.4563 - val_sparse_categorical_accuracy: 0.7880\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4786 - sparse_categorical_accuracy: 0.7791 - val_loss: 0.4561 - val_sparse_categorical_accuracy: 0.7795\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4747 - sparse_categorical_accuracy: 0.7772 - val_loss: 0.4512 - val_sparse_categorical_accuracy: 0.7913\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4678 - sparse_categorical_accuracy: 0.7823 - val_loss: 0.4615 - val_sparse_categorical_accuracy: 0.7769\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4703 - sparse_categorical_accuracy: 0.7859 - val_loss: 0.4545 - val_sparse_categorical_accuracy: 0.7863\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4730 - sparse_categorical_accuracy: 0.7835 - val_loss: 0.4528 - val_sparse_categorical_accuracy: 0.7854\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4681 - sparse_categorical_accuracy: 0.7833 - val_loss: 0.4558 - val_sparse_categorical_accuracy: 0.7829\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4723 - sparse_categorical_accuracy: 0.7784 - val_loss: 0.4482 - val_sparse_categorical_accuracy: 0.7913\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 5ms/step - loss: 0.4601 - sparse_categorical_accuracy: 0.7876 - val_loss: 0.4530 - val_sparse_categorical_accuracy: 0.7863\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4715 - sparse_categorical_accuracy: 0.7831 - val_loss: 0.4541 - val_sparse_categorical_accuracy: 0.7871\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4721 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4531 - val_sparse_categorical_accuracy: 0.7803\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4598 - sparse_categorical_accuracy: 0.7914 - val_loss: 0.4421 - val_sparse_categorical_accuracy: 0.7939\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4592 - sparse_categorical_accuracy: 0.7899 - val_loss: 0.4581 - val_sparse_categorical_accuracy: 0.7888\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4610 - sparse_categorical_accuracy: 0.7884 - val_loss: 0.4423 - val_sparse_categorical_accuracy: 0.7930\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4656 - sparse_categorical_accuracy: 0.7905 - val_loss: 0.4492 - val_sparse_categorical_accuracy: 0.7846\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4542 - sparse_categorical_accuracy: 0.7846 - val_loss: 0.4396 - val_sparse_categorical_accuracy: 0.7973\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4656 - sparse_categorical_accuracy: 0.7899 - val_loss: 0.4465 - val_sparse_categorical_accuracy: 0.7913\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4662 - sparse_categorical_accuracy: 0.7920 - val_loss: 0.4474 - val_sparse_categorical_accuracy: 0.7812\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4690 - sparse_categorical_accuracy: 0.7814 - val_loss: 0.4357 - val_sparse_categorical_accuracy: 0.7990\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4546 - sparse_categorical_accuracy: 0.7895 - val_loss: 0.4424 - val_sparse_categorical_accuracy: 0.7829\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4558 - sparse_categorical_accuracy: 0.7978 - val_loss: 0.4386 - val_sparse_categorical_accuracy: 0.7922\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4626 - sparse_categorical_accuracy: 0.7880 - val_loss: 0.4387 - val_sparse_categorical_accuracy: 0.7905\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4588 - sparse_categorical_accuracy: 0.7867 - val_loss: 0.4459 - val_sparse_categorical_accuracy: 0.7913\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4524 - sparse_categorical_accuracy: 0.7965 - val_loss: 0.4431 - val_sparse_categorical_accuracy: 0.7930\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4579 - sparse_categorical_accuracy: 0.7871 - val_loss: 0.4345 - val_sparse_categorical_accuracy: 0.7922\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4529 - sparse_categorical_accuracy: 0.7946 - val_loss: 0.4395 - val_sparse_categorical_accuracy: 0.7888\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4563 - sparse_categorical_accuracy: 0.7954 - val_loss: 0.4391 - val_sparse_categorical_accuracy: 0.7905\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4442 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.4333 - val_sparse_categorical_accuracy: 0.7964\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4566 - sparse_categorical_accuracy: 0.7937 - val_loss: 0.4378 - val_sparse_categorical_accuracy: 0.7930\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4567 - sparse_categorical_accuracy: 0.7907 - val_loss: 0.4377 - val_sparse_categorical_accuracy: 0.7964\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4524 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.4396 - val_sparse_categorical_accuracy: 0.7973\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4503 - sparse_categorical_accuracy: 0.7848 - val_loss: 0.4354 - val_sparse_categorical_accuracy: 0.7922\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4487 - sparse_categorical_accuracy: 0.7950 - val_loss: 0.4396 - val_sparse_categorical_accuracy: 0.7880\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4469 - sparse_categorical_accuracy: 0.7982 - val_loss: 0.4439 - val_sparse_categorical_accuracy: 0.7913\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4509 - sparse_categorical_accuracy: 0.7914 - val_loss: 0.4335 - val_sparse_categorical_accuracy: 0.7973\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4506 - sparse_categorical_accuracy: 0.7935 - val_loss: 0.4368 - val_sparse_categorical_accuracy: 0.8024\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4561 - sparse_categorical_accuracy: 0.7920 - val_loss: 0.4347 - val_sparse_categorical_accuracy: 0.7939\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4485 - sparse_categorical_accuracy: 0.7941 - val_loss: 0.4358 - val_sparse_categorical_accuracy: 0.7956\n",
      "Epoch 46: early stopping\n",
      "150/150 [==============================] - 1s 3ms/step - loss: 0.4197 - sparse_categorical_accuracy: 0.8075\n",
      "150/150 [==============================] - 1s 3ms/step\n",
      "Test accuracy: 80.748%\n",
      "Postprocessing Test accuracy: 85.3%\n",
      "Test F1_score: 79.026%\n",
      "Postprocessing F1_score: 83.278%\n",
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_45 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_46 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_7 (Encoder1)          (None, 30)           20670       ['input_45[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_7 (Encoder2)          (None, 30)           38910       ['input_46[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 60)           0           ['encoder1_7[0][0]',             \n",
      "                                                                  'encoder2_7[0][0]']             \n",
      "                                                                                                  \n",
      " dense_58 (Dense)               (None, 16)           976         ['concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "148/148 [==============================] - 4s 10ms/step - loss: 5.8305 - val_loss: 5.4544\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 5.4724 - val_loss: 5.4472\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 5.2558 - val_loss: 5.1632\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 5.2365 - val_loss: 5.2128\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 5.2394 - val_loss: 5.2037\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 5.1250 - val_loss: 5.0959\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 4.8554 - val_loss: 4.8679\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 4.6439 - val_loss: 4.2336\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 4.2078 - val_loss: 3.3827\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.5743 - val_loss: 2.3480\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2498 - val_loss: 2.1572\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2199 - val_loss: 2.2016\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.3007 - val_loss: 2.2146\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2570 - val_loss: 2.0743\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2922 - val_loss: 2.1078\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.5314 - val_loss: 2.0846\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 2.5386 - val_loss: 2.0236\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.4668 - val_loss: 2.0085\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.3289 - val_loss: 2.0925\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2714 - val_loss: 2.0925\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 4.4625 - val_loss: 5.3847\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 5.3456 - val_loss: 5.0577\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 5.0996 - val_loss: 5.1967\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 4.7784 - val_loss: 5.0610\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 4.8431 - val_loss: 5.0199\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 4.8505 - val_loss: 4.7014\n",
      "Epoch 27/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 4.7466 - val_loss: 4.6660\n",
      "Epoch 28/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 4.8064 - val_loss: 4.9919\n",
      "Epoch 28: early stopping\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_47 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_48 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_7 (Encoder1)          (None, 30)           20670       ['input_47[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_7 (Encoder2)          (None, 30)           38910       ['input_48[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 60)           0           ['encoder1_7[1][0]',             \n",
      "                                                                  'encoder2_7[1][0]']             \n",
      "                                                                                                  \n",
      " dense_59 (Dense)               (None, 64)           3904        ['concatenate_15[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64)          256         ['dense_59[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 64)           0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dense_60 (Dense)               (None, 32)           2080        ['dropout_30[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 32)           0           ['dense_60[0][0]']               \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 16)           528         ['dropout_31[0][0]']             \n",
      "                                                                                                  \n",
      " dense_62 (Dense)               (None, 4)            68          ['dense_61[0][0]']               \n",
      "                                                                                                  \n",
      " dense_63 (Dense)               (None, 2)            10          ['dense_62[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 5s 7ms/step - loss: 0.5712 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.4702 - val_sparse_categorical_accuracy: 0.7837\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4710 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.4351 - val_sparse_categorical_accuracy: 0.7939\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4667 - sparse_categorical_accuracy: 0.7871 - val_loss: 0.4259 - val_sparse_categorical_accuracy: 0.7998\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4480 - sparse_categorical_accuracy: 0.7990 - val_loss: 0.4128 - val_sparse_categorical_accuracy: 0.8092\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4468 - sparse_categorical_accuracy: 0.7935 - val_loss: 0.4064 - val_sparse_categorical_accuracy: 0.8083\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4335 - sparse_categorical_accuracy: 0.8024 - val_loss: 0.4047 - val_sparse_categorical_accuracy: 0.8176\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4397 - sparse_categorical_accuracy: 0.8007 - val_loss: 0.4004 - val_sparse_categorical_accuracy: 0.8185\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4192 - sparse_categorical_accuracy: 0.8113 - val_loss: 0.4081 - val_sparse_categorical_accuracy: 0.8142\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4199 - sparse_categorical_accuracy: 0.8081 - val_loss: 0.4058 - val_sparse_categorical_accuracy: 0.8210\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4192 - sparse_categorical_accuracy: 0.8065 - val_loss: 0.4069 - val_sparse_categorical_accuracy: 0.8185\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4148 - sparse_categorical_accuracy: 0.8158 - val_loss: 0.3948 - val_sparse_categorical_accuracy: 0.8261\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4179 - sparse_categorical_accuracy: 0.8090 - val_loss: 0.3994 - val_sparse_categorical_accuracy: 0.8236\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4154 - sparse_categorical_accuracy: 0.8202 - val_loss: 0.3956 - val_sparse_categorical_accuracy: 0.8312\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4105 - sparse_categorical_accuracy: 0.8122 - val_loss: 0.3966 - val_sparse_categorical_accuracy: 0.8210\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4178 - sparse_categorical_accuracy: 0.8109 - val_loss: 0.3957 - val_sparse_categorical_accuracy: 0.8270\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4129 - sparse_categorical_accuracy: 0.8171 - val_loss: 0.3913 - val_sparse_categorical_accuracy: 0.8321\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4156 - sparse_categorical_accuracy: 0.8200 - val_loss: 0.3920 - val_sparse_categorical_accuracy: 0.8295\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4027 - sparse_categorical_accuracy: 0.8190 - val_loss: 0.3874 - val_sparse_categorical_accuracy: 0.8244\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4034 - sparse_categorical_accuracy: 0.8190 - val_loss: 0.3855 - val_sparse_categorical_accuracy: 0.8363\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4043 - sparse_categorical_accuracy: 0.8181 - val_loss: 0.3843 - val_sparse_categorical_accuracy: 0.8312\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4079 - sparse_categorical_accuracy: 0.8154 - val_loss: 0.3922 - val_sparse_categorical_accuracy: 0.8304\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4106 - sparse_categorical_accuracy: 0.8164 - val_loss: 0.3878 - val_sparse_categorical_accuracy: 0.8304\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4070 - sparse_categorical_accuracy: 0.8135 - val_loss: 0.3869 - val_sparse_categorical_accuracy: 0.8312\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4022 - sparse_categorical_accuracy: 0.8228 - val_loss: 0.3822 - val_sparse_categorical_accuracy: 0.8405\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4004 - sparse_categorical_accuracy: 0.8200 - val_loss: 0.3826 - val_sparse_categorical_accuracy: 0.8372\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3962 - sparse_categorical_accuracy: 0.8311 - val_loss: 0.3796 - val_sparse_categorical_accuracy: 0.8422\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3925 - sparse_categorical_accuracy: 0.8266 - val_loss: 0.3802 - val_sparse_categorical_accuracy: 0.8405\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3969 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.3818 - val_sparse_categorical_accuracy: 0.8397\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4004 - sparse_categorical_accuracy: 0.8232 - val_loss: 0.3808 - val_sparse_categorical_accuracy: 0.8414\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3981 - sparse_categorical_accuracy: 0.8162 - val_loss: 0.3804 - val_sparse_categorical_accuracy: 0.8414\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3811 - sparse_categorical_accuracy: 0.8319 - val_loss: 0.3786 - val_sparse_categorical_accuracy: 0.8431\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3941 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.3815 - val_sparse_categorical_accuracy: 0.8355\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3940 - sparse_categorical_accuracy: 0.8245 - val_loss: 0.3772 - val_sparse_categorical_accuracy: 0.8405\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3942 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.3835 - val_sparse_categorical_accuracy: 0.8346\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3904 - sparse_categorical_accuracy: 0.8272 - val_loss: 0.3839 - val_sparse_categorical_accuracy: 0.8346\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3881 - sparse_categorical_accuracy: 0.8287 - val_loss: 0.3821 - val_sparse_categorical_accuracy: 0.8397\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3887 - sparse_categorical_accuracy: 0.8355 - val_loss: 0.3811 - val_sparse_categorical_accuracy: 0.8422\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3877 - sparse_categorical_accuracy: 0.8264 - val_loss: 0.3775 - val_sparse_categorical_accuracy: 0.8448\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3856 - sparse_categorical_accuracy: 0.8302 - val_loss: 0.3798 - val_sparse_categorical_accuracy: 0.8397\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3875 - sparse_categorical_accuracy: 0.8343 - val_loss: 0.3744 - val_sparse_categorical_accuracy: 0.8490\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3817 - sparse_categorical_accuracy: 0.8334 - val_loss: 0.3793 - val_sparse_categorical_accuracy: 0.8405\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3855 - sparse_categorical_accuracy: 0.8306 - val_loss: 0.3790 - val_sparse_categorical_accuracy: 0.8355\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3777 - sparse_categorical_accuracy: 0.8336 - val_loss: 0.3767 - val_sparse_categorical_accuracy: 0.8431\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3824 - sparse_categorical_accuracy: 0.8277 - val_loss: 0.3718 - val_sparse_categorical_accuracy: 0.8465\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3837 - sparse_categorical_accuracy: 0.8294 - val_loss: 0.3744 - val_sparse_categorical_accuracy: 0.8312\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3894 - sparse_categorical_accuracy: 0.8300 - val_loss: 0.3706 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3857 - sparse_categorical_accuracy: 0.8217 - val_loss: 0.3770 - val_sparse_categorical_accuracy: 0.8422\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3803 - sparse_categorical_accuracy: 0.8330 - val_loss: 0.3743 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3832 - sparse_categorical_accuracy: 0.8336 - val_loss: 0.3702 - val_sparse_categorical_accuracy: 0.8380\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3867 - sparse_categorical_accuracy: 0.8289 - val_loss: 0.3734 - val_sparse_categorical_accuracy: 0.8456\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3771 - sparse_categorical_accuracy: 0.8336 - val_loss: 0.3663 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3854 - sparse_categorical_accuracy: 0.8313 - val_loss: 0.3722 - val_sparse_categorical_accuracy: 0.8482\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3848 - sparse_categorical_accuracy: 0.8332 - val_loss: 0.3748 - val_sparse_categorical_accuracy: 0.8431\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3719 - sparse_categorical_accuracy: 0.8368 - val_loss: 0.3695 - val_sparse_categorical_accuracy: 0.8482\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3821 - sparse_categorical_accuracy: 0.8317 - val_loss: 0.3700 - val_sparse_categorical_accuracy: 0.8448\n",
      "Epoch 56/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3874 - sparse_categorical_accuracy: 0.8287 - val_loss: 0.3760 - val_sparse_categorical_accuracy: 0.8397\n",
      "Epoch 57/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3843 - sparse_categorical_accuracy: 0.8272 - val_loss: 0.3693 - val_sparse_categorical_accuracy: 0.8465\n",
      "Epoch 58/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3794 - sparse_categorical_accuracy: 0.8317 - val_loss: 0.3681 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 59/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3796 - sparse_categorical_accuracy: 0.8330 - val_loss: 0.3764 - val_sparse_categorical_accuracy: 0.8414\n",
      "Epoch 60/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3874 - sparse_categorical_accuracy: 0.8289 - val_loss: 0.3666 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 61/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3848 - sparse_categorical_accuracy: 0.8315 - val_loss: 0.3669 - val_sparse_categorical_accuracy: 0.8448\n",
      "Epoch 61: early stopping\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.3468 - sparse_categorical_accuracy: 0.8524\n",
      "150/150 [==============================] - 1s 2ms/step\n",
      "Test accuracy: 85.237%\n",
      "Postprocessing Test accuracy: 89.351%\n",
      "Test F1_score: 83.646%\n",
      "Postprocessing F1_score: 87.886%\n",
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_51 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_52 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_8 (Encoder1)          (None, 30)           20670       ['input_51[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_8 (Encoder2)          (None, 30)           38910       ['input_52[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 60)           0           ['encoder1_8[0][0]',             \n",
      "                                                                  'encoder2_8[0][0]']             \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 16)           976         ['concatenate_16[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "148/148 [==============================] - 4s 9ms/step - loss: 4.2921 - val_loss: 4.2000\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 4.0693 - val_loss: 4.0571\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 4.1233 - val_loss: 3.9475\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 3.9086 - val_loss: 3.9422\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.8219 - val_loss: 3.7868\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.9586 - val_loss: 3.8898\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.8547 - val_loss: 3.8582\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.8454 - val_loss: 3.8867\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.7792 - val_loss: 3.7184\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.6755 - val_loss: 3.7105\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.5879 - val_loss: 3.4983\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.5647 - val_loss: 3.8037\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.5392 - val_loss: 3.7352\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.6375 - val_loss: 3.5459\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.7974 - val_loss: 3.8069\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.6560 - val_loss: 3.6329\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.4728 - val_loss: 3.4456\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.3369 - val_loss: 3.2257\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 3.0011 - val_loss: 3.0051\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.9372 - val_loss: 2.9632\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.6792 - val_loss: 2.6831\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.4519 - val_loss: 2.4466\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.4128 - val_loss: 2.8538\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.5167 - val_loss: 2.6909\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.2477 - val_loss: 2.2490\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.9562 - val_loss: 1.8750\n",
      "Epoch 27/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.7031 - val_loss: 1.7930\n",
      "Epoch 28/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.5938 - val_loss: 1.7715\n",
      "Epoch 29/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.5164 - val_loss: 1.7552\n",
      "Epoch 30/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.4746 - val_loss: 1.6720\n",
      "Epoch 31/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.4824 - val_loss: 1.6367\n",
      "Epoch 32/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.4107 - val_loss: 1.5999\n",
      "Epoch 33/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3048 - val_loss: 1.5820\n",
      "Epoch 34/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3669 - val_loss: 1.5765\n",
      "Epoch 35/100\n",
      "148/148 [==============================] - 1s 7ms/step - loss: 1.4161 - val_loss: 1.5382\n",
      "Epoch 36/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3814 - val_loss: 1.5409\n",
      "Epoch 37/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3123 - val_loss: 1.4991\n",
      "Epoch 38/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3600 - val_loss: 1.4991\n",
      "Epoch 39/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.4043 - val_loss: 1.4991\n",
      "Epoch 40/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3350 - val_loss: 1.4726\n",
      "Epoch 41/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.5185 - val_loss: 1.8183\n",
      "Epoch 42/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.6004 - val_loss: 1.6012\n",
      "Epoch 43/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.4144 - val_loss: 1.7222\n",
      "Epoch 44/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.5173 - val_loss: 1.5460\n",
      "Epoch 45/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3866 - val_loss: 1.5977\n",
      "Epoch 46/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3733 - val_loss: 1.5783\n",
      "Epoch 47/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.4187 - val_loss: 1.5783\n",
      "Epoch 48/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3655 - val_loss: 1.5930\n",
      "Epoch 49/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.2956 - val_loss: 1.5930\n",
      "Epoch 50/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3494 - val_loss: 1.5912\n",
      "Epoch 50: early stopping\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_53 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_54 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_8 (Encoder1)          (None, 30)           20670       ['input_53[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_8 (Encoder2)          (None, 30)           38910       ['input_54[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 60)           0           ['encoder1_8[1][0]',             \n",
      "                                                                  'encoder2_8[1][0]']             \n",
      "                                                                                                  \n",
      " dense_67 (Dense)               (None, 64)           3904        ['concatenate_17[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 64)          256         ['dense_67[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 64)           0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_68 (Dense)               (None, 32)           2080        ['dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 32)           0           ['dense_68[0][0]']               \n",
      "                                                                                                  \n",
      " dense_69 (Dense)               (None, 16)           528         ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 4)            68          ['dense_69[0][0]']               \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 2)            10          ['dense_70[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 5s 7ms/step - loss: 0.5881 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.4723 - val_sparse_categorical_accuracy: 0.7913\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4881 - sparse_categorical_accuracy: 0.7725 - val_loss: 0.4287 - val_sparse_categorical_accuracy: 0.8092\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4613 - sparse_categorical_accuracy: 0.7914 - val_loss: 0.4042 - val_sparse_categorical_accuracy: 0.8168\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4451 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.4029 - val_sparse_categorical_accuracy: 0.8193\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4413 - sparse_categorical_accuracy: 0.7958 - val_loss: 0.3926 - val_sparse_categorical_accuracy: 0.8355\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4297 - sparse_categorical_accuracy: 0.8050 - val_loss: 0.3912 - val_sparse_categorical_accuracy: 0.8321\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4327 - sparse_categorical_accuracy: 0.8062 - val_loss: 0.3919 - val_sparse_categorical_accuracy: 0.8388\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4224 - sparse_categorical_accuracy: 0.8092 - val_loss: 0.3817 - val_sparse_categorical_accuracy: 0.8414\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4170 - sparse_categorical_accuracy: 0.8124 - val_loss: 0.3838 - val_sparse_categorical_accuracy: 0.8388\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4229 - sparse_categorical_accuracy: 0.8098 - val_loss: 0.3817 - val_sparse_categorical_accuracy: 0.8414\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4363 - sparse_categorical_accuracy: 0.8035 - val_loss: 0.3864 - val_sparse_categorical_accuracy: 0.8329\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4194 - sparse_categorical_accuracy: 0.8098 - val_loss: 0.3825 - val_sparse_categorical_accuracy: 0.8448\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4236 - sparse_categorical_accuracy: 0.8073 - val_loss: 0.3754 - val_sparse_categorical_accuracy: 0.8405\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4248 - sparse_categorical_accuracy: 0.8111 - val_loss: 0.3744 - val_sparse_categorical_accuracy: 0.8456\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4126 - sparse_categorical_accuracy: 0.8152 - val_loss: 0.3745 - val_sparse_categorical_accuracy: 0.8482\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4024 - sparse_categorical_accuracy: 0.8241 - val_loss: 0.3719 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4051 - sparse_categorical_accuracy: 0.8154 - val_loss: 0.3721 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4094 - sparse_categorical_accuracy: 0.8109 - val_loss: 0.3706 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4015 - sparse_categorical_accuracy: 0.8228 - val_loss: 0.3746 - val_sparse_categorical_accuracy: 0.8380\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4008 - sparse_categorical_accuracy: 0.8262 - val_loss: 0.3689 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3962 - sparse_categorical_accuracy: 0.8241 - val_loss: 0.3678 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4005 - sparse_categorical_accuracy: 0.8211 - val_loss: 0.3703 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3928 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.3812 - val_sparse_categorical_accuracy: 0.8439\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4047 - sparse_categorical_accuracy: 0.8232 - val_loss: 0.3717 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3967 - sparse_categorical_accuracy: 0.8253 - val_loss: 0.3710 - val_sparse_categorical_accuracy: 0.8490\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3928 - sparse_categorical_accuracy: 0.8234 - val_loss: 0.3695 - val_sparse_categorical_accuracy: 0.8456\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3986 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.3666 - val_sparse_categorical_accuracy: 0.8490\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3897 - sparse_categorical_accuracy: 0.8249 - val_loss: 0.3641 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4103 - sparse_categorical_accuracy: 0.8147 - val_loss: 0.3712 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3938 - sparse_categorical_accuracy: 0.8249 - val_loss: 0.3693 - val_sparse_categorical_accuracy: 0.8405\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3916 - sparse_categorical_accuracy: 0.8224 - val_loss: 0.3657 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3976 - sparse_categorical_accuracy: 0.8266 - val_loss: 0.3602 - val_sparse_categorical_accuracy: 0.8567\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3925 - sparse_categorical_accuracy: 0.8262 - val_loss: 0.3657 - val_sparse_categorical_accuracy: 0.8422\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3849 - sparse_categorical_accuracy: 0.8245 - val_loss: 0.3597 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3864 - sparse_categorical_accuracy: 0.8287 - val_loss: 0.3566 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3883 - sparse_categorical_accuracy: 0.8289 - val_loss: 0.3657 - val_sparse_categorical_accuracy: 0.8456\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3787 - sparse_categorical_accuracy: 0.8383 - val_loss: 0.3573 - val_sparse_categorical_accuracy: 0.8533\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3873 - sparse_categorical_accuracy: 0.8266 - val_loss: 0.3588 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3838 - sparse_categorical_accuracy: 0.8340 - val_loss: 0.3545 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3856 - sparse_categorical_accuracy: 0.8283 - val_loss: 0.3564 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3831 - sparse_categorical_accuracy: 0.8304 - val_loss: 0.3599 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3824 - sparse_categorical_accuracy: 0.8285 - val_loss: 0.3582 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3810 - sparse_categorical_accuracy: 0.8283 - val_loss: 0.3596 - val_sparse_categorical_accuracy: 0.8567\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3942 - sparse_categorical_accuracy: 0.8256 - val_loss: 0.3577 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3790 - sparse_categorical_accuracy: 0.8368 - val_loss: 0.3589 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3912 - sparse_categorical_accuracy: 0.8247 - val_loss: 0.3558 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3816 - sparse_categorical_accuracy: 0.8300 - val_loss: 0.3502 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3809 - sparse_categorical_accuracy: 0.8315 - val_loss: 0.3476 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3785 - sparse_categorical_accuracy: 0.8330 - val_loss: 0.3564 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3870 - sparse_categorical_accuracy: 0.8230 - val_loss: 0.3476 - val_sparse_categorical_accuracy: 0.8651\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3824 - sparse_categorical_accuracy: 0.8370 - val_loss: 0.3505 - val_sparse_categorical_accuracy: 0.8567\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3839 - sparse_categorical_accuracy: 0.8319 - val_loss: 0.3476 - val_sparse_categorical_accuracy: 0.8533\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3727 - sparse_categorical_accuracy: 0.8315 - val_loss: 0.3573 - val_sparse_categorical_accuracy: 0.8397\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3739 - sparse_categorical_accuracy: 0.8362 - val_loss: 0.3494 - val_sparse_categorical_accuracy: 0.8524\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3756 - sparse_categorical_accuracy: 0.8353 - val_loss: 0.3493 - val_sparse_categorical_accuracy: 0.8567\n",
      "Epoch 56/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3826 - sparse_categorical_accuracy: 0.8319 - val_loss: 0.3467 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 57/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3764 - sparse_categorical_accuracy: 0.8343 - val_loss: 0.3512 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 58/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3727 - sparse_categorical_accuracy: 0.8436 - val_loss: 0.3419 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 59/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3747 - sparse_categorical_accuracy: 0.8368 - val_loss: 0.3489 - val_sparse_categorical_accuracy: 0.8617\n",
      "Epoch 60/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3707 - sparse_categorical_accuracy: 0.8385 - val_loss: 0.3492 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 61/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3764 - sparse_categorical_accuracy: 0.8321 - val_loss: 0.3427 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 62/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3706 - sparse_categorical_accuracy: 0.8393 - val_loss: 0.3562 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 63/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3859 - sparse_categorical_accuracy: 0.8294 - val_loss: 0.3466 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 64/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3661 - sparse_categorical_accuracy: 0.8364 - val_loss: 0.3417 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 65/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3676 - sparse_categorical_accuracy: 0.8391 - val_loss: 0.3492 - val_sparse_categorical_accuracy: 0.8524\n",
      "Epoch 66/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3760 - sparse_categorical_accuracy: 0.8372 - val_loss: 0.3495 - val_sparse_categorical_accuracy: 0.8524\n",
      "Epoch 67/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3726 - sparse_categorical_accuracy: 0.8383 - val_loss: 0.3451 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 68/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3751 - sparse_categorical_accuracy: 0.8385 - val_loss: 0.3529 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 69/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3744 - sparse_categorical_accuracy: 0.8376 - val_loss: 0.3491 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 70/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3755 - sparse_categorical_accuracy: 0.8387 - val_loss: 0.3500 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 71/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3628 - sparse_categorical_accuracy: 0.8404 - val_loss: 0.3499 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 72/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3740 - sparse_categorical_accuracy: 0.8353 - val_loss: 0.3473 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 73/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3600 - sparse_categorical_accuracy: 0.8440 - val_loss: 0.3497 - val_sparse_categorical_accuracy: 0.8490\n",
      "Epoch 74/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3672 - sparse_categorical_accuracy: 0.8430 - val_loss: 0.3472 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 74: early stopping\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.3437 - sparse_categorical_accuracy: 0.8515\n",
      "150/150 [==============================] - 1s 3ms/step\n",
      "Test accuracy: 85.153%\n",
      "Postprocessing Test accuracy: 89.559%\n",
      "Test F1_score: 83.591%\n",
      "Postprocessing F1_score: 88.124%\n",
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_57 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_58 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_9 (Encoder1)          (None, 30)           20670       ['input_57[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_9 (Encoder2)          (None, 30)           38910       ['input_58[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 60)           0           ['encoder1_9[0][0]',             \n",
      "                                                                  'encoder2_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_74 (Dense)               (None, 16)           976         ['concatenate_18[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "148/148 [==============================] - 4s 9ms/step - loss: 2.8808 - val_loss: 2.7153\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.6501 - val_loss: 2.6046\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.5002 - val_loss: 2.7504\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.7713 - val_loss: 2.7877\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.6996 - val_loss: 2.6848\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 2.5312 - val_loss: 2.4182\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.5510 - val_loss: 2.4941\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.6190 - val_loss: 2.7848\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 2.3885 - val_loss: 2.4323\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.1078 - val_loss: 2.0984\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 2.0836 - val_loss: 1.9967\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.7065 - val_loss: 1.6003\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.5896 - val_loss: 1.7168\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.5022 - val_loss: 1.4344\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.2833 - val_loss: 1.3523\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1323 - val_loss: 1.1484\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0833 - val_loss: 1.1102\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1146 - val_loss: 1.1345\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1334 - val_loss: 1.1480\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1638 - val_loss: 1.2217\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1476 - val_loss: 1.2706\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1637 - val_loss: 1.1515\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1786 - val_loss: 1.0949\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1365 - val_loss: 1.0951\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0452 - val_loss: 1.0951\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1833 - val_loss: 1.0458\n",
      "Epoch 27/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1281 - val_loss: 1.0458\n",
      "Epoch 28/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.2892 - val_loss: 1.0458\n",
      "Epoch 29/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.2715 - val_loss: 1.0458\n",
      "Epoch 30/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1510 - val_loss: 0.7822\n",
      "Epoch 31/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1264 - val_loss: 0.7185\n",
      "Epoch 32/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.2172 - val_loss: 0.7089\n",
      "Epoch 33/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.2948 - val_loss: 0.7134\n",
      "Epoch 34/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1173 - val_loss: 0.8204\n",
      "Epoch 35/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0960 - val_loss: 0.8204\n",
      "Epoch 36/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1121 - val_loss: 0.8204\n",
      "Epoch 37/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0797 - val_loss: 0.8173\n",
      "Epoch 38/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1731 - val_loss: 0.8173\n",
      "Epoch 39/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0558 - val_loss: 0.8147\n",
      "Epoch 40/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1030 - val_loss: 0.8147\n",
      "Epoch 41/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1751 - val_loss: 0.8147\n",
      "Epoch 42/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0176 - val_loss: 0.8147\n",
      "Epoch 42: early stopping\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_59 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_60 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_9 (Encoder1)          (None, 30)           20670       ['input_59[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_9 (Encoder2)          (None, 30)           38910       ['input_60[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 60)           0           ['encoder1_9[1][0]',             \n",
      "                                                                  'encoder2_9[1][0]']             \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 64)           3904        ['concatenate_19[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 64)          256         ['dense_75[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 64)           0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 32)           2080        ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, 32)           0           ['dense_76[0][0]']               \n",
      "                                                                                                  \n",
      " dense_77 (Dense)               (None, 16)           528         ['dropout_39[0][0]']             \n",
      "                                                                                                  \n",
      " dense_78 (Dense)               (None, 4)            68          ['dense_77[0][0]']               \n",
      "                                                                                                  \n",
      " dense_79 (Dense)               (None, 2)            10          ['dense_78[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 5s 8ms/step - loss: 0.5904 - sparse_categorical_accuracy: 0.6753 - val_loss: 0.4852 - val_sparse_categorical_accuracy: 0.7769\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4813 - sparse_categorical_accuracy: 0.7827 - val_loss: 0.4503 - val_sparse_categorical_accuracy: 0.7854\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4679 - sparse_categorical_accuracy: 0.7880 - val_loss: 0.4466 - val_sparse_categorical_accuracy: 0.7956\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4730 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.4409 - val_sparse_categorical_accuracy: 0.7939\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4680 - sparse_categorical_accuracy: 0.7854 - val_loss: 0.4376 - val_sparse_categorical_accuracy: 0.7939\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4645 - sparse_categorical_accuracy: 0.7918 - val_loss: 0.4465 - val_sparse_categorical_accuracy: 0.7939\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4665 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.4391 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4559 - sparse_categorical_accuracy: 0.7907 - val_loss: 0.4373 - val_sparse_categorical_accuracy: 0.7973\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4591 - sparse_categorical_accuracy: 0.7924 - val_loss: 0.4340 - val_sparse_categorical_accuracy: 0.8007\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4549 - sparse_categorical_accuracy: 0.7937 - val_loss: 0.4348 - val_sparse_categorical_accuracy: 0.7990\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4433 - sparse_categorical_accuracy: 0.8001 - val_loss: 0.4290 - val_sparse_categorical_accuracy: 0.8058\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4556 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.4281 - val_sparse_categorical_accuracy: 0.7990\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4517 - sparse_categorical_accuracy: 0.7997 - val_loss: 0.4299 - val_sparse_categorical_accuracy: 0.8083\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4429 - sparse_categorical_accuracy: 0.8026 - val_loss: 0.4300 - val_sparse_categorical_accuracy: 0.8058\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4447 - sparse_categorical_accuracy: 0.7975 - val_loss: 0.4350 - val_sparse_categorical_accuracy: 0.8032\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4389 - sparse_categorical_accuracy: 0.8003 - val_loss: 0.4268 - val_sparse_categorical_accuracy: 0.8066\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4415 - sparse_categorical_accuracy: 0.8005 - val_loss: 0.4222 - val_sparse_categorical_accuracy: 0.8092\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4427 - sparse_categorical_accuracy: 0.8022 - val_loss: 0.4180 - val_sparse_categorical_accuracy: 0.8151\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4432 - sparse_categorical_accuracy: 0.7956 - val_loss: 0.4216 - val_sparse_categorical_accuracy: 0.8100\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4359 - sparse_categorical_accuracy: 0.8094 - val_loss: 0.4197 - val_sparse_categorical_accuracy: 0.8066\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4318 - sparse_categorical_accuracy: 0.8090 - val_loss: 0.4163 - val_sparse_categorical_accuracy: 0.8126\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4402 - sparse_categorical_accuracy: 0.8020 - val_loss: 0.4235 - val_sparse_categorical_accuracy: 0.8100\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4489 - sparse_categorical_accuracy: 0.7961 - val_loss: 0.4237 - val_sparse_categorical_accuracy: 0.8083\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4377 - sparse_categorical_accuracy: 0.8045 - val_loss: 0.4183 - val_sparse_categorical_accuracy: 0.8066\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4387 - sparse_categorical_accuracy: 0.7990 - val_loss: 0.4195 - val_sparse_categorical_accuracy: 0.8109\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4328 - sparse_categorical_accuracy: 0.8037 - val_loss: 0.4151 - val_sparse_categorical_accuracy: 0.8109\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4358 - sparse_categorical_accuracy: 0.8050 - val_loss: 0.4152 - val_sparse_categorical_accuracy: 0.8176\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4343 - sparse_categorical_accuracy: 0.8075 - val_loss: 0.4137 - val_sparse_categorical_accuracy: 0.8117\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4338 - sparse_categorical_accuracy: 0.8084 - val_loss: 0.4162 - val_sparse_categorical_accuracy: 0.8202\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4383 - sparse_categorical_accuracy: 0.8077 - val_loss: 0.4166 - val_sparse_categorical_accuracy: 0.8134\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4303 - sparse_categorical_accuracy: 0.8122 - val_loss: 0.4138 - val_sparse_categorical_accuracy: 0.8159\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4356 - sparse_categorical_accuracy: 0.8056 - val_loss: 0.4160 - val_sparse_categorical_accuracy: 0.8210\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.4366 - sparse_categorical_accuracy: 0.8011 - val_loss: 0.4130 - val_sparse_categorical_accuracy: 0.8219\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4337 - sparse_categorical_accuracy: 0.8056 - val_loss: 0.4096 - val_sparse_categorical_accuracy: 0.8193\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4328 - sparse_categorical_accuracy: 0.8054 - val_loss: 0.4078 - val_sparse_categorical_accuracy: 0.8202\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4238 - sparse_categorical_accuracy: 0.8065 - val_loss: 0.4088 - val_sparse_categorical_accuracy: 0.8202\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4336 - sparse_categorical_accuracy: 0.8107 - val_loss: 0.4133 - val_sparse_categorical_accuracy: 0.8185\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4290 - sparse_categorical_accuracy: 0.8073 - val_loss: 0.4118 - val_sparse_categorical_accuracy: 0.8219\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4325 - sparse_categorical_accuracy: 0.7999 - val_loss: 0.4105 - val_sparse_categorical_accuracy: 0.8185\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4321 - sparse_categorical_accuracy: 0.8067 - val_loss: 0.4143 - val_sparse_categorical_accuracy: 0.8109\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4349 - sparse_categorical_accuracy: 0.8084 - val_loss: 0.4123 - val_sparse_categorical_accuracy: 0.8185\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4325 - sparse_categorical_accuracy: 0.8003 - val_loss: 0.4094 - val_sparse_categorical_accuracy: 0.8168\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4275 - sparse_categorical_accuracy: 0.8107 - val_loss: 0.4108 - val_sparse_categorical_accuracy: 0.8176\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4289 - sparse_categorical_accuracy: 0.8088 - val_loss: 0.4114 - val_sparse_categorical_accuracy: 0.8193\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4309 - sparse_categorical_accuracy: 0.8096 - val_loss: 0.4050 - val_sparse_categorical_accuracy: 0.8151\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4313 - sparse_categorical_accuracy: 0.8056 - val_loss: 0.4064 - val_sparse_categorical_accuracy: 0.8210\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4260 - sparse_categorical_accuracy: 0.8090 - val_loss: 0.4126 - val_sparse_categorical_accuracy: 0.8168\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4281 - sparse_categorical_accuracy: 0.8054 - val_loss: 0.4025 - val_sparse_categorical_accuracy: 0.8227\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4280 - sparse_categorical_accuracy: 0.8090 - val_loss: 0.4061 - val_sparse_categorical_accuracy: 0.8193\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4208 - sparse_categorical_accuracy: 0.8079 - val_loss: 0.4039 - val_sparse_categorical_accuracy: 0.8210\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4228 - sparse_categorical_accuracy: 0.8111 - val_loss: 0.4065 - val_sparse_categorical_accuracy: 0.8219\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4238 - sparse_categorical_accuracy: 0.8077 - val_loss: 0.4057 - val_sparse_categorical_accuracy: 0.8227\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4286 - sparse_categorical_accuracy: 0.8109 - val_loss: 0.4065 - val_sparse_categorical_accuracy: 0.8253\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4258 - sparse_categorical_accuracy: 0.8128 - val_loss: 0.4059 - val_sparse_categorical_accuracy: 0.8210\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4249 - sparse_categorical_accuracy: 0.8058 - val_loss: 0.4098 - val_sparse_categorical_accuracy: 0.8202\n",
      "Epoch 56/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4254 - sparse_categorical_accuracy: 0.8130 - val_loss: 0.4002 - val_sparse_categorical_accuracy: 0.8219\n",
      "Epoch 57/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4285 - sparse_categorical_accuracy: 0.8069 - val_loss: 0.4023 - val_sparse_categorical_accuracy: 0.8270\n",
      "Epoch 58/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4214 - sparse_categorical_accuracy: 0.8111 - val_loss: 0.4054 - val_sparse_categorical_accuracy: 0.8202\n",
      "Epoch 59/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4292 - sparse_categorical_accuracy: 0.8090 - val_loss: 0.4012 - val_sparse_categorical_accuracy: 0.8219\n",
      "Epoch 60/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4222 - sparse_categorical_accuracy: 0.8045 - val_loss: 0.4055 - val_sparse_categorical_accuracy: 0.8287\n",
      "Epoch 61/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4181 - sparse_categorical_accuracy: 0.8143 - val_loss: 0.4013 - val_sparse_categorical_accuracy: 0.8193\n",
      "Epoch 62/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4213 - sparse_categorical_accuracy: 0.8135 - val_loss: 0.3991 - val_sparse_categorical_accuracy: 0.8227\n",
      "Epoch 63/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4208 - sparse_categorical_accuracy: 0.8120 - val_loss: 0.3978 - val_sparse_categorical_accuracy: 0.8270\n",
      "Epoch 64/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4169 - sparse_categorical_accuracy: 0.8105 - val_loss: 0.4015 - val_sparse_categorical_accuracy: 0.8244\n",
      "Epoch 65/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4264 - sparse_categorical_accuracy: 0.8058 - val_loss: 0.4031 - val_sparse_categorical_accuracy: 0.8287\n",
      "Epoch 66/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4202 - sparse_categorical_accuracy: 0.8094 - val_loss: 0.4042 - val_sparse_categorical_accuracy: 0.8210\n",
      "Epoch 67/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4187 - sparse_categorical_accuracy: 0.8164 - val_loss: 0.4050 - val_sparse_categorical_accuracy: 0.8202\n",
      "Epoch 68/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4247 - sparse_categorical_accuracy: 0.8143 - val_loss: 0.4058 - val_sparse_categorical_accuracy: 0.8270\n",
      "Epoch 69/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4164 - sparse_categorical_accuracy: 0.8158 - val_loss: 0.4012 - val_sparse_categorical_accuracy: 0.8287\n",
      "Epoch 70/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4154 - sparse_categorical_accuracy: 0.8137 - val_loss: 0.4002 - val_sparse_categorical_accuracy: 0.8193\n",
      "Epoch 71/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4178 - sparse_categorical_accuracy: 0.8107 - val_loss: 0.4017 - val_sparse_categorical_accuracy: 0.8185\n",
      "Epoch 72/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.4216 - sparse_categorical_accuracy: 0.8130 - val_loss: 0.4025 - val_sparse_categorical_accuracy: 0.8193\n",
      "Epoch 73/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4285 - sparse_categorical_accuracy: 0.8122 - val_loss: 0.4012 - val_sparse_categorical_accuracy: 0.8253\n",
      "Epoch 73: early stopping\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.3896 - sparse_categorical_accuracy: 0.8307\n",
      "150/150 [==============================] - 1s 3ms/step\n",
      "Test accuracy: 83.065%\n",
      "Postprocessing Test accuracy: 88.098%\n",
      "Test F1_score: 81.249%\n",
      "Postprocessing F1_score: 86.461%\n",
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_63 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_64 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_10 (Encoder1)         (None, 30)           20670       ['input_63[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_10 (Encoder2)         (None, 30)           38910       ['input_64[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 60)           0           ['encoder1_10[0][0]',            \n",
      "                                                                  'encoder2_10[0][0]']            \n",
      "                                                                                                  \n",
      " dense_82 (Dense)               (None, 16)           976         ['concatenate_20[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,556\n",
      "Trainable params: 60,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "148/148 [==============================] - 4s 9ms/step - loss: 1.4605 - val_loss: 1.3720\n",
      "Epoch 2/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3750 - val_loss: 1.3368\n",
      "Epoch 3/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.3390 - val_loss: 1.3118\n",
      "Epoch 4/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.3040 - val_loss: 1.3132\n",
      "Epoch 5/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.2904 - val_loss: 1.2510\n",
      "Epoch 6/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.2683 - val_loss: 1.3231\n",
      "Epoch 7/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.2764 - val_loss: 1.2737\n",
      "Epoch 8/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.2561 - val_loss: 1.2821\n",
      "Epoch 9/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.2370 - val_loss: 1.2659\n",
      "Epoch 10/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.2210 - val_loss: 1.1974\n",
      "Epoch 11/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.2280 - val_loss: 1.2274\n",
      "Epoch 12/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.2084 - val_loss: 1.1853\n",
      "Epoch 13/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.1369 - val_loss: 1.1211\n",
      "Epoch 14/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1568 - val_loss: 1.1297\n",
      "Epoch 15/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0991 - val_loss: 1.1098\n",
      "Epoch 16/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1123 - val_loss: 1.0770\n",
      "Epoch 17/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0785 - val_loss: 1.1508\n",
      "Epoch 18/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1458 - val_loss: 1.1230\n",
      "Epoch 19/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1505 - val_loss: 1.1092\n",
      "Epoch 20/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.1088 - val_loss: 1.0508\n",
      "Epoch 21/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0539 - val_loss: 1.0117\n",
      "Epoch 22/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 1.0495 - val_loss: 0.9952\n",
      "Epoch 23/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 1.0232 - val_loss: 0.9824\n",
      "Epoch 24/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.9459 - val_loss: 0.9382\n",
      "Epoch 25/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.7972 - val_loss: 0.8064\n",
      "Epoch 26/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.7762 - val_loss: 0.8321\n",
      "Epoch 27/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.7055 - val_loss: 0.7753\n",
      "Epoch 28/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.6993 - val_loss: 0.7737\n",
      "Epoch 29/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.6836 - val_loss: 0.7444\n",
      "Epoch 30/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.6126 - val_loss: 0.7162\n",
      "Epoch 31/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.6041 - val_loss: 0.7191\n",
      "Epoch 32/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.6000 - val_loss: 0.6982\n",
      "Epoch 33/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.5981 - val_loss: 0.7012\n",
      "Epoch 34/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.5939 - val_loss: 0.7302\n",
      "Epoch 35/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.6230 - val_loss: 0.7326\n",
      "Epoch 36/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.6265 - val_loss: 0.7157\n",
      "Epoch 37/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.6814 - val_loss: 0.7991\n",
      "Epoch 38/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.6842 - val_loss: 0.7251\n",
      "Epoch 39/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.6206 - val_loss: 0.6380\n",
      "Epoch 40/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.5435 - val_loss: 0.6215\n",
      "Epoch 41/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.5292 - val_loss: 0.5982\n",
      "Epoch 42/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.4894 - val_loss: 0.5982\n",
      "Epoch 43/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.5042 - val_loss: 0.5973\n",
      "Epoch 44/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.4937 - val_loss: 0.5806\n",
      "Epoch 45/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.4970 - val_loss: 0.5720\n",
      "Epoch 46/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.4861 - val_loss: 0.5807\n",
      "Epoch 47/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.4863 - val_loss: 0.5802\n",
      "Epoch 48/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.4887 - val_loss: 0.5802\n",
      "Epoch 49/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.4606 - val_loss: 0.5816\n",
      "Epoch 50/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.4905 - val_loss: 0.5917\n",
      "Epoch 51/100\n",
      "148/148 [==============================] - 1s 9ms/step - loss: 0.5011 - val_loss: 0.5918\n",
      "Epoch 52/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.5080 - val_loss: 0.5987\n",
      "Epoch 53/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.5087 - val_loss: 0.6016\n",
      "Epoch 54/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.5025 - val_loss: 0.6016\n",
      "Epoch 55/100\n",
      "148/148 [==============================] - 1s 8ms/step - loss: 0.4947 - val_loss: 0.6016\n",
      "Epoch 55: early stopping\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_65 (InputLayer)          [(None, 19)]         0           []                               \n",
      "                                                                                                  \n",
      " input_66 (InputLayer)          [(None, 38)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder1_10 (Encoder1)         (None, 30)           20670       ['input_65[0][0]']               \n",
      "                                                                                                  \n",
      " encoder2_10 (Encoder2)         (None, 30)           38910       ['input_66[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 60)           0           ['encoder1_10[1][0]',            \n",
      "                                                                  'encoder2_10[1][0]']            \n",
      "                                                                                                  \n",
      " dense_83 (Dense)               (None, 64)           3904        ['concatenate_21[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 64)          256         ['dense_83[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 64)           0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dense_84 (Dense)               (None, 32)           2080        ['dropout_42[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 32)           0           ['dense_84[0][0]']               \n",
      "                                                                                                  \n",
      " dense_85 (Dense)               (None, 16)           528         ['dropout_43[0][0]']             \n",
      "                                                                                                  \n",
      " dense_86 (Dense)               (None, 4)            68          ['dense_85[0][0]']               \n",
      "                                                                                                  \n",
      " dense_87 (Dense)               (None, 2)            10          ['dense_86[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,426\n",
      "Trainable params: 6,718\n",
      "Non-trainable params: 59,708\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "295/295 [==============================] - 5s 8ms/step - loss: 0.6432 - sparse_categorical_accuracy: 0.6464 - val_loss: 0.5659 - val_sparse_categorical_accuracy: 0.8024\n",
      "Epoch 2/200\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.5300 - sparse_categorical_accuracy: 0.7903 - val_loss: 0.4084 - val_sparse_categorical_accuracy: 0.8168\n",
      "Epoch 3/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4152 - sparse_categorical_accuracy: 0.8232 - val_loss: 0.3831 - val_sparse_categorical_accuracy: 0.8312\n",
      "Epoch 4/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3996 - sparse_categorical_accuracy: 0.8283 - val_loss: 0.3746 - val_sparse_categorical_accuracy: 0.8329\n",
      "Epoch 5/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.4091 - sparse_categorical_accuracy: 0.8209 - val_loss: 0.3697 - val_sparse_categorical_accuracy: 0.8422\n",
      "Epoch 6/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3991 - sparse_categorical_accuracy: 0.8285 - val_loss: 0.3637 - val_sparse_categorical_accuracy: 0.8422\n",
      "Epoch 7/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3838 - sparse_categorical_accuracy: 0.8343 - val_loss: 0.3632 - val_sparse_categorical_accuracy: 0.8405\n",
      "Epoch 8/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3846 - sparse_categorical_accuracy: 0.8381 - val_loss: 0.3585 - val_sparse_categorical_accuracy: 0.8431\n",
      "Epoch 9/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3818 - sparse_categorical_accuracy: 0.8374 - val_loss: 0.3620 - val_sparse_categorical_accuracy: 0.8397\n",
      "Epoch 10/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3827 - sparse_categorical_accuracy: 0.8379 - val_loss: 0.3544 - val_sparse_categorical_accuracy: 0.8456\n",
      "Epoch 11/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3781 - sparse_categorical_accuracy: 0.8366 - val_loss: 0.3597 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 12/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3755 - sparse_categorical_accuracy: 0.8432 - val_loss: 0.3522 - val_sparse_categorical_accuracy: 0.8448\n",
      "Epoch 13/200\n",
      "295/295 [==============================] - 2s 7ms/step - loss: 0.3698 - sparse_categorical_accuracy: 0.8396 - val_loss: 0.3583 - val_sparse_categorical_accuracy: 0.8388\n",
      "Epoch 14/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3731 - sparse_categorical_accuracy: 0.8362 - val_loss: 0.3455 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 15/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3684 - sparse_categorical_accuracy: 0.8362 - val_loss: 0.3538 - val_sparse_categorical_accuracy: 0.8482\n",
      "Epoch 16/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3679 - sparse_categorical_accuracy: 0.8400 - val_loss: 0.3562 - val_sparse_categorical_accuracy: 0.8456\n",
      "Epoch 17/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3631 - sparse_categorical_accuracy: 0.8430 - val_loss: 0.3474 - val_sparse_categorical_accuracy: 0.8482\n",
      "Epoch 18/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3584 - sparse_categorical_accuracy: 0.8493 - val_loss: 0.3498 - val_sparse_categorical_accuracy: 0.8524\n",
      "Epoch 19/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3624 - sparse_categorical_accuracy: 0.8434 - val_loss: 0.3470 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 20/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3611 - sparse_categorical_accuracy: 0.8432 - val_loss: 0.3412 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 21/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3565 - sparse_categorical_accuracy: 0.8487 - val_loss: 0.3598 - val_sparse_categorical_accuracy: 0.8363\n",
      "Epoch 22/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3619 - sparse_categorical_accuracy: 0.8447 - val_loss: 0.3479 - val_sparse_categorical_accuracy: 0.8524\n",
      "Epoch 23/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3543 - sparse_categorical_accuracy: 0.8468 - val_loss: 0.3520 - val_sparse_categorical_accuracy: 0.8448\n",
      "Epoch 24/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3554 - sparse_categorical_accuracy: 0.8493 - val_loss: 0.3497 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 25/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3658 - sparse_categorical_accuracy: 0.8427 - val_loss: 0.3452 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 26/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3557 - sparse_categorical_accuracy: 0.8449 - val_loss: 0.3510 - val_sparse_categorical_accuracy: 0.8465\n",
      "Epoch 27/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3507 - sparse_categorical_accuracy: 0.8534 - val_loss: 0.3478 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 28/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3544 - sparse_categorical_accuracy: 0.8493 - val_loss: 0.3465 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 29/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3572 - sparse_categorical_accuracy: 0.8514 - val_loss: 0.3411 - val_sparse_categorical_accuracy: 0.8533\n",
      "Epoch 30/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3502 - sparse_categorical_accuracy: 0.8480 - val_loss: 0.3550 - val_sparse_categorical_accuracy: 0.8490\n",
      "Epoch 31/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3543 - sparse_categorical_accuracy: 0.8512 - val_loss: 0.3399 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 32/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3517 - sparse_categorical_accuracy: 0.8506 - val_loss: 0.3432 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 33/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3576 - sparse_categorical_accuracy: 0.8451 - val_loss: 0.3432 - val_sparse_categorical_accuracy: 0.8465\n",
      "Epoch 34/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3508 - sparse_categorical_accuracy: 0.8542 - val_loss: 0.3408 - val_sparse_categorical_accuracy: 0.8524\n",
      "Epoch 35/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3452 - sparse_categorical_accuracy: 0.8521 - val_loss: 0.3367 - val_sparse_categorical_accuracy: 0.8533\n",
      "Epoch 36/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3538 - sparse_categorical_accuracy: 0.8472 - val_loss: 0.3402 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 37/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3485 - sparse_categorical_accuracy: 0.8497 - val_loss: 0.3404 - val_sparse_categorical_accuracy: 0.8533\n",
      "Epoch 38/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3510 - sparse_categorical_accuracy: 0.8538 - val_loss: 0.3427 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 39/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3599 - sparse_categorical_accuracy: 0.8493 - val_loss: 0.3425 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 40/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3484 - sparse_categorical_accuracy: 0.8563 - val_loss: 0.3360 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 41/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3452 - sparse_categorical_accuracy: 0.8580 - val_loss: 0.3376 - val_sparse_categorical_accuracy: 0.8592\n",
      "Epoch 42/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3448 - sparse_categorical_accuracy: 0.8597 - val_loss: 0.3382 - val_sparse_categorical_accuracy: 0.8575\n",
      "Epoch 43/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3580 - sparse_categorical_accuracy: 0.8389 - val_loss: 0.3383 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 44/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3548 - sparse_categorical_accuracy: 0.8476 - val_loss: 0.3358 - val_sparse_categorical_accuracy: 0.8516\n",
      "Epoch 45/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3340 - sparse_categorical_accuracy: 0.8551 - val_loss: 0.3363 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 46/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3435 - sparse_categorical_accuracy: 0.8551 - val_loss: 0.3369 - val_sparse_categorical_accuracy: 0.8524\n",
      "Epoch 47/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3375 - sparse_categorical_accuracy: 0.8589 - val_loss: 0.3343 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 48/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3391 - sparse_categorical_accuracy: 0.8504 - val_loss: 0.3393 - val_sparse_categorical_accuracy: 0.8473\n",
      "Epoch 49/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3411 - sparse_categorical_accuracy: 0.8531 - val_loss: 0.3394 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 50/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3364 - sparse_categorical_accuracy: 0.8546 - val_loss: 0.3354 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 51/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3350 - sparse_categorical_accuracy: 0.8593 - val_loss: 0.3323 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 52/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3436 - sparse_categorical_accuracy: 0.8483 - val_loss: 0.3386 - val_sparse_categorical_accuracy: 0.8507\n",
      "Epoch 53/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3446 - sparse_categorical_accuracy: 0.8582 - val_loss: 0.3345 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 54/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3366 - sparse_categorical_accuracy: 0.8565 - val_loss: 0.3359 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 55/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3387 - sparse_categorical_accuracy: 0.8604 - val_loss: 0.3361 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 56/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3457 - sparse_categorical_accuracy: 0.8587 - val_loss: 0.3316 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 57/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3401 - sparse_categorical_accuracy: 0.8572 - val_loss: 0.3376 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 58/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3309 - sparse_categorical_accuracy: 0.8629 - val_loss: 0.3404 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 59/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3378 - sparse_categorical_accuracy: 0.8572 - val_loss: 0.3345 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 60/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3397 - sparse_categorical_accuracy: 0.8574 - val_loss: 0.3329 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 61/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3416 - sparse_categorical_accuracy: 0.8447 - val_loss: 0.3346 - val_sparse_categorical_accuracy: 0.8567\n",
      "Epoch 62/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3432 - sparse_categorical_accuracy: 0.8538 - val_loss: 0.3329 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 63/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3393 - sparse_categorical_accuracy: 0.8559 - val_loss: 0.3325 - val_sparse_categorical_accuracy: 0.8601\n",
      "Epoch 64/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3274 - sparse_categorical_accuracy: 0.8616 - val_loss: 0.3365 - val_sparse_categorical_accuracy: 0.8499\n",
      "Epoch 65/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3435 - sparse_categorical_accuracy: 0.8519 - val_loss: 0.3385 - val_sparse_categorical_accuracy: 0.8541\n",
      "Epoch 66/200\n",
      "295/295 [==============================] - 2s 6ms/step - loss: 0.3370 - sparse_categorical_accuracy: 0.8555 - val_loss: 0.3343 - val_sparse_categorical_accuracy: 0.8558\n",
      "Epoch 66: early stopping\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.3189 - sparse_categorical_accuracy: 0.8664\n",
      "150/150 [==============================] - 1s 2ms/step\n",
      "Test accuracy: 86.636%\n",
      "Postprocessing Test accuracy: 90.645%\n",
      "Test F1_score: 84.834%\n",
      "Postprocessing F1_score: 89.359%\n"
     ]
    }
   ],
   "source": [
    "elements = {1, 0.5,0.35,0.3,0.25,0.2,0.15,0.1,0.05}\n",
    "accuracies = []\n",
    "f1scores= []\n",
    "\n",
    "# Iterate through the set and print each element\n",
    "for element in elements:\n",
    "  class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=temperature, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        # Compute pairwise distances\n",
    "        pairwise_distances = tfa.losses.TripletSemiHardLoss()(labels, feature_vectors_normalized)\n",
    "\n",
    "        # Scale distances by temperature\n",
    "        scaled_distances = pairwise_distances / self.temperature\n",
    "        # Apply a hinge loss\n",
    "        triplet_loss = tf.maximum(scaled_distances - 1.0, 0)\n",
    "        return (element)*(tf.reduce_mean(triplet_loss))\n",
    "\n",
    "  latent_dim = 30\n",
    "  class Encoder1(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "      super(Encoder1, self).__init__()\n",
    "      self.latent_dim = latent_dim \n",
    "      inputs = Input(shape=(19,1))\n",
    "      outputs = inputs  \n",
    "      self.encoder = tf.keras.Sequential([\n",
    "        inputs,\n",
    "        \n",
    "        layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=1),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=1),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(latent_dim, activation='relu'),\n",
    "      ])\n",
    "\n",
    "    def call(self, x):\n",
    "      encoded = self.encoder(x)\n",
    "      return encoded\n",
    "\n",
    "  class Encoder2(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "      super(Encoder2, self).__init__()\n",
    "      self.latent_dim = latent_dim\n",
    "      inputs = Input(shape=(38,1))\n",
    "      outputs = inputs  \n",
    "      self.encoder = tf.keras.Sequential([\n",
    "        inputs,\n",
    "        layers.Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=1),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=1),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(latent_dim, activation='relu'),\n",
    "      ])\n",
    "\n",
    "    def call(self, x):\n",
    "      encoded = self.encoder(x)\n",
    "      return encoded\n",
    "\n",
    "  def create_encoder1():\n",
    "    return Encoder1(latent_dim)\n",
    "\n",
    "  def create_encoder2():\n",
    "    return Encoder2(latent_dim)\n",
    "\n",
    "  def add_projection_head1(Encoder1, Encoder2):\n",
    "    inp1 = keras.Input(shape=input_shape1)\n",
    "    inp2 = keras.Input(shape=input_shape2)\n",
    "    hidden3a  = Encoder1(inp1)\n",
    "    hidden3b = Encoder2(inp2)\n",
    "    features = tf.keras.layers.Concatenate(axis=1)([hidden3a, hidden3b])\n",
    "    features = layers.Dense(16, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=[inp1, inp2], outputs=features, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "  def create_classifier(encoder, trainable):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs1 = keras.Input(shape=input_shape1)\n",
    "    inputs2 = keras.Input(shape=input_shape2)\n",
    "    features1 = encoder1(inputs1)\n",
    "    features2 = encoder2(inputs2)\n",
    "    features = tf.keras.layers.Concatenate(axis=1)([features1, features2])\n",
    "    # features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    \n",
    "    features = layers.BatchNormalization()(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(32, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(16, activation=\"relu\")(features)\n",
    "    # features = layers.BatchNormalization()(features)\n",
    "    # features = layers.Dropout(0.2)(features)\n",
    "    features = layers.Dense(4, activation=\"relu\")(features)\n",
    "    # features = layers.BatchNormalization()(features)\n",
    "    # features = layers.Dropout(0.1)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=[inputs1,inputs2], outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  from sklearn.metrics import f1_score\n",
    "  # Splitting xtrain and ytrain into training and validation sets\n",
    "  xtra_a, xval_a, ytra_a, yval_a = train_test_split(xtrain, ytrain, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Splitting xtrain1 and ytrain1 into training and validation sets\n",
    "  xtra_ac, xval_ac, ytra_ac, yval_ac = train_test_split(xtrain1, ytrain1, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "  y = np.concatenate((ytra_a,ytra_a), axis=0)\n",
    "  yv = np.concatenate((yval_a,yval_a), axis=0)\n",
    "\n",
    "  encoder1 = create_encoder1()\n",
    "  encoder2 = create_encoder2()\n",
    "  encoder_with_projection_head = add_projection_head1(encoder1, encoder2)\n",
    "  encoder_with_projection_head.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),loss=SupervisedContrastiveLoss(temperature))\n",
    "  encoder_with_projection_head.summary()\n",
    "                                                            #ytra_a                                  #yval_a\n",
    "  history = encoder_with_projection_head.fit([xtra_a,xtra_ac], ytra_ac , validation_data =([xval_a,xval_ac],yval_ac), batch_size=32, epochs=100, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "\n",
    "  learning_rate = 0.0005\n",
    "  batch_size = 16\n",
    "  hidden_units = 64\n",
    "  projection_units = 128\n",
    "  num_epochs = 200\n",
    "  dropout_rate = 0.3\n",
    "  num_classes = 2\n",
    "  input_shape1 = (19,)\n",
    "  input_shape2 = (38,)\n",
    "\n",
    "  from keras.callbacks import ModelCheckpoint ,EarlyStopping\n",
    "  classifier = create_classifier(encoder_with_projection_head, trainable=False)\n",
    "  classifier.summary()\n",
    "  history = classifier.fit(x=[xtra_a,xtra_ac], y=ytra_a, validation_data =([xval_a,xval_ac],yval_a), batch_size=batch_size, epochs=num_epochs, callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)])\n",
    "\n",
    "  accuracy = classifier.evaluate([xtest_a,xtest_ac], ytest)[1]\n",
    "\n",
    "  ##  Accuracy on post processed labels (each word should contain only one stressed syllable)\n",
    "  pred_output= classifier.predict([xtest_a,xtest_ac])\n",
    "  # pred_labels= pred_output.argmax(axis =1)\n",
    "  pred1_labels = pred_output[:,1]\n",
    "  post_labels = make_partitions(wtest, pred1_labels)\n",
    "  post_accuracy = calculate_accuracy(post_labels, ytest)\n",
    "\n",
    "  F1_score_WoPP = f1_score(ytest, pred_output.argmax(axis =1))\n",
    "  F1_score_WPP = f1_score(ytest, post_labels)\n",
    "\n",
    "  print(f\"Test accuracy: {round(accuracy * 100, 3)}%\")\n",
    "  print(f\"Postprocessing Test accuracy: {round(post_accuracy * 100, 3)}%\")\n",
    "  print(f\"Test F1_score: {round(F1_score_WoPP * 100, 3)}%\")\n",
    "  print(f\"Postprocessing F1_score: {round(F1_score_WPP * 100, 3)}%\")\n",
    "  accuracies.append(round(accuracy * 100, 3))\n",
    "  f1scores.append(round(F1_score_WoPP * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.5, 1, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05}\n",
      "[88.139, 80.518, 84.757, 87.388, 80.748, 85.237, 85.153, 83.065, 86.636]\n",
      "[86.961, 77.025, 82.775, 85.868, 79.026, 83.646, 83.591, 81.249, 84.834]\n"
     ]
    }
   ],
   "source": [
    "print(elements)\n",
    "print(accuracies)\n",
    "print(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
